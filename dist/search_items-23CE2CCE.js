searchNodes=[{"doc":"A high-level interface for creating neural network models. Axon is built entirely on top of Nx numerical definitions, so every neural network can be JIT or AOT compiled using any Nx compiler, or even transformed into high-level neural network formats like TensorFlow Lite and ONNX . For a more in-depth overview of Axon, refer to the Guides . Model Creation All Axon models start with an input layer, optionally specifying the expected shape of the input data: input = Axon . input ( &quot;input&quot; , shape : { nil , 784 } ) Notice you can specify some dimensions as nil , indicating that the dimension size will be filled in at model runtime. You can then compose inputs with other layers: model = input |&gt; Axon . dense ( 128 , activation : :relu ) |&gt; Axon . batch_norm ( ) |&gt; Axon . dropout ( rate : 0.8 ) |&gt; Axon . dense ( 64 ) |&gt; Axon . tanh ( ) |&gt; Axon . dense ( 10 ) |&gt; Axon . activation ( :softmax ) You can inspect the model for a nice summary: IO . inspect ( model ) # Axon &lt; inputs : %{ &quot;input&quot; =&gt; { nil , 784 } } outputs : &quot;softmax_0&quot; nodes : 9 &gt; Or use the Axon.Display module to see more in-depth summaries: Axon.Display . as_table ( model , Nx . template ( { 1 , 784 } , :f32 ) ) |&gt; IO . puts + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + | Model | + === === === === === === === === === === === === === + === === === === = + === === === === == + === === === === === === = + === === === === === === === == + | Layer | Input Shape | Output Shape | Options | Parameters | + === === === === === === === === === === === === === + === === === === = + === === === === == + === === === === === === = + === === === === === === === == + | input ( input ) | [ ] | { 1 , 784 } | shape : { nil , 784 } | | | | | | optional : false | | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- - + -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- -- -- -- -- -- - + | dense_0 ( dense [ &quot;input&quot; ] ) | [ { 1 , 784 } ] | { 1 , 128 } | | kernel : f32 [ 784 ] [ 128 ] | | | | | | bias : f32 [ 128 ] | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- - + -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- -- -- -- -- -- - + | relu_0 ( relu [ &quot;dense_0&quot; ] ) | [ { 1 , 128 } ] | { 1 , 128 } | | | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- - + -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- -- -- -- -- -- - + | batch_norm_0 ( batch_norm [ &quot;relu_0&quot; ] ) | [ { 1 , 128 } ] | { 1 , 128 } | epsilon : 1.0e-5 | gamma : f32 [ 128 ] | | | | | channel_index : 1 | beta : f32 [ 128 ] | | | | | momentum : 0.1 | mean : f32 [ 128 ] | | | | | | var : f32 [ 128 ] | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- - + -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- -- -- -- -- -- - + | dropout_0 ( dropout [ &quot;batch_norm_0&quot; ] ) | [ { 1 , 128 } ] | { 1 , 128 } | rate : 0.8 | | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- - + -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- -- -- -- -- -- - + | dense_1 ( dense [ &quot;dropout_0&quot; ] ) | [ { 1 , 128 } ] | { 1 , 64 } | | kernel : f32 [ 128 ] [ 64 ] | | | | | | bias : f32 [ 64 ] | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- - + -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- -- -- -- -- -- - + | tanh_0 ( tanh [ &quot;dense_1&quot; ] ) | [ { 1 , 64 } ] | { 1 , 64 } | | | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- - + -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- -- -- -- -- -- - + | dense_2 ( dense [ &quot;tanh_0&quot; ] ) | [ { 1 , 64 } ] | { 1 , 10 } | | kernel : f32 [ 64 ] [ 10 ] | | | | | | bias : f32 [ 10 ] | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- - + -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- -- -- -- -- -- - + | softmax_0 ( softmax [ &quot;dense_2&quot; ] ) | [ { 1 , 10 } ] | { 1 , 10 } | | | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- - + -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- -- -- -- -- -- - + Multiple Inputs Creating a model with multiple inputs is as easy as declaring an additional input in your Axon graph. Every input layer present in the final Axon graph will be required to be passed as input at the time of model execution. inp1 = Axon . input ( &quot;input_0&quot; , shape : { nil , 1 } ) inp2 = Axon . input ( &quot;input_1&quot; , shape : { nil , 1 } ) # Both inputs will be used model1 = Axon . add ( inp1 , inp2 ) # Only inp2 will be used model2 = Axon . add ( inp2 , inp2 ) Axon graphs are immutable, which means composing and manipulating an Axon graph creates an entirely new graph. Additionally, layer names are lazily generated at model execution time. To avoid non-deterministic input orderings and names, Axon requires each input to have a unique binary identifier. You can then reference inputs by name when passing to models at execution time: inp1 = Axon . input ( &quot;input_0&quot; , shape : { nil , 1 } ) inp2 = Axon . input ( &quot;input_1&quot; , shape : { nil , 1 } ) model1 = Axon . add ( inp1 , inp2 ) { init_fn , predict_fn } = Axon . build ( model1 ) params1 = init_fn . ( Nx . template ( { 1 , 1 } , { :f , 32 } ) , %{ } ) # Inputs are referenced by name predict_fn . ( params1 , %{ &quot;input_0&quot; =&gt; x , &quot;input_1&quot; =&gt; y } ) Multiple Outputs Nx offers robust container support which is extended to Axon. Axon allows you to wrap any valid Nx container in a layer. Containers are most commonly used to structure outputs: inp1 = Axon . input ( &quot;input_0&quot; , shape : { nil , 1 } ) inp2 = Axon . input ( &quot;input_1&quot; , shape : { nil , 1 } ) model = Axon . container ( %{ foo : inp1 , bar : inp2 } ) Containers can be arbitrarily nested: inp1 = Axon . input ( &quot;input_0&quot; , shape : { nil , 1 } ) inp2 = Axon . input ( &quot;input_1&quot; , shape : { nil , 1 } ) model = Axon . container ( { %{ foo : { inp1 , %{ bar : inp2 } } } } ) You can even use custom structs which implement the container protocol: inp1 = Axon . input ( &quot;input_0&quot; , shape : { nil , 1 } ) inp2 = Axon . input ( &quot;input_1&quot; , shape : { nil , 1 } ) model = Axon . container ( % MyStruct { foo : inp1 , bar : inp2 } ) Custom Layers If you find that Axon's built-in layers are insufficient for your needs, you can create your own using the custom layer API. All of Axon's built-in layers (aside from special ones such as input , constant , and container ) make use of this same API. Axon layers are really just placeholders for Nx computations with trainable parameters and possibly state. To define a custom layer, you just need to define a defn implementation: defn my_layer ( x , weight , _opts \\ [ ] ) do Nx . atan2 ( x , weight ) end Notice the only stipulation is that your custom layer implementation must accept at least 1 input and a list of options. At execution time, every layer will be passed a :mode option which can be used to control behavior at training and inference time. Inputs to your custom layer can be either Axon graph inputs or trainable parameters. You can pass Axon graph inputs as-is to a custom layer. To declare trainable parameters, use Axon.param/3 : weight = Axon . param ( &quot;weight&quot; , param_shape ) To create a custom layer, you &quot;wrap&quot; your implementation and inputs into a layer using Axon.layer . You'll notice the API mirrors Elixir's apply : def atan2_layer ( % Axon { } = input ) do weight = Axon . param ( &quot;weight&quot; , param_shape ) Axon . layer ( &amp; my_layer / 3 , [ input , weight ] ) end Model Execution Under the hood, Axon models are represented as Elixir structs. You can initialize and apply models by building or compiling them with Axon.build/2 or Axon.compile/4 and then calling the produced initialization and predict functions: { init_fn , predict_fn } = Axon . build ( model ) init_fn . ( Nx . template ( { 1 , 1 } , { :f , 32 } ) , %{ } ) predict_fn . ( params , inputs ) You may either set the default JIT compiler or backend globally, or pass a specific compiler to Axon.build/2 : EXLA . set_as_nx_default ( [ :tpu , :cuda , :rocm , :host ] ) { init_fn , predict_fn } = Axon . build ( model , compiler : EXLA , mode : :train ) init_fn . ( Nx . template ( { 1 , 1 } , { :f , 32 } ) , %{ } ) predict_fn . ( params , inputs ) predict_fn by default runs in inference mode, which performs certain optimizations and removes layers such as dropout layers. If constructing a training step using Axon.predict/4 or Axon.build/2 , be sure to specify mode: :train . Model Training Combining the Axon model creation API with the optimization and training APIs, you can create and train neural networks with ease: model = Axon . input ( &quot;input_0&quot; , shape : { nil , 784 } ) |&gt; Axon . dense ( 128 , activation : :relu ) |&gt; Axon . layer_norm ( ) |&gt; Axon . dropout ( ) |&gt; Axon . dense ( 10 , activation : :softmax ) IO . inspect model model_state = model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , Axon.Optimizers . adamw ( 0.005 ) ) |&gt; Axon.Loop . run ( train_data , epochs : 10 , compiler : EXLA ) See Axon.Updates and Axon.Loop for a more in-depth treatment of model optimization and model training.","ref":"Axon.html","title":"Axon","type":"module"},{"doc":"Adds an activation layer to the network. Activation layers are element-wise functions typically called after the output of another layer. Options :name - layer name.","ref":"Axon.html#activation/3","title":"Axon.activation/3","type":"function"},{"doc":"Adds an Adaptive average pool layer to the network. See Axon.Layers.adaptive_avg_pool/2 for more details. Options :name - layer name. :output_size - layer output size. :channels - channel configuration. One of :first or :last . Defaults to :last .","ref":"Axon.html#adaptive_avg_pool/2","title":"Axon.adaptive_avg_pool/2","type":"function"},{"doc":"Adds an Adaptive power average pool layer to the network. See Axon.Layers.adaptive_lp_pool/2 for more details. Options :name - layer name. :output_size - layer output size. :channels - channel configuration. One of :first or :last . Defaults to :last .","ref":"Axon.html#adaptive_lp_pool/2","title":"Axon.adaptive_lp_pool/2","type":"function"},{"doc":"Adds an Adaptive max pool layer to the network. See Axon.Layers.adaptive_max_pool/2 for more details. Options :name - layer name. :output_size - layer output size. :channels - channel configuration. One of :first or :last . Defaults to :last .","ref":"Axon.html#adaptive_max_pool/2","title":"Axon.adaptive_max_pool/2","type":"function"},{"doc":"Adds a add layer to the network. This layer performs an element-wise add operation on input layers. All input layers must be capable of being broadcast together. If one shape has a static batch size, all other shapes must have a static batch size as well. Options :name - layer name.","ref":"Axon.html#add/3","title":"Axon.add/3","type":"function"},{"doc":"Adds an Alpha dropout layer to the network. See Axon.Layers.alpha_dropout/2 for more details. Options :name - layer name. :rate - dropout rate. Defaults to 0.5 . Needs to be equal or greater than zero and less than one.","ref":"Axon.html#alpha_dropout/2","title":"Axon.alpha_dropout/2","type":"function"},{"doc":"Attaches a hook to the given Axon model. Hooks compile down to Nx.Defn.Kernel.hook/3 and provide the same functionality for adding side-effecting operations to a compiled model. For example, you can use hooks to inspect intermediate activations, send data to an external service, and more. Hooks can be configured to be invoked on the following events: :initialize - on model initialization. :pre_forward - before layer forward pass is invoked. :forward - after layer forward pass is invoked. :backward - after layer backward pass is invoked. To invoke a hook on every single event, you may pass :all to on: . Axon . input ( &quot;input&quot; , shape : { nil , 1 } ) |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 , on : :all ) The default event is :forward , assuming you want a hook invoked on the layers forward pass. You may configure hooks to run in one of only training or inference mode using the :mode option. The default mode is :both to be invoked during both train and inference mode. Axon . input ( &quot;input&quot; , shape : { nil , 1 } ) |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 , on : :forward , mode : :train ) You can also attach multiple hooks to a single layer. Hooks are invoked in the order in which they are declared. If order is important, you should attach hooks in the order you want them to be executed: Axon . input ( &quot;input&quot; , shape : { nil , 1 } ) # I will be executed first |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 ) # I will be executed second |&gt; Axon . attach_hook ( fn _ -&gt; IO . write ( &quot;HERE&quot; ) end ) Hooks are executed at their point of attachment. You must insert hooks at each point you want a hook to execute during model execution. Axon . input ( &quot;input&quot; , shape : { nil , 1 } ) |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 ) |&gt; Axon . relu ( ) |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 )","ref":"Axon.html#attach_hook/3","title":"Axon.attach_hook/3","type":"function"},{"doc":"Adds an Average pool layer to the network. See Axon.Layers.avg_pool/2 for more details. Options :name - layer name. :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to size of kernel. :padding - padding to the spatial dimensions of the input. Defaults to :valid . :dilations - window dilations. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :last .","ref":"Axon.html#avg_pool/2","title":"Axon.avg_pool/2","type":"function"},{"doc":"Adds a Batch normalization layer to the network. See Axon.Layers.batch_norm/6 for more details. Options :name - layer name. :gamma_initializer - gamma parameter initializer. Defaults to :glorot_uniform . :beta_initializer - beta parameter initializer. Defaults to :zeros . :channel_index - input feature index used for calculating mean and variance. Defaults to -1 . :epsilon - numerical stability term.","ref":"Axon.html#batch_norm/2","title":"Axon.batch_norm/2","type":"function"},{"doc":"Adds a bias layer to the network. A bias layer simply adds a trainable bias to an input. Options :name - layer name. :bias_initializer - initializer for bias weights. Defaults to :zeros .","ref":"Axon.html#bias/2","title":"Axon.bias/2","type":"function"},{"doc":"Adds a bilinear layer to the network. The bilinear layer implements: output = activation ( dot ( dot ( input1 , kernel ) , input2 ) + bias ) where activation is given by the :activation option and both kernel and bias are layer parameters. units specifies the number of output units. All dimensions but the last of input1 and input2 must match. The batch sizes of both inputs must also match or at least one must be nil . Inferred output batch size coerces to the strictest input batch size. Compiles to Axon.Layers.bilinear/5 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros . :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true .","ref":"Axon.html#bilinear/4","title":"Axon.bilinear/4","type":"function"},{"doc":"Builds the given model to {init_fn, predict_fn} . Once built, a model can be passed as argument to Nx.Defn . init_fn The init_fn receives two arguments, the input template and an optional map with initial parameters for layers or namespaces: { init_fn , predict_fn } = Axon . build ( model ) init_fn . ( Nx . template ( { 1 , 1 } , { :f , 32 } ) , %{ &quot;dense_0&quot; =&gt; dense_params } ) predict_fn The predict_fn receives two arguments, the trained parameters and the actual inputs: { _init_fn , predict_fn } = Axon . build ( model , opts ) predict_fn . ( params , input ) Options :mode - one of :inference or :training . Forwarded to layers to control differences in compilation at training or inference time. Defaults to :inference :debug - if true , will log graph traversal and generation metrics. Also forwarded to JIT if debug mode is available for your chosen compiler or backend. Defaults to false All other options are forwarded to the default JIT compiler or backend.","ref":"Axon.html#build/2","title":"Axon.build/2","type":"function"},{"doc":"Adds a Continuously-differentiable exponential linear unit activation layer to the network. See Axon.Activations.celu/1 for more details. Options :name - layer name.","ref":"Axon.html#celu/2","title":"Axon.celu/2","type":"function"},{"doc":"Compiles the given model to {init_fn, predict_fn} . This function will compile a model specialized to the given input shapes and types. This is useful for avoiding the overhead of long compilations at program runtime. You must provide template inputs which match the expected shapes and types of inputs at execution time. This function makes use of the built-in Nx.Defn.compile/3 . Note that passing inputs which differ in shape or type from the templates provided to this function will result in potentially expensive recompilation.","ref":"Axon.html#compile/4","title":"Axon.compile/4","type":"function"},{"doc":"Adds a concatenate layer to the network. This layer will concatenate inputs along the last dimension unless specified otherwise. Options :name - layer name. :axis - concatenate axis. Defaults to -1 .","ref":"Axon.html#concatenate/3","title":"Axon.concatenate/3","type":"function"},{"doc":"Adds a conditional layer which conditionally executes true_graph or false_graph based on the condition cond_fn at runtime. cond_fn is an arity-1 function executed on the output of the parent graph. It must return a boolean scalar tensor (e.g. 1 or 0). The shapes of true_graph and false_graph must be equal.","ref":"Axon.html#cond/5","title":"Axon.cond/5","type":"function"},{"doc":"Adds a constant layer to the network. Constant layers encapsulate Nx tensors in an Axon layer for ease of use with other Axon layers. They can be used interchangeably with other Axon layers: inp = Axon . input ( &quot;input&quot; , shape : { nil , 32 } ) my_constant = Axon . constant ( Nx . iota ( { 1 , 32 } ) ) model = Axon . add ( inp , my_constant ) Constant layers will be cast according to the mixed precision policy. If it's important for your constant to retain it's type during the computation, you will need to set the mixed precision policy to ignore constant layers. Options :name - layer name.","ref":"Axon.html#constant/2","title":"Axon.constant/2","type":"function"},{"doc":"Adds a container layer to the network. In certain cases you may want your model to have multiple outputs. In order to make this work, you must &quot;join&quot; the outputs into an Axon layer using this function for use in initialization and inference later on. The given container can be any valid Axon Nx container. Options :name - layer name. Examples iex&gt; inp1 = Axon . input ( &quot;input_0&quot; , shape : { nil , 1 } ) iex&gt; inp2 = Axon . input ( &quot;input_1&quot; , shape : { nil , 2 } ) iex&gt; model = Axon . container ( %{ a : inp1 , b : inp2 } ) iex&gt; %{ a : a , b : b } = Axon . predict ( model , %{ } , %{ ...&gt; &quot;input_0&quot; =&gt; Nx . tensor ( [ [ 1.0 ] ] ) , ...&gt; &quot;input_1&quot; =&gt; Nx . tensor ( [ [ 1.0 , 2.0 ] ] ) ...&gt; } ) iex&gt; a # Nx.Tensor &lt; f32 [ 1 ] [ 1 ] [ [ 1.0 ] ] &gt; iex&gt; b # Nx.Tensor &lt; f32 [ 1 ] [ 2 ] [ [ 1.0 , 2.0 ] ] &gt;","ref":"Axon.html#container/2","title":"Axon.container/2","type":"function"},{"doc":"Adds a convolution layer to the network. The convolution layer implements a general dimensional convolutional layer - which convolves a kernel over the input to produce an output. Compiles to Axon.Layers.conv/4 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to 1 . :padding - padding to the spatial dimensions of the input. Defaults to :valid . :input_dilation - dilation to apply to input. Defaults to 1 . :kernel_dilation - dilation to apply to kernel. Defaults to 1 . :feature_group_size - feature group size for convolution. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :last .","ref":"Axon.html#conv/3","title":"Axon.conv/3","type":"function"},{"doc":"See conv_lstm/3 .","ref":"Axon.html#conv_lstm/2","title":"Axon.conv_lstm/2","type":"function"},{"doc":"Adds a convolutional long short-term memory (LSTM) layer to the network with a random initial hidden state. See conv_lstm/4 for more details. Additional options :recurrent_initializer - initializer for hidden state. Defaults to :orthogonal .","ref":"Axon.html#conv_lstm/3","title":"Axon.conv_lstm/3","type":"function"},{"doc":"Adds a convolutional long short-term memory (LSTM) layer to the network with the given initial hidden state.. ConvLSTMs apply Axon.Layers.conv_lstm_cell/5 over an entire input sequence and return: { { new_cell , new_hidden } , output_sequence } You can use the output state as the hidden state of another ConvLSTM layer. Options :name - layer name. :padding - convolutional padding. Defaults to :same . :kernel_size - convolutional kernel size. Defaults to 1 . :strides - convolutional strides. Defaults to 1 . :unroll - :dynamic (loop preserving) or :static (compiled) unrolling of RNN. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros . :use_bias - whether the layer should add bias to the output. Defaults to true .","ref":"Axon.html#conv_lstm/4","title":"Axon.conv_lstm/4","type":"function"},{"doc":"Adds a transposed convolution layer to the network. The transposed convolution layer is sometimes referred to as a fractionally strided convolution or (incorrectly) as a deconvolution. Compiles to Axon.Layers.conv_transpose/4 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to 1 . :padding - padding to the spatial dimensions of the input. Defaults to :valid . :kernel_dilation - dilation to apply to kernel. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :last .","ref":"Axon.html#conv_transpose/3","title":"Axon.conv_transpose/3","type":"function"},{"doc":"Adds a dense layer to the network. The dense layer implements: output = activation ( dot ( input , kernel ) + bias ) where activation is given by the :activation option and both kernel and bias are layer parameters. units specifies the number of output units. Compiles to Axon.Layers.dense/4 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros . :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true .","ref":"Axon.html#dense/3","title":"Axon.dense/3","type":"function"},{"doc":"Adds a depthwise convolution layer to the network. The depthwise convolution layer implements a general dimensional depthwise convolution - which is a convolution where the feature group size is equal to the number of input channels. Channel multiplier grows the input channels by the given factor. An input factor of 1 means the output channels are the same as the input channels. Compiles to Axon.Layers.depthwise_conv/4 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to 1 . :padding - padding to the spatial dimensions of the input. Defaults to :valid . :input_dilation - dilation to apply to input. Defaults to 1 . :kernel_dilation - dilation to apply to kernel. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :last .","ref":"Axon.html#depthwise_conv/3","title":"Axon.depthwise_conv/3","type":"function"},{"doc":"Deserializes serialized model and parameters into a {model, params} tuple. It is the opposite of Axon.serialize/3 . Examples iex&gt; model = Axon . input ( &quot;input&quot; , shape : { nil , 2 } ) |&gt; Axon . dense ( 1 , kernel_initializer : :zeros , activation : :relu ) iex&gt; { init_fn , _ } = Axon . build ( model ) iex&gt; params = init_fn . ( Nx . template ( { 1 , 2 } , :f32 ) , %{ } ) iex&gt; serialized = Axon . serialize ( model , params ) iex&gt; { saved_model , saved_params } = Axon . deserialize ( serialized ) iex&gt; { _ , predict_fn } = Axon . build ( saved_model ) iex&gt; predict_fn . ( saved_params , Nx . tensor ( [ [ 1.0 , 1.0 ] ] ) ) # Nx.Tensor &lt; f32 [ 1 ] [ 1 ] [ [ 0.0 ] ] &gt;","ref":"Axon.html#deserialize/2","title":"Axon.deserialize/2","type":"function"},{"doc":"Adds a Dropout layer to the network. See Axon.Layers.dropout/2 for more details. Options :name - layer name. :rate - dropout rate. Defaults to 0.5 . Needs to be equal or greater than zero and less than one.","ref":"Axon.html#dropout/2","title":"Axon.dropout/2","type":"function"},{"doc":"Adds an Exponential linear unit activation layer to the network. See Axon.Activations.elu/1 for more details. Options :name - layer name.","ref":"Axon.html#elu/2","title":"Axon.elu/2","type":"function"},{"doc":"Adds an embedding layer to the network. An embedding layer initializes a kernel of shape {vocab_size, embedding_size} which acts as a lookup table for sequences of discrete tokens (e.g. sentences). Embeddings are typically used to obtain a dense representation of a sparse input space. Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :uniform .","ref":"Axon.html#embedding/4","title":"Axon.embedding/4","type":"function"},{"doc":"Adds an Exponential activation layer to the network. See Axon.Activations.exp/1 for more details. Options :name - layer name.","ref":"Axon.html#exp/2","title":"Axon.exp/2","type":"function"},{"doc":"Adds a Feature alpha dropout layer to the network. See Axon.Layers.feature_alpha_dropout/2 for more details. Options :name - layer name. :rate - dropout rate. Defaults to 0.5 . Needs to be equal or greater than zero and less than one.","ref":"Axon.html#feature_alpha_dropout/2","title":"Axon.feature_alpha_dropout/2","type":"function"},{"doc":"Adds a flatten layer to the network. This layer will flatten all but the batch dimensions of the input into a single layer. Typically called to flatten the output of a convolution for use with a dense layer. Options :name - layer name.","ref":"Axon.html#flatten/2","title":"Axon.flatten/2","type":"function"},{"doc":"Freezes parameters returned from fun in the given model. fun takes the model's parameter list and returns the list of parameters it wishes to freeze. fun defaults to the identity function, freezing all of the parameters in model . Freezing parameters is useful when performing transfer learning to leverage features learned from another problem in a new problem. For example, it's common to combine the convolutional base from larger models trained on ImageNet with fresh fully-connected classifiers. The combined model is then trained on fresh data, with the convolutional base frozen so as not to lose information. You can see this example in code here: cnn_base = get_pretrained_cnn_base ( ) model = cnn_base |&gt; Axon . freeze ( ) |&gt; Axon . flatten ( ) |&gt; Axon . dense ( 1024 , activation : :relu ) |&gt; Axon . dropout ( ) |&gt; Axon . dense ( 1000 , activation : :softmax ) model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , Axon.Optimizers . adam ( 0.005 ) ) |&gt; Axon.Loop . run ( data , epochs : 10 ) When compiled, frozen parameters are wrapped in Nx.Defn.Kernel.stop_grad/1 , which zeros out the gradient with respect to the frozen parameter. Gradients of frozen parameters will return 0.0 , meaning they won't be changed during the update process.","ref":"Axon.html#freeze/2","title":"Axon.freeze/2","type":"function"},{"doc":"Adds a Gaussian error linear unit activation layer to the network. See Axon.Activations.gelu/1 for more details. Options :name - layer name.","ref":"Axon.html#gelu/2","title":"Axon.gelu/2","type":"function"},{"doc":"Returns information about a model's inputs.","ref":"Axon.html#get_inputs/1","title":"Axon.get_inputs/1","type":"function"},{"doc":"Returns a map of model op counts for each unique operation in a model by their given :op_name . Examples iex&gt; model = Axon . input ( &quot;input&quot; , shape : { nil , 1 } ) |&gt; Axon . dense ( 2 ) iex&gt; Axon . get_op_counts ( model ) %{ input : 1 , dense : 1 } iex&gt; model = Axon . input ( &quot;input&quot; , shape : { nil , 1 } ) |&gt; Axon . tanh ( ) |&gt; Axon . tanh ( ) iex&gt; Axon . get_op_counts ( model ) %{ input : 1 , tanh : 2 }","ref":"Axon.html#get_op_counts/1","title":"Axon.get_op_counts/1","type":"function"},{"doc":"Returns a node's immediate input options. Note that this does not take into account options of parent layers, only the option which belong to the immediate layer.","ref":"Axon.html#get_options/1","title":"Axon.get_options/1","type":"function"},{"doc":"Returns a model's output shape from the given input template.","ref":"Axon.html#get_output_shape/3","title":"Axon.get_output_shape/3","type":"function"},{"doc":"Returns a node's immediate parameters. Note this does not take into account parameters of parent layers - only the parameters which belong to the immediate layer.","ref":"Axon.html#get_parameters/1","title":"Axon.get_parameters/1","type":"function"},{"doc":"Adds a Global average pool layer to the network. See Axon.Layers.global_avg_pool/2 for more details. Typically used to connect feature extractors such as those in convolutional neural networks to fully-connected models by reducing inputs along spatial dimensions to only feature and batch dimensions. Options :name - layer name. :keep_axes - option to keep reduced axes. If true , keeps reduced axes with a dimension size of 1. :channels - channel configuration. One of :first or :last . Defaults to :last .","ref":"Axon.html#global_avg_pool/2","title":"Axon.global_avg_pool/2","type":"function"},{"doc":"Adds a Global LP pool layer to the network. See Axon.Layers.global_lp_pool/2 for more details. Typically used to connect feature extractors such as those in convolutional neural networks to fully-connected models by reducing inputs along spatial dimensions to only feature and batch dimensions. Options :name - layer name. :keep_axes - option to keep reduced axes. If true , keeps reduced axes with a dimension size of 1. :channels - channel configuration. One of :first or :last . Defaults to :last .","ref":"Axon.html#global_lp_pool/2","title":"Axon.global_lp_pool/2","type":"function"},{"doc":"Adds a Global max pool layer to the network. See Axon.Layers.global_max_pool/2 for more details. Typically used to connect feature extractors such as those in convolutional neural networks to fully-connected models by reducing inputs along spatial dimensions to only feature and batch dimensions. Options :name - layer name. :keep_axes - option to keep reduced axes. If true , keeps reduced axes with a dimension size of 1. :channels - channel configuration. One of :first or :last . Defaults to :last .","ref":"Axon.html#global_max_pool/2","title":"Axon.global_max_pool/2","type":"function"},{"doc":"Adds a group normalization layer to the network. See Axon.Layers.group_norm/4 for more details. Options :name - layer name. :gamma_initializer - gamma parameter initializer. Defaults to :glorot_uniform . :beta_initializer - beta parameter initializer. Defaults to :zeros . :channel_index - input feature index used for calculating mean and variance. Defaults to -1 . :epsilon - numerical stability term.","ref":"Axon.html#group_norm/3","title":"Axon.group_norm/3","type":"function"},{"doc":"See gru/3 .","ref":"Axon.html#gru/2","title":"Axon.gru/2","type":"function"},{"doc":"Adds a gated recurrent unit (GRU) layer to the network with a random initial hidden state. See gru/4 for more details. Additional options :recurrent_initializer - initializer for hidden state. Defaults to :orthogonal .","ref":"Axon.html#gru/3","title":"Axon.gru/3","type":"function"},{"doc":"Adds a gated recurrent unit (GRU) layer to the network with the given initial hidden state. GRUs apply Axon.Layers.gru_cell/7 over an entire input sequence and return: { { new_hidden } , output_sequence } You can use the output state as the hidden state of another GRU layer. Options :name - layer name. :activation - recurrent activation. Defaults to :tanh . :gate - recurrent gate function. Defaults to :sigmoid . :unroll - :dynamic (loop preserving) or :static (compiled) unrolling of RNN. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros . :use_bias - whether the layer should add bias to the output. Defaults to true .","ref":"Axon.html#gru/4","title":"Axon.gru/4","type":"function"},{"doc":"Adds a Hard sigmoid activation layer to the network. See Axon.Activations.hard_sigmoid/1 for more details. Options :name - layer name.","ref":"Axon.html#hard_sigmoid/2","title":"Axon.hard_sigmoid/2","type":"function"},{"doc":"Adds a Hard sigmoid weighted linear unit activation layer to the network. See Axon.Activations.hard_silu/1 for more details. Options :name - layer name.","ref":"Axon.html#hard_silu/2","title":"Axon.hard_silu/2","type":"function"},{"doc":"Adds a Hard hyperbolic tangent activation layer to the network. See Axon.Activations.hard_tanh/1 for more details. Options :name - layer name.","ref":"Axon.html#hard_tanh/2","title":"Axon.hard_tanh/2","type":"function"},{"doc":"Adds an input layer to the network. Input layers specify a model's inputs. Input layers are always the root layers of the neural network. You must specify the input layers name, which will be used to uniquely identify it in the case of multiple inputs. Options :shape - the expected input shape, use nil for dimensions of a dynamic size. :optional - if true , the input may be omitted when using the model. This needs to be handled in one of the subsequent layers. See optional/2 for more details.","ref":"Axon.html#input/2","title":"Axon.input/2","type":"function"},{"doc":"","ref":"Axon.html#input/3","title":"Axon.input/3","type":"function"},{"doc":"Adds an Instance normalization layer to the network. See Axon.Layers.instance_norm/6 for more details. Options :name - layer name. :gamma_initializer - gamma parameter initializer. Defaults to :glorot_uniform . :beta_initializer - beta parameter initializer. Defaults to :zeros . :channel_index - input feature index used for calculating mean and variance. Defaults to -1 . :epsilon - numerical stability term.","ref":"Axon.html#instance_norm/2","title":"Axon.instance_norm/2","type":"function"},{"doc":"Custom Axon layer with given inputs. Inputs may be other Axon layers or trainable parameters created with Axon.param . At inference time, op will be applied with inputs in specified order and an additional opts parameter which specifies inference options. All options passed to layer are forwarded to inference function except: :name - layer name. :op_name - layer operation for inspection and building parameter map. Note this means your layer should not use these as input options, as they will always be dropped during inference compilation. Axon's compiler will additionally forward the following options to every layer at inference time: :mode - :inference or :train . To control layer behavior based on inference or train time. op is a function of the form: fun = fn input , weight , bias , _opts -&gt; input * weight + bias end","ref":"Axon.html#layer/3","title":"Axon.layer/3","type":"function"},{"doc":"Adds a Layer normalization layer to the network. See Axon.Layers.layer_norm/4 for more details. Options :name - layer name. :gamma_initializer - gamma parameter initializer. Defaults to :glorot_uniform . :beta_initializer - beta parameter initializer. Defaults to :zeros . :channel_index - input feature index used for calculating mean and variance. Defaults to -1 . :epsilon - numerical stability term.","ref":"Axon.html#layer_norm/2","title":"Axon.layer_norm/2","type":"function"},{"doc":"Adds a Leaky rectified linear unit activation layer to the network. See Axon.Activations.leaky_relu/1 for more details. Options :name - layer name.","ref":"Axon.html#leaky_relu/2","title":"Axon.leaky_relu/2","type":"function"},{"doc":"Adds a Linear activation layer to the network. See Axon.Activations.linear/1 for more details. Options :name - layer name.","ref":"Axon.html#linear/2","title":"Axon.linear/2","type":"function"},{"doc":"Adds a Log-sigmoid activation layer to the network. See Axon.Activations.log_sigmoid/1 for more details. Options :name - layer name.","ref":"Axon.html#log_sigmoid/2","title":"Axon.log_sigmoid/2","type":"function"},{"doc":"Adds a Log-softmax activation layer to the network. See Axon.Activations.log_softmax/1 for more details. Options :name - layer name.","ref":"Axon.html#log_softmax/2","title":"Axon.log_softmax/2","type":"function"},{"doc":"Adds a Log-sumexp activation layer to the network. See Axon.Activations.log_sumexp/1 for more details. Options :name - layer name.","ref":"Axon.html#log_sumexp/2","title":"Axon.log_sumexp/2","type":"function"},{"doc":"Adds a Power average pool layer to the network. See Axon.Layers.lp_pool/2 for more details. Options :name - layer name. :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to size of kernel. :padding - padding to the spatial dimensions of the input. Defaults to :valid . :dilations - window dilations. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :last .","ref":"Axon.html#lp_pool/2","title":"Axon.lp_pool/2","type":"function"},{"doc":"See lstm/3 .","ref":"Axon.html#lstm/2","title":"Axon.lstm/2","type":"function"},{"doc":"Adds a long short-term memory (LSTM) layer to the network with a random initial hidden state. See lstm/4 for more details. Additional options :recurrent_initializer - initializer for hidden state. Defaults to :orthogonal .","ref":"Axon.html#lstm/3","title":"Axon.lstm/3","type":"function"},{"doc":"Adds a long short-term memory (LSTM) layer to the network with the given initial hidden state. LSTMs apply Axon.Layers.lstm_cell/7 over an entire input sequence and return: { output_sequence , { new_cell , new_hidden } } You can use the output state as the hidden state of another LSTM layer. Options :name - layer name. :activation - recurrent activation. Defaults to :tanh . :gate - recurrent gate function. Defaults to :sigmoid . :unroll - :dynamic (loop preserving) or :static (compiled) unrolling of RNN. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros . :use_bias - whether the layer should add bias to the output. Defaults to true .","ref":"Axon.html#lstm/4","title":"Axon.lstm/4","type":"function"},{"doc":"Traverses graph nodes in order, applying fun to each node exactly once to return a transformed node in its place(s) in the graph. This function maintains an internal cache which ensures each node is only visited and transformed exactly once. fun must accept an Axon node and return an Axon node. Please note that modifying node lineage (e.g. altering a node's parent) will result in disconnected graphs. Examples One common use of this function is to implement common instrumentation between layers without needing to build a new explicitly instrumented version of a model. For example, you can use this function to visualize intermediate activations of all convolutional layers in a model: instrumented_model = Axon . ( model , fn % Axon { op : :conv } = graph -&gt; Axon . attach_hook ( graph , &amp; visualize_activations / 1 ) graph -&gt; graph end ) Another use case is to replace entire classes of layers with another. For example, you may want to replace all relu layers with tanh layers: new_model = Axon . map_nodes ( model , fn % Axon { op : :relu } = graph -&gt; # Get nodes immediate parent parent = Axon . get_parent ( graph ) # Replace node with a tanh Axon . tanh ( parent ) graph -&gt; graph end )","ref":"Axon.html#map_nodes/2","title":"Axon.map_nodes/2","type":"function"},{"doc":"Adds a Max pool layer to the network. See Axon.Layers.max_pool/2 for more details. Options :name - layer name. :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to size of kernel. :padding - padding to the spatial dimensions of the input. Defaults to :valid . :dilations - window dilations. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :last .","ref":"Axon.html#max_pool/2","title":"Axon.max_pool/2","type":"function"},{"doc":"Adds a Mish activation layer to the network. See Axon.Activations.mish/1 for more details. Options :name - layer name.","ref":"Axon.html#mish/2","title":"Axon.mish/2","type":"function"},{"doc":"Adds a multiply layer to the network. This layer performs an element-wise multiply operation on input layers. All input layers must be capable of being broadcast together. If one shape has a static batch size, all other shapes must have a static batch size as well. Options :name - layer name.","ref":"Axon.html#multiply/3","title":"Axon.multiply/3","type":"function"},{"doc":"Wraps an Axon model into a namespace. A namespace is a part of an Axon model which is meant to be a self-contained collection of Axon layers. Namespaces are guaranteed to always generate with the same internal layer names and can be re-used universally across models. Namespaces are most useful for containing large collections of layers and offering a straightforward means for accessing the parameters of individual model components. A common application of namespaces is to use them in with a pre-trained model for fine-tuning: { base , resnet_params } = resnet ( ) base = base |&gt; Axon . namespace ( &quot;resnet&quot; ) model = base |&gt; Axon . dense ( 1 ) { init_fn , predict_fn } = Axon . build ( model ) init_fn . ( Nx . template ( { 1 , 3 , 224 , 224 } , { :f , 32 } ) , %{ &quot;resnset&quot; =&gt; resnet_params } ) Notice you can use init_fn in conjunction with namespaces to specify which portion of a model you'd like to initialize from a fixed starting point. Namespaces have fixed names, which means it's easy to run into namespace collisions. Re-using namespaces, re-using inner parts of a namespace, and attempting to share layers between namespaces are still sharp edges in namespace usage.","ref":"Axon.html#namespace/2","title":"Axon.namespace/2","type":"function"},{"doc":"Applies the given Nx expression to the input. Nx layers are meant for quick applications of functions without trainable parameters. For example, they are useful for applying functions which apply accessors to containers: model = Axon . container ( { foo , bar } ) Axon . nx ( model , &amp; elem ( &amp;1 , 0 ) ) Options :name - layer name.","ref":"Axon.html#nx/3","title":"Axon.nx/3","type":"function"},{"doc":"Wraps an Axon model in an optional node. By default, when an optional input is missing, all subsequent layers are nullified. For example, consider this model: values = Axon . input ( &quot;values&quot; ) mask = Axon . input ( &quot;mask&quot; , optional : true ) model = values |&gt; Axon . dense ( 10 ) |&gt; Axon . multiply ( mask ) |&gt; Axon . dense ( 1 ) |&gt; Axon . sigmoid ( ) In case the mask is not provided, the input node will resolve to %Axon.None{} and so will all the layers that depend on it. By using optional/2 a layer may opt-in to receive %Axon.None{} . To fix our example, we could define a custom layer to apply the mask only when present def apply_optional_mask ( % Axon { } = x , % Axon { } = mask ) do Axon . layer ( fn x , mask , _opts -&gt; case mask do % Axon.None { } -&gt; x mask -&gt; Nx . multiply ( x , mask ) end end , [ x , Axon . optional ( mask ) ] ) end # ... model = values |&gt; Axon . dense ( 10 ) |&gt; apply_optional_mask ( mask ) |&gt; Axon . dense ( 1 ) |&gt; Axon . sigmoid ( ) Options :name - layer name.","ref":"Axon.html#optional/2","title":"Axon.optional/2","type":"function"},{"doc":"Adds a pad layer to the network. This layer will pad the spatial dimensions of the input. Padding configuration is a list of tuples for each spatial dimension. Options :name - layer name. :channels - channel configuration. One of :first or :last . Defaults to :last .","ref":"Axon.html#pad/4","title":"Axon.pad/4","type":"function"},{"doc":"Trainable Axon parameter used to create custom layers. Parameters are specified in usages of Axon.layer and will be automatically initialized and used in subsequent applications of Axon models. Parameters must be specified in order of their usage. Options :initializer - parameter initializer. Defaults to :glorot_uniform .","ref":"Axon.html#param/3","title":"Axon.param/3","type":"function"},{"doc":"Compiles and runs the given Axon model with params on input with the given compiler options. Options :mode - one of :inference or :training . Forwarded to layers to control differences in compilation at training or inference time. Defaults to :inference :debug - if true , will log graph traversal and generation metrics. Also forwarded to JIT if debug mode is available for your chosen compiler or backend. Defaults to false All other options are forwarded to the default JIT compiler or backend.","ref":"Axon.html#predict/4","title":"Axon.predict/4","type":"function"},{"doc":"Traverses graph nodes in order, applying fun to each node exactly once to return a transformed node in its place(s) in the graph. This function maintains an internal cache which ensures each node is only visited and transformed exactly once. fun must accept an Axon node and accumulator and return an updated accumulator. Examples Internally this function is used in several places to accumulate graph metadata. For example, you can use it to count the number of a certain type of operation in the graph: Axon . reduce_nodes ( model , 0 , fn % Axon.Nodes { op : :relu } , acc -&gt; acc + 1 _ , acc -&gt; acc end )","ref":"Axon.html#reduce_nodes/3","title":"Axon.reduce_nodes/3","type":"function"},{"doc":"Adds a Rectified linear unit 6 activation layer to the network. See Axon.Activations.relu6/1 for more details. Options :name - layer name.","ref":"Axon.html#relu6/2","title":"Axon.relu6/2","type":"function"},{"doc":"Adds a Rectified linear unit activation layer to the network. See Axon.Activations.relu/1 for more details. Options :name - layer name.","ref":"Axon.html#relu/2","title":"Axon.relu/2","type":"function"},{"doc":"Adds a reshape layer to the network. This layer implements a special case of Nx.reshape which accounts for possible batch dimensions in the input tensor. You may pass the magic dimension :batch as a placeholder for dynamic batch sizes. You can use :batch seamlessly with :auto dimension sizes. If the input is an Axon constant, the reshape behavior matches that of Nx.reshape/2 . Options :name - layer name.","ref":"Axon.html#reshape/3","title":"Axon.reshape/3","type":"function"},{"doc":"Adds a resize layer to the network. Resizing can be used for interpolation or upsampling input values in a neural network. For example, you can use this layer as an upsampling layer within a GAN. Resize shape must be a tuple representing the resized spatial dimensions of the input tensor. Compiles to Axon.Layers.resize/2 . Options :name - layer name. :method - resize method. Defaults to :nearest . :channels - channel configuration. One of :first or :last . Defaults to :last .","ref":"Axon.html#resize/3","title":"Axon.resize/3","type":"function"},{"doc":"Adds a Scaled exponential linear unit activation layer to the network. See Axon.Activations.selu/1 for more details. Options :name - layer name.","ref":"Axon.html#selu/2","title":"Axon.selu/2","type":"function"},{"doc":"Adds a depthwise separable 2-dimensional convolution to the network. Depthwise separable convolutions break the kernel into kernels for each dimension of the input and perform a depthwise conv over the input with each kernel. Compiles to Axon.Layers.separable_conv2d/6 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to 1 . :padding - padding to the spatial dimensions of the input. Defaults to :valid . :input_dilation - dilation to apply to input. Defaults to 1 . :kernel_dilation - dilation to apply to kernel. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :last .","ref":"Axon.html#separable_conv2d/3","title":"Axon.separable_conv2d/3","type":"function"},{"doc":"Adds a depthwise separable 3-dimensional convolution to the network. Depthwise separable convolutions break the kernel into kernels for each dimension of the input and perform a depthwise conv over the input with each kernel. Compiles to Axon.Layers.separable_conv3d/8 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to 1 . :padding - padding to the spatial dimensions of the input. Defaults to :valid . :input_dilation - dilation to apply to input. Defaults to 1 . :kernel_dilation - dilation to apply to kernel. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :last .","ref":"Axon.html#separable_conv3d/3","title":"Axon.separable_conv3d/3","type":"function"},{"doc":"Serializes a model and its parameters for persisting models to disk or elsewhere. Model and parameters are serialized as a tuple, where the model is converted to a recursive map to ensure compatibility with future Axon versions and the parameters are serialized using Nx.serialize/2 . There is some additional metadata included such as current serialization version for compatibility. Serialization opts are forwarded to Nx.serialize/2 and :erlang.term_to_binary/2 for controlling compression options. Examples iex&gt; model = Axon . input ( &quot;input&quot; , shape : { nil , 2 } ) |&gt; Axon . dense ( 1 , kernel_initializer : :zeros , activation : :relu ) iex&gt; { init_fn , _ } = Axon . build ( model ) iex&gt; params = init_fn . ( Nx . template ( { 1 , 2 } , :f32 ) , %{ } ) iex&gt; serialized = Axon . serialize ( model , params ) iex&gt; { saved_model , saved_params } = Axon . deserialize ( serialized ) iex&gt; { _ , predict_fn } = Axon . build ( saved_model ) iex&gt; predict_fn . ( saved_params , Nx . tensor ( [ [ 1.0 , 1.0 ] ] ) ) # Nx.Tensor &lt; f32 [ 1 ] [ 1 ] [ [ 0.0 ] ] &gt;","ref":"Axon.html#serialize/3","title":"Axon.serialize/3","type":"function"},{"doc":"Sets a node's immediate options to the given input options. Note that this does not take into account options of parent layers, only the option which belong to the immediate layer. New options must be compatible with the given layer op. Adding unsupported options to an Axon layer will result in an error at graph execution time.","ref":"Axon.html#set_options/2","title":"Axon.set_options/2","type":"function"},{"doc":"Sets a node's immediate parameters to the given parameters. Note this does not take into account parameters of parent layers - only the parameters which belong to the immediate layer. The new parameters must be compatible with the layer's old parameters.","ref":"Axon.html#set_parameters/2","title":"Axon.set_parameters/2","type":"function"},{"doc":"Adds a Sigmoid activation layer to the network. See Axon.Activations.sigmoid/1 for more details. Options :name - layer name.","ref":"Axon.html#sigmoid/2","title":"Axon.sigmoid/2","type":"function"},{"doc":"Adds a Sigmoid weighted linear unit activation layer to the network. See Axon.Activations.silu/1 for more details. Options :name - layer name.","ref":"Axon.html#silu/2","title":"Axon.silu/2","type":"function"},{"doc":"Adds a Softmax activation layer to the network. See Axon.Activations.softmax/1 for more details. Options :name - layer name.","ref":"Axon.html#softmax/2","title":"Axon.softmax/2","type":"function"},{"doc":"Adds a Softplus activation layer to the network. See Axon.Activations.softplus/1 for more details. Options :name - layer name.","ref":"Axon.html#softplus/2","title":"Axon.softplus/2","type":"function"},{"doc":"Adds a Softsign activation layer to the network. See Axon.Activations.softsign/1 for more details. Options :name - layer name.","ref":"Axon.html#softsign/2","title":"Axon.softsign/2","type":"function"},{"doc":"Adds a Spatial dropout layer to the network. See Axon.Layers.spatial_dropout/2 for more details. Options :name - layer name. :rate - dropout rate. Defaults to 0.5 . Needs to be equal or greater than zero and less than one.","ref":"Axon.html#spatial_dropout/2","title":"Axon.spatial_dropout/2","type":"function"},{"doc":"Splits input graph into a container of n input graphs along the given axis. Options :name - layer name. :axis - concatenate axis. Defaults to -1 .","ref":"Axon.html#split/3","title":"Axon.split/3","type":"function"},{"doc":"Adds a subtract layer to the network. This layer performs an element-wise subtract operation on input layers. All input layers must be capable of being broadcast together. If one shape has a static batch size, all other shapes must have a static batch size as well. Options :name - layer name.","ref":"Axon.html#subtract/3","title":"Axon.subtract/3","type":"function"},{"doc":"Adds a Hyperbolic tangent activation layer to the network. See Axon.Activations.tanh/1 for more details. Options :name - layer name.","ref":"Axon.html#tanh/2","title":"Axon.tanh/2","type":"function"},{"doc":"Compiles and returns the given model's backward function expression with respect to the given loss function. The returned expression is an Nx expression which can be traversed and lowered to an IR or inspected for debugging purposes. The given loss function must be a scalar loss function which expects inputs and targets with the same shapes as the model's output shapes as determined by the model's signature. Options :debug - if true , will log graph traversal and generation metrics. Also forwarded to JIT if debug mode is available for your chosen compiler or backend. Defaults to false","ref":"Axon.html#trace_backward/5","title":"Axon.trace_backward/5","type":"function"},{"doc":"Compiles and returns the given model's forward function expression with the given options. The returned expression is an Nx expression which can be traversed and lowered to an IR or inspected for debugging purposes. Options :mode - one of :inference or :training . Forwarded to layers to control differences in compilation at training or inference time. Defaults to :inference :debug - if true , will log graph traversal and generation metrics. Also forwarded to JIT if debug mode is available for your chosen compiler or backend. Defaults to false","ref":"Axon.html#trace_forward/4","title":"Axon.trace_forward/4","type":"function"},{"doc":"Compiles and returns the given model's init function expression with the given options. The returned expression is an Nx expression which can be traversed and lowered to an IR or inspected for debugging purposes. You may optionally specify initial parameters for some layers or namespaces by passing a partial parameter map: Axon . trace_init ( model , %{ &quot;dense_0&quot; =&gt; dense_params } ) The parameter map will be merged with the initialized model parameters. Options :debug - if true , will log graph traversal and generation metrics. Also forwarded to JIT if debug mode is available for your chosen compiler or backend. Defaults to false","ref":"Axon.html#trace_init/4","title":"Axon.trace_init/4","type":"function"},{"doc":"Adds a transpose layer to the network. Options :name - layer name.","ref":"Axon.html#transpose/3","title":"Axon.transpose/3","type":"function"},{"doc":"","ref":"Axon.html#t:t/0","title":"Axon.t/0","type":"type"},{"doc":"Utilities for creating mixed precision policies. Mixed precision is useful for increasing model throughput at the possible price of a small dip in accuracy. When creating a mixed precision policy, you define the policy for params , compute , and output . The params policy dictates what type parameters should be stored as during training. The compute policy dictates what type should be used during intermediate computations in the model's forward pass. The output policy dictates what type the model should output. Here's an example of creating a mixed precision policy and applying it to a model: model = Axon . input ( &quot;input&quot; , shape : { nil , 784 } ) |&gt; Axon . dense ( 128 , activation : :relu ) |&gt; Axon . batch_norm ( ) |&gt; Axon . dropout ( rate : 0.5 ) |&gt; Axon . dense ( 64 , activation : :relu ) |&gt; Axon . batch_norm ( ) |&gt; Axon . dropout ( rate : 0.5 ) |&gt; Axon . dense ( 10 , activation : :softmax ) policy = Axon.MixedPrecision . create_policy ( params : { :f , 32 } , compute : { :f , 16 } , output : { :f , 32 } ) mp_model = model |&gt; Axon.MixedPrecision . apply_policy ( policy , except : [ :batch_norm ] ) The example above applies the mixed precision policy to every layer in the model except Batch Normalization layers. The policy will cast parameters and inputs to {:f, 16} for intermediate computations in the model's forward pass before casting the output back to {:f, 32} .","ref":"Axon.MixedPrecision.html","title":"Axon.MixedPrecision","type":"module"},{"doc":"Creates a mixed precision policy with the given options. Options params - parameter precision policy. Defaults to {:f, 32} compute - compute precision policy. Defaults to {:f, 32} output - output precision policy. Defaults to {:f, 32} Examples iex&gt; Axon.MixedPrecision . create_policy ( params : { :f , 16 } , output : { :f , 16 } ) % Policy { params : { :f , 16 } , compute : { :f , 32 } , output : { :f , 16 } } iex&gt; Axon.MixedPrecision . create_policy ( compute : { :bf , 16 } ) % Policy { params : { :f , 32 } , compute : { :bf , 16 } , output : { :f , 32 } }","ref":"Axon.MixedPrecision.html#create_policy/1","title":"Axon.MixedPrecision.create_policy/1","type":"function"},{"doc":"Represents a missing value of an optional node. See Axon.input/2 and Axon.optional/2 for more details.","ref":"Axon.None.html","title":"Axon.None","type":"module"},{"doc":"Container for returning stateful outputs from Axon layers. Some layers, such as Axon.batch_norm/2 , keep a running internal state which is updated continuously at train time and used statically at inference time. In order for the Axon compiler to differentiate ordinary layer outputs from internal state, you must mark output as stateful. Stateful Outputs consist of two fields: :output - Actual layer output to be forwarded to next layer :state - Internal layer state to be tracked and updated :output is simply forwarded to the next layer. :state is aggregated with other stateful outputs, and then is treated specially by internal Axon training functions such that update state parameters reflect returned values from stateful outputs. :state must be a map with keys that map directly to layer internal state names. For example, Axon.Layers.batch_norm returns StatefulOutput with :state keys of &quot;mean&quot; and &quot;var&quot; .","ref":"Axon.StatefulOutput.html","title":"Axon.StatefulOutput","type":"module"},{"doc":"Module for rendering various visual representations of Axon models.","ref":"Axon.Display.html","title":"Axon.Display","type":"module"},{"doc":"Traces execution of the given Axon model with the given inputs, rendering the execution flow as a mermaid flowchart. You must include kino as a dependency in your project to make use of this function. Options :direction - defines the direction of the graph visual. The value can either be :top_down or :left_right . Defaults to :top_down . Examples Given an Axon model: model = Axon . input ( &quot;input&quot; ) |&gt; Axon . dense ( 32 ) You can define input templates for each input: input = Nx . template ( { 1 , 16 } , :f32 ) And then display the execution flow of the model: Axon.Display . as_graph ( model , input , direction : :top_down )","ref":"Axon.Display.html#as_graph/3","title":"Axon.Display.as_graph/3","type":"function"},{"doc":"Traces execution of the given Axon model with the given inputs, rendering the execution flow as a table. You must include table_rex as a dependency in your project to make use of this function. Examples Given an Axon model: model = Axon . input ( &quot;input&quot; ) |&gt; Axon . dense ( 32 ) You can define input templates for each input: input = Nx . template ( { 1 , 16 } , :f32 ) And then display the execution flow of the model: Axon.Display . as_table ( model , input )","ref":"Axon.Display.html#as_table/2","title":"Axon.Display.as_table/2","type":"function"},{"doc":"Activation functions. Activation functions are element-wise, (typically) non-linear functions called on the output of another layer, such as a dense layer: x |&gt; dense ( weight , bias ) |&gt; relu ( ) Activation functions output the &quot;activation&quot; or how active a given layer's neurons are in learning a representation of the data-generating distribution. Some activations are commonly used as output activations. For example softmax is often used as the output in multiclass classification problems because it returns a categorical probability distribution: iex&gt; Axon.Activations . softmax ( Nx . tensor ( [ [ 1 , 2 , 3 ] ] , type : { :f , 32 } ) ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ [ 0.09003057330846786 , 0.2447284758090973 , 0.6652409434318542 ] ] &gt; Other activations such as tanh or sigmoid are used because they have desirable properties, such as keeping the output tensor constrained within a certain range. Generally, the choice of activation function is arbitrary; although some activations work better than others in certain problem domains. For example ReLU (rectified linear unit) activation is a widely-accepted default. You can see a list of activation functions and implementations here . All of the functions in this module are implemented as numerical functions and can be JIT or AOT compiled with any supported Nx compiler.","ref":"Axon.Activations.html","title":"Axon.Activations","type":"module"},{"doc":"Continuously-differentiable exponential linear unit activation. $$f(x_i) = \\max(0, x_i) + \\min(0, \\alpha * e^{\\frac{x_i}{\\alpha}} - 1)$$ Options alpha - $\\alpha$ in CELU formulation. Must be non-zero. Defaults to 1.0 Examples iex&gt; Axon.Activations . celu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] ) ) # Nx.Tensor &lt; f32 [ 7 ] [ - 0.9502129554748535 , - 0.8646647334098816 , - 0.6321205496788025 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . celu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } ) ) # Nx.Tensor &lt; bf16 [ 2 ] [ 3 ] [ [ - 0.62890625 , - 0.86328125 , - 0.94921875 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt; Error cases iex&gt; Axon.Activations . celu ( Nx . tensor ( [ 0.0 , 1.0 , 2.0 ] , type : { :f , 32 } ) , alpha : 0.0 ) ** (ArgumentError) :alpha must be non-zero in CELU activation References Continuously Differentiable Exponential Linear Units","ref":"Axon.Activations.html#celu/2","title":"Axon.Activations.celu/2","type":"function"},{"doc":"Exponential linear unit activation. Equivalent to celu for $\\alpha = 1$ $$f(x_i) = \\begin{cases}x_i &amp; x _i &gt; 0 \\newline \\alpha * (e^{x_i} - 1) &amp; x_i \\leq 0 \\ \\end{cases}$$ Options alpha - $\\alpha$ in ELU formulation. Defaults to 1.0 Examples iex&gt; Axon.Activations . elu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] ) ) # Nx.Tensor &lt; f32 [ 7 ] [ - 0.9502129554748535 , - 0.8646647334098816 , - 0.6321205496788025 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . elu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } ) ) # Nx.Tensor &lt; bf16 [ 2 ] [ 3 ] [ [ - 0.62890625 , - 0.86328125 , - 0.94921875 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt; References Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)","ref":"Axon.Activations.html#elu/2","title":"Axon.Activations.elu/2","type":"function"},{"doc":"Exponential activation. $$f(x_i) = e^{x_i}$$ Examples iex&gt; Axon.Activations . exp ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ 0.049787066876888275 , 0.1353352814912796 , 0.3678794503211975 , 1.0 , 2.7182817459106445 , 7.389056205749512 , 20.08553695678711 ] &gt; iex&gt; Axon.Activations . exp ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.3671875 , 0.134765625 , 0.049560546875 ] , [ 2.703125 , 7.375 , 20.0 ] ] &gt;","ref":"Axon.Activations.html#exp/1","title":"Axon.Activations.exp/1","type":"function"},{"doc":"Gaussian error linear unit activation. $$f(x_i) = \\frac{x_i}{2}(1 + {erf}(\\frac{x_i}{\\sqrt{2}}))$$ Examples iex&gt; Axon.Activations . gelu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.0040496885776519775 , - 0.04550027847290039 , - 0.15865525603294373 , 0.0 , 0.8413447141647339 , 1.9544997215270996 , 2.995950222015381 ] &gt; iex&gt; Axon.Activations . gelu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.16015625 , - 0.046875 , - 0.005859375 ] , [ 0.83984375 , 1.953125 , 2.984375 ] ] &gt; References Gaussian Error Linear Units (GELUs)","ref":"Axon.Activations.html#gelu/1","title":"Axon.Activations.gelu/1","type":"function"},{"doc":"Hard sigmoid activation. Examples iex&gt; Axon.Activations . hard_sigmoid ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ 0.0 , 0.0 , 0.0 , 0.20000000298023224 , 0.4000000059604645 , 0.6000000238418579 , 0.800000011920929 ] &gt; iex&gt; Axon.Activations . hard_sigmoid ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 7.781982421875e-4 , 0.0 , 0.0 ] , [ 0.3984375 , 0.59765625 , 0.796875 ] ] &gt;","ref":"Axon.Activations.html#hard_sigmoid/2","title":"Axon.Activations.hard_sigmoid/2","type":"function"},{"doc":"Hard sigmoid weighted linear unit activation. $$f(x_i) = \\begin{cases} 0 &amp; x_i \\leq -3 \\newline x &amp; x_i \\geq 3 \\newline \\frac{x_i^2}{6} + \\frac{x_i}{2} &amp; otherwise \\end{cases}$$ Examples iex&gt; Axon.Activations . hard_silu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.0 , - 0.0 , - 0.0 , 0.0 , 0.4000000059604645 , 1.2000000476837158 , 2.4000000953674316 ] &gt; iex&gt; Axon.Activations . hard_silu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 7.781982421875e-4 , - 0.0 , - 0.0 ] , [ 0.3984375 , 1.1953125 , 2.390625 ] ] &gt;","ref":"Axon.Activations.html#hard_silu/2","title":"Axon.Activations.hard_silu/2","type":"function"},{"doc":"Hard hyperbolic tangent activation. $$f(x_i) = \\begin{cases} 1 &amp; x &gt; 1 \\newline -1 &amp; x &lt; -1 \\newline x &amp; otherwise \\end{cases}$$ Examples iex&gt; Axon.Activations . hard_tanh ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 1.0 , - 1.0 , - 1.0 , 0.0 , 1.0 , 1.0 , 1.0 ] &gt; iex&gt; Axon.Activations . hard_tanh ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 1.0 , - 1.0 , - 1.0 ] , [ 1.0 , 1.0 , 1.0 ] ] &gt;","ref":"Axon.Activations.html#hard_tanh/1","title":"Axon.Activations.hard_tanh/1","type":"function"},{"doc":"Leaky rectified linear unit activation. $$f(x_i) = \\begin{cases} x &amp; x \\geq 0 \\newline \\alpha * x &amp; otherwise \\end{cases}$$ Options :alpha - $\\alpha$ in Leaky ReLU formulation. Defaults to 1.0e-2 Examples iex&gt; Axon.Activations . leaky_relu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) , alpha : 0.5 ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 1.5 , - 1.0 , - 0.5 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . leaky_relu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , names : [ :batch , :data ] ) , alpha : 0.5 ) # Nx.Tensor &lt; f32 [ batch : 2 ] [ data : 3 ] [ [ - 0.5 , - 1.0 , - 1.5 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt;","ref":"Axon.Activations.html#leaky_relu/2","title":"Axon.Activations.leaky_relu/2","type":"function"},{"doc":"Linear activation. $$f(x_i) = x_i$$ Examples iex&gt; Axon.Activations . linear ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . linear ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt;","ref":"Axon.Activations.html#linear/1","title":"Axon.Activations.linear/1","type":"function"},{"doc":"Log-sigmoid activation. $$f(x_i) = \\log(\\sigmoid(x))$$ Examples iex&gt; Axon.Activations . log_sigmoid ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , type : { :f , 32 } , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 3.0485873222351074 , - 2.1269280910491943 , - 1.3132617473602295 , - 0.6931471824645996 , - 0.3132616877555847 , - 0.12692801654338837 , - 0.04858734831213951 ] &gt; iex&gt; Axon.Activations . log_sigmoid ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 1.3125 , - 2.125 , - 3.046875 ] , [ - 0.3125 , - 0.1259765625 , - 0.04833984375 ] ] &gt;","ref":"Axon.Activations.html#log_sigmoid/1","title":"Axon.Activations.log_sigmoid/1","type":"function"},{"doc":"Log-softmax activation. $$f(x_i) = -log( um{e^x_i})$$ Examples iex&gt; Axon.Activations . log_softmax ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , type : { :f , 32 } , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 6.457762718200684 , - 5.457762718200684 , - 4.457762718200684 , - 3.4577627182006836 , - 2.4577627182006836 , - 1.4577628374099731 , - 0.45776283740997314 ] &gt; iex&gt; Axon.Activations . log_softmax ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.404296875 , - 1.3984375 , - 2.390625 ] , [ - 2.390625 , - 1.3984375 , - 0.404296875 ] ] &gt;","ref":"Axon.Activations.html#log_softmax/2","title":"Axon.Activations.log_softmax/2","type":"function"},{"doc":"Logsumexp activation. $$\\log(sum e^x_i)$$ Examples iex&gt; Axon.Activations . log_sumexp ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 1 ] [ 0.45776283740997314 ] &gt; iex&gt; Axon.Activations . log_sumexp ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 1 ] [ [ 0.404296875 ] , [ 0.404296875 ] ] &gt;","ref":"Axon.Activations.html#log_sumexp/2","title":"Axon.Activations.log_sumexp/2","type":"function"},{"doc":"Mish activation. $$f(x_i) = x_i* \\tanh(\\log(1 + e^x_i))$$ Examples iex&gt; Axon.Activations . mish ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , type : { :f , 32 } , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.14564745128154755 , - 0.2525014877319336 , - 0.30340147018432617 , 0.0 , 0.8650984168052673 , 1.9439589977264404 , 2.98653507232666 ] &gt; iex&gt; Axon.Activations . mish ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.30078125 , - 0.25 , - 0.1435546875 ] , [ 0.86328125 , 1.9375 , 2.96875 ] ] &gt;","ref":"Axon.Activations.html#mish/1","title":"Axon.Activations.mish/1","type":"function"},{"doc":"Rectified linear unit 6 activation. $$f(x_i) = \\min_i(\\max_i(x, 0), 6)$$ Examples iex&gt; Axon.Activations . relu6 ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] ) ) # Nx.Tensor &lt; f32 [ 7 ] [ 0.0 , 0.0 , 0.0 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . relu6 ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.0 , 0.0 , 0.0 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt; References MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications","ref":"Axon.Activations.html#relu6/1","title":"Axon.Activations.relu6/1","type":"function"},{"doc":"Rectified linear unit activation. $$f(x_i) = \\max_i(x, 0)$$ Examples iex&gt; Axon.Activations . relu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ 0.0 , 0.0 , 0.0 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . relu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.0 , 0.0 , 0.0 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt;","ref":"Axon.Activations.html#relu/1","title":"Axon.Activations.relu/1","type":"function"},{"doc":"Scaled exponential linear unit activation. $$f(x_i) = \\begin{cases} \\lambda x &amp; x \\geq 0 \\newline \\lambda \\alpha(e^{x} - 1) &amp; x &lt; 0 \\end{cases}$$ $$\\alpha \\approx 1.6733$$ $$\\lambda \\approx 1.0507$$ Examples iex&gt; Axon.Activations . selu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 1.670568823814392 , - 1.5201665163040161 , - 1.1113307476043701 , 0.0 , 1.0507010221481323 , 2.1014020442962646 , 3.1521029472351074 ] &gt; iex&gt; Axon.Activations . selu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 1.09375 , - 1.5078125 , - 1.6640625 ] , [ 1.046875 , 2.09375 , 3.140625 ] ] &gt; References Self-Normalizing Neural Networks","ref":"Axon.Activations.html#selu/2","title":"Axon.Activations.selu/2","type":"function"},{"doc":"Sigmoid activation. $$f(x_i) = \\frac{1}{1 + e^{-x_i}}$$ Implementation Note: Sigmoid logits are cached as metadata in the expression and can be used in calculations later on. For example, they are used in cross-entropy calculations for better stability. Examples iex&gt; Axon.Activations . sigmoid ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ 0.04742587357759476 , 0.11920291930437088 , 0.2689414322376251 , 0.5 , 0.7310585975646973 , 0.8807970881462097 , 0.9525741338729858 ] &gt; iex&gt; Axon.Activations . sigmoid ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.267578125 , 0.119140625 , 0.04736328125 ] , [ 0.73046875 , 0.87890625 , 0.94921875 ] ] &gt;","ref":"Axon.Activations.html#sigmoid/1","title":"Axon.Activations.sigmoid/1","type":"function"},{"doc":"Sigmoid weighted linear unit activation. $$f(x_i) = x\\sigmoid(x)$$ Examples iex&gt; Axon.Activations . silu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.14227762818336487 , - 0.23840583860874176 , - 0.2689414322376251 , 0.0 , 0.7310585975646973 , 1.7615941762924194 , 2.857722282409668 ] &gt; iex&gt; Axon.Activations . silu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.267578125 , - 0.23828125 , - 0.1416015625 ] , [ 0.73046875 , 1.7578125 , 2.84375 ] ] &gt; References Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning","ref":"Axon.Activations.html#silu/1","title":"Axon.Activations.silu/1","type":"function"},{"doc":"Softmax activation along an axis. $$\\frac{e^{x_i}}{\\sum_i e^{x_i}}$$ Implementation Note: Softmax logits are cached as metadata in the expression and can be used in calculations later on. For example, they are used in cross-entropy calculations for better stability. Options :axis - softmax axis along which to calculate distribution. Defaults to 1. Examples iex&gt; Axon.Activations . softmax ( Nx . tensor ( [ [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] ] , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; f32 [ batch : 1 ] [ data : 7 ] [ [ 0.0015683004166930914 , 0.004263082519173622 , 0.011588259600102901 , 0.03150015324354172 , 0.08562629669904709 , 0.23275642096996307 , 0.6326975226402283 ] ] &gt; iex&gt; Axon.Activations . softmax ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.6640625 , 0.2431640625 , 0.08935546875 ] , [ 0.08935546875 , 0.2431640625 , 0.6640625 ] ] &gt;","ref":"Axon.Activations.html#softmax/2","title":"Axon.Activations.softmax/2","type":"function"},{"doc":"Softplus activation. $$\\log(1 + e^x_i)$$ Examples iex&gt; Axon.Activations . softplus ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ 0.04858734831213951 , 0.12692801654338837 , 0.3132616877555847 , 0.6931471824645996 , 1.3132617473602295 , 2.1269280910491943 , 3.0485873222351074 ] &gt; iex&gt; Axon.Activations . softplus ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.3125 , 0.1259765625 , 0.04833984375 ] , [ 1.3125 , 2.125 , 3.046875 ] ] &gt;","ref":"Axon.Activations.html#softplus/1","title":"Axon.Activations.softplus/1","type":"function"},{"doc":"Softsign activation. $$f(x_i) = \\frac{x_i}{|x_i| + 1}$$ Examples iex&gt; Axon.Activations . softsign ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.75 , - 0.6666666865348816 , - 0.5 , 0.0 , 0.5 , 0.6666666865348816 , 0.75 ] &gt; iex&gt; Axon.Activations . softsign ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.5 , - 0.6640625 , - 0.75 ] , [ 0.5 , 0.6640625 , 0.75 ] ] &gt;","ref":"Axon.Activations.html#softsign/1","title":"Axon.Activations.softsign/1","type":"function"},{"doc":"Hyperbolic tangent activation. $$f(x_i) = \\tanh(x_i)$$ Examples iex&gt; Axon.Activations . tanh ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.9950547814369202 , - 0.9640275835990906 , - 0.7615941762924194 , 0.0 , 0.7615941762924194 , 0.9640275835990906 , 0.9950547814369202 ] &gt; iex&gt; Axon.Activations . tanh ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.7578125 , - 0.9609375 , - 0.9921875 ] , [ 0.7578125 , 0.9609375 , 0.9921875 ] ] &gt;","ref":"Axon.Activations.html#tanh/1","title":"Axon.Activations.tanh/1","type":"function"},{"doc":"Parameter initializers. Parameter initializers are used to initialize the weights and biases of a neural network. Because most deep learning optimization algorithms are iterative, they require an initial point to iterate from. Sometimes the initialization of a model can determine whether or not a model converges. In some cases, the initial point is unstable, and therefore the model has no chance of converging using common first-order optimization methods. In cases where the model will converge, initialization can have a significant impact on how quickly the model converges. Most initialization strategies are built from intuition and heuristics rather than theory. It's commonly accepted that the parameters of different layers should be different - motivating the use of random initialization for each layer's parameters. Usually, only the weights of a layer are initialized using a random distribution - while the biases are initialized to a uniform constant (like 0). Most initializers use Gaussian (normal) or uniform distributions with variations on scale. The output scale of an initializer should generally be large enough to avoid information loss but small enough to avoid exploding values. The initializers in this module have a default scale known to work well with the initialization strategy. The functions in this module return initialization functions which take shapes and types and return tensors: init_fn = Axon.Initializers . zeros ( ) init_fn . ( { 1 , 2 } , { :f , 32 } ) You may use these functions from within defn or outside.","ref":"Axon.Initializers.html","title":"Axon.Initializers","type":"module"},{"doc":"Initializes parameters to value. Examples iex&gt; init_fn = Axon.Initializers . full ( 1.00 ) iex&gt; out = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; out # Nx.Tensor &lt; f32 [ 2 ] [ 2 ] [ [ 1.0 , 1.0 ] , [ 1.0 , 1.0 ] ] &gt;","ref":"Axon.Initializers.html#full/1","title":"Axon.Initializers.full/1","type":"function"},{"doc":"Initializes parameters with the Glorot normal initializer. The Glorot normal initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_avg and distribution: :truncated_normal . The Glorot normal initializer is also called the Xavier normal initializer. Options :scale - scale of the output distribution. Defaults to 1.0 Examples iex&gt; init_fn = Axon.Initializers . glorot_normal ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . glorot_normal ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Understanding the difficulty of training deep feedforward neural networks","ref":"Axon.Initializers.html#glorot_normal/1","title":"Axon.Initializers.glorot_normal/1","type":"function"},{"doc":"Initializes parameters with the Glorot uniform initializer. The Glorot uniform initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_avg and distribution: :uniform . The Glorot uniform initializer is also called the Xavier uniform initializer. Options :scale - scale of the output distribution. Defaults to 1.0 Examples iex&gt; init_fn = Axon.Initializers . glorot_uniform ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . glorot_uniform ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Understanding the difficulty of training deep feedforward neural networks","ref":"Axon.Initializers.html#glorot_uniform/1","title":"Axon.Initializers.glorot_uniform/1","type":"function"},{"doc":"Initializes parameters with the He normal initializer. The He normal initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_in and distribution: :truncated_normal . Options :scale - scale of the output distribution. Defaults to 2.0 Examples iex&gt; init_fn = Axon.Initializers . he_normal ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . he_normal ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification","ref":"Axon.Initializers.html#he_normal/1","title":"Axon.Initializers.he_normal/1","type":"function"},{"doc":"Initializes parameters with the He uniform initializer. The He uniform initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_ni and distribution: :uniform . Options :scale - scale of the output distribution. Defaults to 2.0 Examples iex&gt; init_fn = Axon.Initializers . he_uniform ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . he_uniform ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification","ref":"Axon.Initializers.html#he_uniform/1","title":"Axon.Initializers.he_uniform/1","type":"function"},{"doc":"Initializes parameters to an identity matrix. Examples iex&gt; init_fn = Axon.Initializers . identity ( ) iex&gt; out = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; out # Nx.Tensor &lt; f32 [ 2 ] [ 2 ] [ [ 1.0 , 0.0 ] , [ 0.0 , 1.0 ] ] &gt;","ref":"Axon.Initializers.html#identity/0","title":"Axon.Initializers.identity/0","type":"function"},{"doc":"Initializes parameters with the Lecun normal initializer. The Lecun normal initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_in and distribution: :truncated_normal . Options :scale - scale of the output distribution. Defaults to 1.0 Examples iex&gt; init_fn = Axon.Initializers . lecun_normal ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . lecun_normal ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Efficient BackProp","ref":"Axon.Initializers.html#lecun_normal/1","title":"Axon.Initializers.lecun_normal/1","type":"function"},{"doc":"Initializes parameters with the Lecun uniform initializer. The Lecun uniform initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_in and distribution: :uniform . Options :scale - scale of the output distribution. Defaults to 1.0 Examples iex&gt; init_fn = Axon.Initializers . lecun_uniform ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . lecun_uniform ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Efficient BackProp","ref":"Axon.Initializers.html#lecun_uniform/1","title":"Axon.Initializers.lecun_uniform/1","type":"function"},{"doc":"Initializes parameters with a random normal distribution. Options :mean - mean of the output distribution. Defaults to 0.0 :scale - scale of the output distribution. Defaults to 1.0e-2 Examples iex&gt; init_fn = Axon.Initializers . normal ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . normal ( mean : 1.0 , scale : 1.0 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 }","ref":"Axon.Initializers.html#normal/1","title":"Axon.Initializers.normal/1","type":"function"},{"doc":"Initializes parameters to 1. Examples iex&gt; init_fn = Axon.Initializers . ones ( ) iex&gt; out = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; out # Nx.Tensor &lt; f32 [ 2 ] [ 2 ] [ [ 1.0 , 1.0 ] , [ 1.0 , 1.0 ] ] &gt;","ref":"Axon.Initializers.html#ones/0","title":"Axon.Initializers.ones/0","type":"function"},{"doc":"Initializes a tensor with an orthogonal distribution. For 2-D tensors, the initialization is generated through the QR decomposition of a random distribution For tensors with more than 2 dimensions, a 2-D tensor with shape {shape[0] * shape[1] * ... * shape[n-2], shape[n-1]} is initialized and then reshaped accordingly. Options :distribution - output distribution. One of [ :normal , :uniform ]. Defaults to :normal Examples iex&gt; init_fn = Axon.Initializers . orthogonal ( ) iex&gt; t = init_fn . ( { 3 , 3 } , { :f , 32 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; Nx . shape ( t ) { 3 , 3 } iex&gt; init_fn = Axon.Initializers . orthogonal ( ) iex&gt; t = init_fn . ( { 1 , 2 , 3 , 4 } , { :f , 64 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . type ( t ) { :f , 64 } iex&gt; Nx . shape ( t ) { 1 , 2 , 3 , 4 }","ref":"Axon.Initializers.html#orthogonal/1","title":"Axon.Initializers.orthogonal/1","type":"function"},{"doc":"Initializes parameters with a random uniform distribution. Options :scale - scale of the output distribution. Defaults to 1.0e-2 Examples iex&gt; init_fn = Axon.Initializers . uniform ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . uniform ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 }","ref":"Axon.Initializers.html#uniform/1","title":"Axon.Initializers.uniform/1","type":"function"},{"doc":"Initializes parameters with variance scaling according to the given distribution and mode. Variance scaling adapts scale to the weights of the output tensor. Options :scale - scale of the output distribution. Defaults to 1.0e-2 :mode - compute fan mode. One of :fan_in , :fan_out , or :fan_avg . Defaults to :fan_in :distribution - output distribution. One of :normal , :truncated_normal , or :uniform . Defaults to :normal Examples iex&gt; init_fn = Axon.Initializers . variance_scaling ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . variance_scaling ( mode : :fan_out , distribution : :truncated_normal ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } iex&gt; init_fn = Axon.Initializers . variance_scaling ( mode : :fan_out , distribution : :normal ) iex&gt; t = init_fn . ( { 64 , 3 , 32 , 32 } , { :f , 32 } , Nx.Random . key ( 1 ) ) iex&gt; Nx . shape ( t ) { 64 , 3 , 32 , 32 } iex&gt; Nx . type ( t ) { :f , 32 }","ref":"Axon.Initializers.html#variance_scaling/1","title":"Axon.Initializers.variance_scaling/1","type":"function"},{"doc":"Initializes parameters to 0. Examples iex&gt; init_fn = Axon.Initializers . zeros ( ) iex&gt; out = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; out # Nx.Tensor &lt; f32 [ 2 ] [ 2 ] [ [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] ] &gt;","ref":"Axon.Initializers.html#zeros/0","title":"Axon.Initializers.zeros/0","type":"function"},{"doc":"Functional implementations of common neural network layer operations. Layers are the building blocks of neural networks. These functional implementations can be used to express higher-level constructs using fundamental building blocks. Neural network layers are stateful with respect to their parameters. These implementations do not assume the responsibility of managing state - instead opting to delegate this responsibility to the caller. Basic neural networks can be seen as a composition of functions: input |&gt; dense ( w1 , b1 ) |&gt; relu ( ) |&gt; dense ( w2 , b2 ) |&gt; softmax ( ) These kinds of models are often referred to as deep feedforward networks or multilayer perceptrons (MLPs) because information flows forward through the network with no feedback connections. Mathematically, a feedforward network can be represented as: $$f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$$ You can see a similar pattern emerge if we condense the call stack in the previous example: softmax ( dense ( relu ( dense ( input , w1 , b1 ) ) , w2 , b2 ) ) The chain structure shown here is the most common structure used in neural networks. You can consider each function $f^{(n)}$ as a layer in the neural network - for example $f^{(2)} is the 2nd layer in the network. The number of function calls in the structure is the depth of the network. This is where the term deep learning comes from. Neural networks are often written as the mapping: $$y = f(x; \\theta)$$ Where $x$ is the input to the neural network and $\\theta$ are the set of learned parameters. In Elixir, you would write this: y = model ( input , params ) From the previous example, params would represent the collection: { w1 , b1 , w2 , b2 } where w1 and w2 are layer kernels , and b1 and b2 are layer biases .","ref":"Axon.Layers.html","title":"Axon.Layers","type":"module"},{"doc":"Functional implementation of general dimensional adaptive average pooling. Adaptive pooling allows you to specify the desired output size of the transformed input. This will automatically adapt the window size and strides to obtain the desired output size. It will then perform average pooling using the calculated window size and strides. Adaptive pooling can be useful when working on multiple inputs with different spatial input shapes. You can guarantee the output of an adaptive pooling operation is always the same size regardless of input shape. Options :output_size - spatial output size. Must be a tuple with size equal to the spatial dimensions in the input tensor. Required. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.Layers.html#adaptive_avg_pool/2","title":"Axon.Layers.adaptive_avg_pool/2","type":"function"},{"doc":"Functional implementation of general dimensional adaptive power average pooling. Computes: $$f(X) = qrt[p]{ um_{x in X} x^{p}}$$ Adaptive pooling allows you to specify the desired output size of the transformed input. This will automatically adapt the window size and strides to obtain the desired output size. It will then perform max pooling using the calculated window size and strides. Adaptive pooling can be useful when working on multiple inputs with different spatial input shapes. You can guarantee the output of an adaptive pooling operation is always the same size regardless of input shape. Options :norm - $p$ from above equation. Defaults to 2. :output_size - spatial output size. Must be a tuple with size equal to the spatial dimensions in the input tensor. Required.","ref":"Axon.Layers.html#adaptive_lp_pool/2","title":"Axon.Layers.adaptive_lp_pool/2","type":"function"},{"doc":"Functional implementation of general dimensional adaptive max pooling. Adaptive pooling allows you to specify the desired output size of the transformed input. This will automatically adapt the window size and strides to obtain the desired output size. It will then perform max pooling using the calculated window size and strides. Adaptive pooling can be useful when working on multiple inputs with different spatial input shapes. You can guarantee the output of an adaptive pooling operation is always the same size regardless of input shape. Options :output_size - spatial output size. Must be a tuple with size equal to the spatial dimensions in the input tensor. Required.","ref":"Axon.Layers.html#adaptive_max_pool/2","title":"Axon.Layers.adaptive_max_pool/2","type":"function"},{"doc":"Functional implementation of an alpha dropout layer. Alpha dropout is a type of dropout that forces the input to have zero mean and unit standard deviation. Randomly masks some elements and scales to enforce self-normalization. Options :rate - dropout rate. Used to determine probability a connection will be dropped. Required. # :noise_shape - input noise shape. Shape of mask which can be useful for broadcasting ` mask ` across feature channels or other dimensions . Defaults to shape of input tensor . References Self-Normalizing Neural Networks","ref":"Axon.Layers.html#alpha_dropout/2","title":"Axon.Layers.alpha_dropout/2","type":"function"},{"doc":"A general dimensional functional average pooling layer. Pooling is applied to the spatial dimension of the input tensor. Average pooling returns the average of all elements in valid windows in the input tensor. It is often used after convolutional layers to downsample the input even further. Options kernel_size - window size. Rank must match spatial dimension of the input tensor. Required. :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :window_dilations - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Can be scalar or list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.Layers.html#avg_pool/2","title":"Axon.Layers.avg_pool/2","type":"function"},{"doc":"Functional implementation of batch normalization. Normalizes the input by calculating mean and variance of the input tensor along every dimension but the given :channel_index , and then scaling according to: $$y = \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} * \\gamma + \\beta$$ gamma and beta are often trainable parameters. If training? is true, this method will compute a new mean and variance, and return the updated ra_mean and ra_var . Otherwise, it will just compute batch norm from the given ra_mean and ra_var. Options :epsilon - numerical stability term. $epsilon$ in the above formulation. :channel_index - channel index used to determine reduction axes for mean and variance calculation. :momentum - momentum to use for EMA update. :training? - if true, uses training mode batch norm. Defaults to false. References Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","ref":"Axon.Layers.html#batch_norm/6","title":"Axon.Layers.batch_norm/6","type":"function"},{"doc":"Functional implementation of a bilinear layer. Bilinear transformation of the input such that: $$y = x_1^{T}Ax_2 + b$$ Parameter Shapes input1 - {batch_size, ..., input1_features} input2 - {batch_size, ..., input2_features} kernel - {out_features, input1_features, input2_features} Output Shape {batch_size, ..., output_features} Examples iex&gt; inp1 = Nx . iota ( { 3 , 2 } , type : { :f , 32 } ) iex&gt; inp2 = Nx . iota ( { 3 , 4 } , type : { :f , 32 } ) iex&gt; kernel = Nx . iota ( { 1 , 2 , 4 } , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( 1.0 ) iex&gt; Axon.Layers . bilinear ( inp1 , inp2 , kernel , bias ) # Nx.Tensor &lt; f32 [ 3 ] [ 1 ] [ [ 39.0 ] , [ 455.0 ] , [ 1319.0 ] ] &gt;","ref":"Axon.Layers.html#bilinear/5","title":"Axon.Layers.bilinear/5","type":"function"},{"doc":"","ref":"Axon.Layers.html#celu/2","title":"Axon.Layers.celu/2","type":"function"},{"doc":"Functional implementation of a general dimensional convolutional layer. Convolutional layers can be described as applying a convolution over an input signal composed of several input planes. Intuitively, the input kernel slides output_channels number of filters over the input tensor to extract features from the input tensor. Convolutional layers are most commonly used in computer vision, but can also be useful when working with sequences and other input signals. Parameter Shapes input - {batch_size, input_channels, input_spatial0, ..., input_spatialN} kernel - {output_channels, input_channels, kernel_spatial0, ..., kernel_spatialN} bias - {} or {output_channels} Options :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :input_dilation - input dilation factor. Equivalent to applying interior padding on the input. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :kernel_dilation - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . Examples One-dimensional convolution iex&gt; input = Nx . tensor ( [ [ [ 0.1294 , - 0.6638 , 1.0251 ] ] , [ [ 0.9182 , 1.1512 , - 1.6149 ] ] ] , type : { :f , 32 } ) iex&gt; kernel = Nx . tensor ( [ [ [ - 1.5475 , 1.2425 ] ] , [ [ 0.1871 , 0.5458 ] ] , [ [ - 0.4488 , 0.8879 ] ] ] , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( [ 0.7791 , 0.1676 , 1.5971 ] , type : { :f , 32 } ) iex&gt; Axon.Layers . conv ( input , kernel , bias , channels : :first ) # Nx.Tensor &lt; f32 [ 2 ] [ 3 ] [ 2 ] [ [ [ - 0.24591797590255737 , 3.08001708984375 ] , [ - 0.1704912781715393 , 0.6029025316238403 ] , [ 0.9496372938156128 , 2.80519962310791 ] ] , [ [ 0.7885514497756958 , - 3.0088953971862793 ] , [ 0.9677201509475708 , - 0.4984228312969208 ] , [ 2.207162380218506 , - 0.3534282445907593 ] ] ] &gt; Two-dimensional convolution iex&gt; input = Nx . tensor ( [ [ [ [ - 1.0476 , - 0.5041 ] , [ - 0.9336 , 1.5907 ] ] ] ] , type : { :f , 32 } ) iex&gt; kernel = Nx . tensor ( [ ...&gt; [ [ [ 0.7514 , 0.7356 ] , [ 1.3909 , 0.6800 ] ] ] , ...&gt; [ [ [ - 0.3450 , 0.4551 ] , [ - 0.6275 , - 0.9875 ] ] ] , ...&gt; [ [ [ 1.8587 , 0.4722 ] , [ 0.6058 , - 1.0301 ] ] ] ...&gt; ] , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( [ 1.9564 , 0.2822 , - 0.5385 ] , type : { :f , 32 } ) iex&gt; Axon.Layers . conv ( input , kernel , bias , channels : :first ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ 1 ] [ 1 ] [ [ [ [ 0.5815491676330566 ] ] , [ [ - 0.5707762241363525 ] ] , [ [ - 4.927865028381348 ] ] ] ] &gt; Three-dimensional convolution iex&gt; input = Nx . tensor ( [ [ [ [ [ - 0.6497 ] , [ 1.0939 ] ] , [ [ - 2.5465 ] , [ 0.7801 ] ] ] ] ] , type : { :f , 32 } ) iex&gt; kernel = Nx . tensor ( [ ...&gt; [ [ [ [ 0.7390 ] , [ - 0.0927 ] ] , [ [ - 0.8675 ] , [ - 0.9209 ] ] ] ] , ...&gt; [ [ [ [ - 0.6638 ] , [ 0.4341 ] ] , [ [ 0.6368 ] , [ 1.1846 ] ] ] ] ...&gt; ] , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( [ - 0.4101 , 0.1776 ] , type : { :f , 32 } ) iex&gt; Axon.Layers . conv ( input , kernel , bias , channels : :first ) # Nx.Tensor &lt; f32 [ 1 ] [ 2 ] [ 1 ] [ 1 ] [ 1 ] [ [ [ [ [ 0.49906185269355774 ] ] ] , [ [ [ 0.38622811436653137 ] ] ] ] ] &gt;","ref":"Axon.Layers.html#conv/4","title":"Axon.Layers.conv/4","type":"function"},{"doc":"","ref":"Axon.Layers.html#conv_lstm/6","title":"Axon.Layers.conv_lstm/6","type":"function"},{"doc":"ConvLSTM Cell. When combined with Axon.Layers.*_unroll , implements a ConvLSTM-based RNN. More memory efficient than traditional LSTM. Options :strides - convolution strides. Defaults to 1 . :padding - convolution padding. Defaults to :same . References Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting","ref":"Axon.Layers.html#conv_lstm_cell/6","title":"Axon.Layers.conv_lstm_cell/6","type":"function"},{"doc":"Functional implementation of a general dimensional transposed convolutional layer. Note: This layer is currently implemented as a fractionally strided convolution by padding the input tensor. Please open an issue if you'd like this behavior changed. Transposed convolutions are sometimes (incorrectly) referred to as deconvolutions because it &quot;reverses&quot; the spatial dimensions of a normal convolution. Transposed convolutions are a form of upsampling - they produce larger spatial dimensions than the input tensor. They can be thought of as a convolution in reverse - and are sometimes implemented as the backward pass of a normal convolution. Options :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :input_dilation - input dilation factor. Equivalent to applying interior padding on the input. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :kernel_dilation - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . Examples iex&gt; input = Nx . iota ( { 1 , 3 , 3 } , type : { :f , 32 } ) iex&gt; kernel = Nx . iota ( { 6 , 3 , 2 } , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( 1.0 , type : { :f , 32 } ) iex&gt; Axon.Layers . conv_transpose ( input , kernel , bias , channels : :first ) # Nx.Tensor &lt; f32 [ 1 ] [ 6 ] [ 4 ] [ [ [ 40.0 , 79.0 , 94.0 , 43.0 ] , [ 94.0 , 205.0 , 256.0 , 133.0 ] , [ 148.0 , 331.0 , 418.0 , 223.0 ] , [ 202.0 , 457.0 , 580.0 , 313.0 ] , [ 256.0 , 583.0 , 742.0 , 403.0 ] , [ 310.0 , 709.0 , 904.0 , 493.0 ] ] ] &gt; References A guide to convolution arithmetic for deep learning Deconvolutional Networks","ref":"Axon.Layers.html#conv_transpose/4","title":"Axon.Layers.conv_transpose/4","type":"function"},{"doc":"Functional implementation of a dense layer. Linear transformation of the input such that: $$y = xW^T + b$$ A dense layer or fully connected layer transforms the input using the given kernel matrix and bias to compute: Nx . dot ( input , kernel ) + bias Typically, both kernel and bias are learnable parameters trained using gradient-based optimization. Parameter Shapes input - {batch_size, * input_features} kernel - {input_features, output_features} bias - {} or {output_features} Output Shape {batch_size, *, output_features} Examples iex&gt; input = Nx . tensor ( [ [ 1.0 , 0.5 , 1.0 , 0.5 ] , [ 0.0 , 0.0 , 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; kernel = Nx . tensor ( [ [ 0.2 ] , [ 0.3 ] , [ 0.5 ] , [ 0.8 ] ] , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( [ 1.0 ] , type : { :f , 32 } ) iex&gt; Axon.Layers . dense ( input , kernel , bias ) # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] [ [ 2.25 ] , [ 1.0 ] ] &gt;","ref":"Axon.Layers.html#dense/4","title":"Axon.Layers.dense/4","type":"function"},{"doc":"Functional implementation of a general dimensional depthwise convolution. Depthwise convolutions apply a single convolutional filter to each input channel. This is done by setting feature_group_size equal to the number of input channels. This will split the output_channels into input_channels number of groups and convolve the grouped kernel channels over the corresponding input channel. Parameter Shapes input - {batch_size, input_channels, input_spatial0, ..., input_spatialN} kernel - {output_channels, 1, kernel_spatial0, ..., kernel_spatialN} bias - {output_channels} or {} output_channels must be a multiple of the input channels. Options :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :input_dilation - input dilation factor. Equivalent to applying interior padding on the input. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :kernel_dilation - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.Layers.html#depthwise_conv/4","title":"Axon.Layers.depthwise_conv/4","type":"function"},{"doc":"Functional implementation of a dropout layer. Applies a mask to some elements of the input tensor with probability rate and scales the input tensor by a factor of $\\frac{1}{1 - rate}$. Dropout is a form of regularization that helps prevent overfitting by preventing models from becoming too reliant on certain connections. Dropout can somewhat be thought of as learning an ensemble of models with random connections masked. Options :rate - dropout rate. Used to determine probability a connection will be dropped. Required. :noise_shape - input noise shape. Shape of mask which can be useful for broadcasting mask across feature channels or other dimensions. Defaults to shape of input tensor. References Dropout: A Simple Way to Prevent Neural Networks from Overfitting","ref":"Axon.Layers.html#dropout/2","title":"Axon.Layers.dropout/2","type":"function"},{"doc":"Dynamically unrolls an RNN. Unrolls implement a scan operation which applies a transformation on the leading axis of input_sequence carrying some state. In this instance cell_fn is an RNN cell function such as lstm_cell or gru_cell . This function will make use of an defn while-loop such and thus may be more efficient for long sequences.","ref":"Axon.Layers.html#dynamic_unroll/6","title":"Axon.Layers.dynamic_unroll/6","type":"function"},{"doc":"","ref":"Axon.Layers.html#elu/2","title":"Axon.Layers.elu/2","type":"function"},{"doc":"Computes embedding by treating kernel matrix as a lookup table for discrete tokens. input is a vector of discrete values, typically representing tokens (e.g. words, characters, etc.) from a vocabulary. kernel is a kernel matrix of shape {vocab_size, embedding_size} from which the dense embeddings will be drawn. Parameter Shapes input - {batch_size, ..., seq_len} kernel - {vocab_size, embedding_size} Examples iex&gt; input = Nx . tensor ( [ [ 1 , 2 , 4 , 5 ] , [ 4 , 3 , 2 , 9 ] ] ) iex&gt; kernels = Nx . tensor ( [ ...&gt; [ 0.46299999952316284 , 0.5562999844551086 , 0.18170000612735748 ] , ...&gt; [ 0.9801999926567078 , 0.09780000150203705 , 0.5333999991416931 ] , ...&gt; [ 0.6980000138282776 , 0.9240999817848206 , 0.23479999601840973 ] , ...&gt; [ 0.31929999589920044 , 0.42250001430511475 , 0.7865999937057495 ] , ...&gt; [ 0.5519000291824341 , 0.5662999749183655 , 0.20559999346733093 ] , ...&gt; [ 0.1898999959230423 , 0.9311000108718872 , 0.8356000185012817 ] , ...&gt; [ 0.6383000016212463 , 0.8794000148773193 , 0.5282999873161316 ] , ...&gt; [ 0.9523000121116638 , 0.7597000002861023 , 0.08250000327825546 ] , ...&gt; [ 0.6622999906539917 , 0.02329999953508377 , 0.8205999732017517 ] , ...&gt; [ 0.9855999946594238 , 0.36419999599456787 , 0.5372999906539917 ] ...&gt; ] ) iex&gt; Axon.Layers . embedding ( input , kernels ) # Nx.Tensor &lt; f32 [ 2 ] [ 4 ] [ 3 ] [ [ [ 0.9801999926567078 , 0.09780000150203705 , 0.5333999991416931 ] , [ 0.6980000138282776 , 0.9240999817848206 , 0.23479999601840973 ] , [ 0.5519000291824341 , 0.5662999749183655 , 0.20559999346733093 ] , [ 0.1898999959230423 , 0.9311000108718872 , 0.8356000185012817 ] ] , [ [ 0.5519000291824341 , 0.5662999749183655 , 0.20559999346733093 ] , [ 0.31929999589920044 , 0.42250001430511475 , 0.7865999937057495 ] , [ 0.6980000138282776 , 0.9240999817848206 , 0.23479999601840973 ] , [ 0.9855999946594238 , 0.36419999599456787 , 0.5372999906539917 ] ] ] &gt;","ref":"Axon.Layers.html#embedding/3","title":"Axon.Layers.embedding/3","type":"function"},{"doc":"Functional implementation of a feature alpha dropout layer. Feature alpha dropout applies dropout in the same manner as spatial dropout; however, it also enforces self-normalization by masking inputs with the SELU activation function and scaling unmasked inputs. Options :rate - dropout rate. Used to determine probability a connection will be dropped. Required. # :noise_shape - input noise shape. Shape of mask which can be useful for broadcasting ` mask ` across feature channels or other dimensions . Defaults to shape of input tensor .","ref":"Axon.Layers.html#feature_alpha_dropout/2","title":"Axon.Layers.feature_alpha_dropout/2","type":"function"},{"doc":"Flattens input to shape of {batch, units} by folding outer dimensions. Examples iex&gt; Axon.Layers . flatten ( Nx . iota ( { 1 , 2 , 2 } , type : { :f , 32 } ) ) # Nx.Tensor &lt; f32 [ 1 ] [ 4 ] [ [ 0.0 , 1.0 , 2.0 , 3.0 ] ] &gt;","ref":"Axon.Layers.html#flatten/2","title":"Axon.Layers.flatten/2","type":"function"},{"doc":"Functional implementation of global average pooling which averages across the spatial dimensions of the input such that the only remaining dimensions are the batch and feature dimensions. Assumes data is configured in a channels-first like format. Parameter Shapes input - {batch_size, features, s1, ..., sN} Options :keep_axes - option to keep reduced axes with size 1 for each reduced dimensions. Defaults to false Examples iex&gt; Axon.Layers . global_avg_pool ( Nx . iota ( { 3 , 2 , 3 } , type : { :f , 32 } ) , channels : :first ) # Nx.Tensor &lt; f32 [ 3 ] [ 2 ] [ [ 1.0 , 4.0 ] , [ 7.0 , 10.0 ] , [ 13.0 , 16.0 ] ] &gt; iex&gt; Axon.Layers . global_avg_pool ( Nx . iota ( { 1 , 3 , 2 , 2 } , type : { :f , 32 } ) , channels : :first , keep_axes : true ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ 1 ] [ 1 ] [ [ [ [ 1.5 ] ] , [ [ 5.5 ] ] , [ [ 9.5 ] ] ] ] &gt;","ref":"Axon.Layers.html#global_avg_pool/2","title":"Axon.Layers.global_avg_pool/2","type":"function"},{"doc":"Functional implementation of global LP pooling which computes the following function across spatial dimensions of the input: $$f(X) = qrt[p]{ um_{x in X} x^{p}}$$ Where $p$ is given by the keyword argument :norm . As $p$ approaches infinity, it becomes equivalent to max pooling. Assumes data is configured in a channels-first like format. Parameter Shapes input - {batch_size, s1, ..., sN, features} Options :keep_axes - option to keep reduced axes with size 1 for each reduced dimensions. Defaults to false :norm - $p$ in above function. Defaults to 2 Examples iex&gt; Axon.Layers . global_lp_pool ( Nx . iota ( { 3 , 2 , 3 } , type : { :f , 32 } ) , norm : 1 , channels : :first ) # Nx.Tensor &lt; f32 [ 3 ] [ 2 ] [ [ 3.0 , 12.0 ] , [ 21.0 , 30.0 ] , [ 39.0 , 48.0 ] ] &gt; iex&gt; Axon.Layers . global_lp_pool ( Nx . iota ( { 1 , 3 , 2 , 2 } , type : { :f , 16 } ) , keep_axes : true , channels : :first ) # Nx.Tensor &lt; f16 [ 1 ] [ 3 ] [ 1 ] [ 1 ] [ [ [ [ 3.7421875 ] ] , [ [ 11.2265625 ] ] , [ [ 19.125 ] ] ] ] &gt;","ref":"Axon.Layers.html#global_lp_pool/2","title":"Axon.Layers.global_lp_pool/2","type":"function"},{"doc":"Functional implementation of global max pooling which computes maximums across the spatial dimensions of the input such that the only remaining dimensions are the batch and feature dimensions. Assumes data is configured in a channels-first like format. Parameter Shapes input - {batch_size, s1, ..., sN, features} Options :keep_axes - option to keep reduced axes with size 1 for each reduced dimensions. Defaults to false Examples iex&gt; Axon.Layers . global_max_pool ( Nx . iota ( { 3 , 2 , 3 } , type : { :f , 32 } ) , channels : :first ) # Nx.Tensor &lt; f32 [ 3 ] [ 2 ] [ [ 2.0 , 5.0 ] , [ 8.0 , 11.0 ] , [ 14.0 , 17.0 ] ] &gt; iex&gt; Axon.Layers . global_max_pool ( Nx . iota ( { 1 , 3 , 2 , 2 } , type : { :f , 32 } ) , keep_axes : true , channels : :first ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ 1 ] [ 1 ] [ [ [ [ 3.0 ] ] , [ [ 7.0 ] ] , [ [ 11.0 ] ] ] ] &gt;","ref":"Axon.Layers.html#global_max_pool/2","title":"Axon.Layers.global_max_pool/2","type":"function"},{"doc":"Functional implementation of group normalization. Normalizes the input by reshaping input into :num_groups groups and then calculating the mean and variance along every dimension but the input batch dimension. $$y = rac{x - E[x]}{ qrt{Var[x] + \epsilon}} * gamma + \beta$$ gamma and beta are often trainable parameters. This method does not maintain an EMA of mean and variance. Options :num_groups - Number of groups. :epsilon - numerical stability term. $epsilon$ in the above formulation. :channel_index - channel index used to determine reduction axes and group shape for mean and variance calculation. References Group Normalization","ref":"Axon.Layers.html#group_norm/4","title":"Axon.Layers.group_norm/4","type":"function"},{"doc":"","ref":"Axon.Layers.html#gru/6","title":"Axon.Layers.gru/6","type":"function"},{"doc":"GRU Cell. When combined with Axon.Layers.*_unroll , implements a GRU-based RNN. More memory efficient than traditional LSTM. References Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling","ref":"Axon.Layers.html#gru_cell/7","title":"Axon.Layers.gru_cell/7","type":"function"},{"doc":"","ref":"Axon.Layers.html#hard_sigmoid/2","title":"Axon.Layers.hard_sigmoid/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#hard_silu/2","title":"Axon.Layers.hard_silu/2","type":"function"},{"doc":"Functional implementation of instance normalization. Normalizes the input by calculating mean and variance of the input tensor along the spatial dimensions of the input. $$y = rac{x - E[x]}{ qrt{Var[x] + \epsilon}} * gamma + \beta$$ gamma and beta are often trainable parameters. If training? is true, this method will compute a new mean and variance, and return the updated ra_mean and ra_var . Otherwise, it will just compute batch norm from the given ra_mean and ra_var. Options :epsilon - numerical stability term. $epsilon$ in the above formulation. :channel_index - channel index used to determine reduction axes for mean and variance calculation. :momentum - momentum to use for EMA update. :training? - if true, uses training mode batch norm. Defaults to false. References Instance Normalization: The Missing Ingredient for Fast Stylization","ref":"Axon.Layers.html#instance_norm/6","title":"Axon.Layers.instance_norm/6","type":"function"},{"doc":"Functional implementation of layer normalization. Normalizes the input by calculating mean and variance of the input tensor along the given feature dimension :channel_index . $$y = \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} * \\gamma + \\beta$$ gamma and beta are often trainable parameters. This method does not maintain an EMA of mean and variance. Options :epsilon - numerical stability term. $epsilon$ in the above formulation. :channel_index - channel index used to determine reduction axes for mean and variance calculation.","ref":"Axon.Layers.html#layer_norm/4","title":"Axon.Layers.layer_norm/4","type":"function"},{"doc":"","ref":"Axon.Layers.html#leaky_relu/2","title":"Axon.Layers.leaky_relu/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#log_softmax/2","title":"Axon.Layers.log_softmax/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#log_sumexp/2","title":"Axon.Layers.log_sumexp/2","type":"function"},{"doc":"Functional implementation of a general dimensional power average pooling layer. Pooling is applied to the spatial dimension of the input tensor. Power average pooling computes the following function on each valid window of the input tensor: $$f(X) = \\sqrt[p]{\\sum_{x \\in X} x^{p}}$$ Where $p$ is given by the keyword argument :norm . As $p$ approaches infinity, it becomes equivalent to max pooling. Options :norm - $p$ from above equation. Defaults to 2. :kernel_size - window size. Rank must match spatial dimension of the input tensor. Required. :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to size of kernel. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :window_dilations - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Can be scalar or list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . Examples iex&gt; t = Nx . tensor ( [ [ [ 0.9450 , 0.4684 , 1.8146 ] , [ 1.2663 , 0.4354 , - 0.0781 ] , [ - 0.4759 , 0.3251 , 0.8742 ] ] ] , type : { :f , 32 } ) iex&gt; Axon.Layers . lp_pool ( t , kernel_size : 2 , norm : 2 , channels : :first ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ 1 ] [ [ [ 1.0547149181365967 ] , [ 1.3390626907348633 ] , [ 0.5763426423072815 ] ] ] &gt;","ref":"Axon.Layers.html#lp_pool/2","title":"Axon.Layers.lp_pool/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#lstm/6","title":"Axon.Layers.lstm/6","type":"function"},{"doc":"LSTM Cell. When combined with Axon.Layers.*_unroll , implements a LSTM-based RNN. More memory efficient than traditional LSTM. References Long Short-Term Memory","ref":"Axon.Layers.html#lstm_cell/7","title":"Axon.Layers.lstm_cell/7","type":"function"},{"doc":"Functional implementation of a general dimensional max pooling layer. Pooling is applied to the spatial dimension of the input tensor. Max pooling returns the maximum element in each valid window of the input tensor. It is often used after convolutional layers to downsample the input even further. Options kernel_size - window size. Rank must match spatial dimension of the input tensor. Required. :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to size of kernel. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :window_dilations - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Can be scalar or list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . Examples iex&gt; t = Nx . tensor ( [ [ ...&gt; [ 0.051500000059604645 , - 0.7042999863624573 , - 0.32899999618530273 ] , ...&gt; [ - 0.37130001187324524 , 1.6191999912261963 , - 0.11829999834299088 ] , ...&gt; [ 0.7099999785423279 , 0.7282999753952026 , - 0.18639999628067017 ] ] ] , type : { :f , 32 } ) iex&gt; Axon.Layers . max_pool ( t , kernel_size : 2 , channels : :first ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ 1 ] [ [ [ 0.051500000059604645 ] , [ 1.6191999912261963 ] , [ 0.7282999753952026 ] ] ] &gt;","ref":"Axon.Layers.html#max_pool/2","title":"Axon.Layers.max_pool/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#multiply/2","title":"Axon.Layers.multiply/2","type":"function"},{"doc":"Resizes a batch of tensors to the given shape using one of a number of sampling methods. Requires input option :to which should be a tuple specifying the resized spatial dimensions of the input tensor. Input tensor must be at least rank 3, with fixed batch and channel dimensions. Resizing will upsample or downsample using the given resize method. Supported resize methods are :nearest, :linear, :bilinear, :trilinear, :cubic, :bicubic, :tricubic . Examples iex&gt; img = Nx . iota ( { 1 , 1 , 3 , 3 } , type : { :f , 32 } ) iex&gt; Axon.Layers . resize ( img , size : { 4 , 4 } , channels : :first ) # Nx.Tensor &lt; f32 [ 1 ] [ 1 ] [ 4 ] [ 4 ] [ [ [ [ 0.0 , 1.0 , 1.0 , 2.0 ] , [ 3.0 , 4.0 , 4.0 , 5.0 ] , [ 3.0 , 4.0 , 4.0 , 5.0 ] , [ 6.0 , 7.0 , 7.0 , 8.0 ] ] ] ] &gt; Error cases iex&gt; img = Nx . iota ( { 1 , 1 , 3 , 3 } , type : { :f , 32 } ) iex&gt; Axon.Layers . resize ( img , size : { 4 , 4 } , method : :foo ) ** (ArgumentError) expected :method to be either of :nearest, :bilinear, :bicubic, :lanczos3, :lanczos5, got: :foo","ref":"Axon.Layers.html#resize/2","title":"Axon.Layers.resize/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#selu/2","title":"Axon.Layers.selu/2","type":"function"},{"doc":"Functional implementation of a 2-dimensional separable depthwise convolution. The 2-d depthwise separable convolution performs 2 depthwise convolutions each over 1 spatial dimension of the input. Parameter Shapes input - {batch_size, input_channels, input_spatial0, ..., input_spatialN} k1 - {output_channels, 1, kernel_spatial0, 1} b1 - {output_channels} or {} k2 - {output_channels, 1, 1, kernel_spatial1} b2 - {output_channels} or {} output_channels must be a multiple of the input channels. Options :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :input_dilation - input dilation factor. Equivalent to applying interior padding on the input. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :kernel_dilation - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . References Xception: Deep Learning with Depthwise Separable Convolutions","ref":"Axon.Layers.html#separable_conv2d/6","title":"Axon.Layers.separable_conv2d/6","type":"function"},{"doc":"Functional implementation of a 3-dimensional separable depthwise convolution. The 3-d depthwise separable convolution performs 3 depthwise convolutions each over 1 spatial dimension of the input. Parameter Shapes input - {batch_size, input_channels, input_spatial0, input_spatial1, input_spatial2} k1 - {output_channels, 1, kernel_spatial0, 1, 1} b1 - {output_channels} or {} k2 - {output_channels, 1, 1, kernel_spatial1, 1} b2 - {output_channels} or {} k3 - {output_channels, 1, 1, 1, 1, kernel_spatial2} b3 - {output_channels} or {} output_channels must be a multiple of the input channels. Options :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :input_dilation - input dilation factor. Equivalent to applying interior padding on the input. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :kernel_dilation - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . References Xception: Deep Learning with Depthwise Separable Convolutions","ref":"Axon.Layers.html#separable_conv3d/8","title":"Axon.Layers.separable_conv3d/8","type":"function"},{"doc":"","ref":"Axon.Layers.html#softmax/2","title":"Axon.Layers.softmax/2","type":"function"},{"doc":"Functional implementation of an n-dimensional spatial dropout layer. Applies a mask to entire feature maps instead of individual elements. This is done by calculating a mask shape equal to the spatial dimensions of the input tensor with 1 channel, and then broadcasting the mask across the feature dimension of the input tensor. Options :rate - dropout rate. Used to determine probability a connection will be dropped. Required. # :noise_shape - input noise shape. Shape of mask which can be useful for broadcasting ` mask ` across feature channels or other dimensions . Defaults to shape of input tensor . References Efficient Object Localization Using Convolutional Networks","ref":"Axon.Layers.html#spatial_dropout/2","title":"Axon.Layers.spatial_dropout/2","type":"function"},{"doc":"Statically unrolls an RNN. Unrolls implement a scan operation which applies a transformation on the leading axis of input_sequence carrying some state. In this instance cell_fn is an RNN cell function such as lstm_cell or gru_cell . This function inlines the unrolling of the sequence such that the entire operation appears as a part of the compilation graph. This makes it suitable for shorter sequences.","ref":"Axon.Layers.html#static_unroll/6","title":"Axon.Layers.static_unroll/6","type":"function"},{"doc":"","ref":"Axon.Layers.html#subtract/2","title":"Axon.Layers.subtract/2","type":"function"},{"doc":"Implementations of loss-scalers for use in mixed precision training. Loss scaling is used to prevent underflow when using mixed precision during the model training process. Each loss-scale implementation here returns a 3-tuple of the functions: { init_fn , scale_fn , unscale_fn , adjust_fn } = Axon.LossScale . static ( Nx . power ( 2 , 15 ) ) You can use these to scale/unscale loss and gradients as well as adjust the loss scale state. Axon.Loop.trainer/3 builds loss-scaling in by default. You can reference the Axon.Loop.train_step/3 implementation to see how loss-scaling is applied in practice.","ref":"Axon.LossScale.html","title":"Axon.LossScale","type":"module"},{"doc":"Implements dynamic loss-scale.","ref":"Axon.LossScale.html#dynamic/2","title":"Axon.LossScale.dynamic/2","type":"function"},{"doc":"Implements identity loss-scale.","ref":"Axon.LossScale.html#identity/0","title":"Axon.LossScale.identity/0","type":"function"},{"doc":"Implements static loss-scale.","ref":"Axon.LossScale.html#static/1","title":"Axon.LossScale.static/1","type":"function"},{"doc":"Loss functions. Loss functions evaluate predictions with respect to true data, often to measure the divergence between a model's representation of the data-generating distribution and the true representation of the data-generating distribution. Each loss function is implemented as an element-wise function measuring the loss with respect to the input target y_true and input prediction y_pred . As an example, the mean_squared_error/2 loss function produces a tensor whose values are the mean squared error between targets and predictions: iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_squared_error ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.5 , 0.5 ] &gt; It's common to compute the loss across an entire minibatch. You can easily do so by specifying a :reduction mode, or by composing one of these with an Nx reduction method: iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_squared_error ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.5 &gt; You can even compose loss functions: defn my_strange_loss ( y_true , y_pred ) do y_true |&gt; Axon.Losses . mean_squared_error ( y_pred ) |&gt; Axon.Losses . binary_cross_entropy ( y_pred ) |&gt; Nx . sum ( ) end Or, more commonly, you can combine loss functions with penalties for regularization: defn regularized_loss ( params , y_true , y_pred ) do loss = Axon . mean_squared_error ( y_true , y_pred ) penalty = l2_penalty ( params ) Nx . sum ( loss ) + penalty end All of the functions in this module are implemented as numerical functions and can be JIT or AOT compiled with any supported Nx compiler.","ref":"Axon.Losses.html","title":"Axon.Losses","type":"module"},{"doc":"Binary cross-entropy loss function. $$l_i = -\\frac{1}{2}(\\hat{y_i} \\cdot \\log(y_i) + (1 - \\hat{y_i}) \\cdot \\log(1 - y_i))$$ Binary cross-entropy loss is most often used in binary classification problems. By default, it expects y_pred to encode probabilities from [0.0, 1.0] , typically as the output of the sigmoid function or another function which squeezes values between 0 and 1. You may optionally set from_logits: true to specify that values are being sent as non-normalized values (e.g. weights with possibly infinite range). In this case, input values will be encoded as probabilities by applying the logistic sigmoid function before computing loss. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . :negative_weights - class weight for 0 class useful for scaling loss by importance of class. Defaults to 1.0 . :positive_weights - class weight for 1 class useful for scaling loss by importance of class. Defaults to 1.0 . :from_logits - whether y_pred is a logits tensor. Defaults to false . Examples iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6811 , 0.5565 ] , [ 0.6551 , 0.4551 ] , [ 0.5422 , 0.2648 ] ] ) iex&gt; Axon.Losses . binary_cross_entropy ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 3 ] [ 0.8644826412200928 , 0.5150600075721741 , 0.45986634492874146 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6811 , 0.5565 ] , [ 0.6551 , 0.4551 ] , [ 0.5422 , 0.2648 ] ] ) iex&gt; Axon.Losses . binary_cross_entropy ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.613136351108551 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6811 , 0.5565 ] , [ 0.6551 , 0.4551 ] , [ 0.5422 , 0.2648 ] ] ) iex&gt; Axon.Losses . binary_cross_entropy ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 1.8394089937210083 &gt;","ref":"Axon.Losses.html#binary_cross_entropy/3","title":"Axon.Losses.binary_cross_entropy/3","type":"function"},{"doc":"Categorical cross-entropy loss function. $$l_i = -\\sum_i^C \\hat{y_i} \\cdot \\log(y_i)$$ Categorical cross-entropy is typically used for multi-class classifcation problems. By default, it expects y_pred to encode a probability distribution along the last axis. You can specify from_logits: true to indicate y_pred is a logits tensor. # Batch size of 3 with 3 target classes y_true = Nx . tensor ( [ 0 , 2 , 1 ] ) y_pred = Nx . tensor ( [ [ 0.2 , 0.8 , 0.0 ] , [ 0.1 , 0.2 , 0.7 ] , [ 0.1 , 0.2 , 0.7 ] ] ) Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . :class_weights - 1-D list corresponding to weight of each class useful for scaling loss according to importance of class. Tensor size must match number of classes in dataset. Defaults to 1.0 for all classes. :from_logits - whether y_pred is a logits tensor. Defaults to false . :sparse - whether y_true encodes a &quot;sparse&quot; tensor. In this case the inputs are integer values corresponding to the target class. Defaults to false . Examples iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05 , 0.95 , 0 ] , [ 0.1 , 0.8 , 0.1 ] ] ) iex&gt; Axon.Losses . categorical_cross_entropy ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.051293306052684784 , 2.3025851249694824 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05 , 0.95 , 0 ] , [ 0.1 , 0.8 , 0.1 ] ] ) iex&gt; Axon.Losses . categorical_cross_entropy ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 1.1769392490386963 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05 , 0.95 , 0 ] , [ 0.1 , 0.8 , 0.1 ] ] ) iex&gt; Axon.Losses . categorical_cross_entropy ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 2.3538784980773926 &gt; iex&gt; y_true = Nx . tensor ( [ 1 , 2 ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05 , 0.95 , 0 ] , [ 0.1 , 0.8 , 0.1 ] ] ) iex&gt; Axon.Losses . categorical_cross_entropy ( y_true , y_pred , reduction : :sum , sparse : true ) # Nx.Tensor &lt; f32 2.3538784980773926 &gt;","ref":"Axon.Losses.html#categorical_cross_entropy/3","title":"Axon.Losses.categorical_cross_entropy/3","type":"function"},{"doc":"Categorical hinge loss function. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05300799 , 0.21617081 , 0.68642382 ] , [ 0.3754382 , 0.08494169 , 0.13442067 ] ] ) iex&gt; Axon.Losses . categorical_hinge ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 1.6334158182144165 , 1.2410175800323486 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05300799 , 0.21617081 , 0.68642382 ] , [ 0.3754382 , 0.08494169 , 0.13442067 ] ] ) iex&gt; Axon.Losses . categorical_hinge ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 1.4372167587280273 &gt; iex&gt; y_true = Nx . tensor ( [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05300799 , 0.21617081 , 0.68642382 ] , [ 0.3754382 , 0.08494169 , 0.13442067 ] ] ) iex&gt; Axon.Losses . categorical_hinge ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 2.8744335174560547 &gt;","ref":"Axon.Losses.html#categorical_hinge/3","title":"Axon.Losses.categorical_hinge/3","type":"function"},{"doc":"Connectionist Temporal Classification loss. Argument Shapes l_true - $(B)$ y_true - $(B, S)$ y_pred - $(B, T, D)$ Options :reduction - reduction mode. One of :sum or :none . Defaults to :none . Description l_true contains lengths of target sequences. Nonzero positive values. y_true contains target sequences. Each value represents a class of element in range of available classes 0 &lt;= y &lt; D. Blank element class is included in this range, but shouldn't be presented among y_true values. Maximum target sequence length should be lower or equal to y_pred sequence length: S &lt;= T. y_pred - log probabilities of classes D along the prediction sequence T.","ref":"Axon.Losses.html#connectionist_temporal_classification/3","title":"Axon.Losses.connectionist_temporal_classification/3","type":"function"},{"doc":"Cosine Similarity error loss function. $$l_i = \\sum_i (\\hat{y_i} - y_i)^2$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . :axes - Defaults to [1] . :eps - Defaults to 1.0e-6 . Examples iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 0.0 ] , [ 1.0 , 1.0 ] ] ) iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 1.0 , 1.0 ] ] ) iex&gt; Axon.Losses . cosine_similarity ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.0 , 1.0000001192092896 ] &gt;","ref":"Axon.Losses.html#cosine_similarity/3","title":"Axon.Losses.cosine_similarity/3","type":"function"},{"doc":"Hinge loss function. $$\\frac{1}{C}\\max_i(1 - \\hat{y_i} * y_i, 0)$$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Examples iex&gt; y_true = Nx . tensor ( [ [ 1 , 1 , - 1 ] , [ 1 , 1 , - 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.45440044 , 0.31470688 , 0.67920924 ] , [ 0.24311459 , 0.93466766 , 0.10914676 ] ] ) iex&gt; Axon.Losses . hinge ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.9700339436531067 , 0.6437881588935852 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 1 , 1 , - 1 ] , [ 1 , 1 , - 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.45440044 , 0.31470688 , 0.67920924 ] , [ 0.24311459 , 0.93466766 , 0.10914676 ] ] ) iex&gt; Axon.Losses . hinge ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.806911051273346 &gt; iex&gt; y_true = Nx . tensor ( [ [ 1 , 1 , - 1 ] , [ 1 , 1 , - 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.45440044 , 0.31470688 , 0.67920924 ] , [ 0.24311459 , 0.93466766 , 0.10914676 ] ] ) iex&gt; Axon.Losses . hinge ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 1.613822102546692 &gt;","ref":"Axon.Losses.html#hinge/3","title":"Axon.Losses.hinge/3","type":"function"},{"doc":"Kullback-Leibler divergence loss function. $$l_i = \\sum_i^C \\hat{y_i} \\cdot \\log(\\frac{\\hat{y_i}}{y_i})$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 0 , 0 ] ] , type : { :u , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6 , 0.4 ] , [ 0.4 , 0.6 ] ] ) iex&gt; Axon.Losses . kl_divergence ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.916289210319519 , - 3.080907390540233e-6 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 0 , 0 ] ] , type : { :u , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6 , 0.4 ] , [ 0.4 , 0.6 ] ] ) iex&gt; Axon.Losses . kl_divergence ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.45814305543899536 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 0 , 0 ] ] , type : { :u , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6 , 0.4 ] , [ 0.4 , 0.6 ] ] ) iex&gt; Axon.Losses . kl_divergence ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 0.9162861108779907 &gt;","ref":"Axon.Losses.html#kl_divergence/3","title":"Axon.Losses.kl_divergence/3","type":"function"},{"doc":"Logarithmic-Hyperbolic Cosine loss function. $$l_i = \\frac{1}{C} \\sum_i^C (\\hat{y_i} - y_i) + \\log(1 + e^{-2(\\hat{y_i} - y_i)}) - \\log(2)$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; Axon.Losses . log_cosh ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.2168903946876526 , 0.0 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; Axon.Losses . log_cosh ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.1084451973438263 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; Axon.Losses . log_cosh ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 0.2168903946876526 &gt;","ref":"Axon.Losses.html#log_cosh/3","title":"Axon.Losses.log_cosh/3","type":"function"},{"doc":"Margin ranking loss function. $$l_i = \\max(0, -\\hat{y_i} * (y^(1)_i - y^(2)_i) + \\alpha)$$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ 1.0 , 1.0 , 1.0 ] , type : { :f , 32 } ) iex&gt; y_pred1 = Nx . tensor ( [ 0.6934 , - 0.7239 , 1.1954 ] , type : { :f , 32 } ) iex&gt; y_pred2 = Nx . tensor ( [ - 0.4691 , 0.2670 , - 1.7452 ] , type : { :f , 32 } ) iex&gt; Axon.Losses . margin_ranking ( y_true , { y_pred1 , y_pred2 } ) # Nx.Tensor &lt; f32 [ 3 ] [ 0.0 , 0.9909000396728516 , 0.0 ] &gt; iex&gt; y_true = Nx . tensor ( [ 1.0 , 1.0 , 1.0 ] , type : { :f , 32 } ) iex&gt; y_pred1 = Nx . tensor ( [ 0.6934 , - 0.7239 , 1.1954 ] , type : { :f , 32 } ) iex&gt; y_pred2 = Nx . tensor ( [ - 0.4691 , 0.2670 , - 1.7452 ] , type : { :f , 32 } ) iex&gt; Axon.Losses . margin_ranking ( y_true , { y_pred1 , y_pred2 } , reduction : :mean ) # Nx.Tensor &lt; f32 0.3303000032901764 &gt; iex&gt; y_true = Nx . tensor ( [ 1.0 , 1.0 , 1.0 ] , type : { :f , 32 } ) iex&gt; y_pred1 = Nx . tensor ( [ 0.6934 , - 0.7239 , 1.1954 ] , type : { :f , 32 } ) iex&gt; y_pred2 = Nx . tensor ( [ - 0.4691 , 0.2670 , - 1.7452 ] , type : { :f , 32 } ) iex&gt; Axon.Losses . margin_ranking ( y_true , { y_pred1 , y_pred2 } , reduction : :sum ) # Nx.Tensor &lt; f32 0.9909000396728516 &gt;","ref":"Axon.Losses.html#margin_ranking/3","title":"Axon.Losses.margin_ranking/3","type":"function"},{"doc":"Mean-absolute error loss function. $$l_i = \\sum_i |\\hat{y_i} - y_i|$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_absolute_error ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.5 , 0.5 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_absolute_error ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.5 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_absolute_error ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 1.0 &gt;","ref":"Axon.Losses.html#mean_absolute_error/3","title":"Axon.Losses.mean_absolute_error/3","type":"function"},{"doc":"Mean-squared error loss function. $$l_i = \\sum_i (\\hat{y_i} - y_i)^2$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_squared_error ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.5 , 0.5 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_squared_error ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.5 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_squared_error ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 1.0 &gt;","ref":"Axon.Losses.html#mean_squared_error/3","title":"Axon.Losses.mean_squared_error/3","type":"function"},{"doc":"Poisson loss function. $$l_i = \\frac{1}{C} \\sum_i^C y_i - (\\hat{y_i} \\cdot \\log(y_i))$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . poisson ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.9999999403953552 , 0.0 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . poisson ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.4999999701976776 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . poisson ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 0.9999999403953552 &gt;","ref":"Axon.Losses.html#poisson/3","title":"Axon.Losses.poisson/3","type":"function"},{"doc":"Soft margin loss function. $$l_i = \\sum_i \\frac{\\log(1 + e^{-\\hat{y_i} * y_i})}{N}$$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ - 1.0 , 1.0 , 1.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.2953 , - 0.1709 , 0.9486 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . soft_margin ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 3 ] [ 0.851658046245575 , 0.7822436094284058 , 0.3273470401763916 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ - 1.0 , 1.0 , 1.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.2953 , - 0.1709 , 0.9486 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . soft_margin ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.6537495255470276 &gt; iex&gt; y_true = Nx . tensor ( [ [ - 1.0 , 1.0 , 1.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.2953 , - 0.1709 , 0.9486 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . soft_margin ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 1.9612486362457275 &gt;","ref":"Axon.Losses.html#soft_margin/3","title":"Axon.Losses.soft_margin/3","type":"function"},{"doc":"Metric functions. Metrics are used to measure the performance and compare performance of models in easy-to-understand terms. Often times, neural networks use surrogate loss functions such as negative log-likelihood to indirectly optimize a certain performance metric. Metrics such as accuracy, also called the 0-1 loss, do not have useful derivatives (e.g. they are information sparse), and are often intractable even with low input dimensions. Despite not being able to train specifically for certain metrics, it's still useful to track these metrics to monitor the performance of a neural network during training. Metrics such as accuracy provide useful feedback during training, whereas loss can sometimes be difficult to interpret. You can attach any of these functions as metrics within the Axon.Loop API using Axon.Loop.metric/3 . All of the functions in this module are implemented as numerical functions and can be JIT or AOT compiled with any supported Nx compiler.","ref":"Axon.Metrics.html","title":"Axon.Metrics","type":"module"},{"doc":"Computes the accuracy of the given predictions. If the size of the last axis is 1, it performs a binary accuracy computation with a threshold of 0.5. Otherwise, computes categorical accuracy. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Examples iex&gt; Axon.Metrics . accuracy ( Nx . tensor ( [ [ 1 ] , [ 0 ] , [ 0 ] ] ) , Nx . tensor ( [ [ 1 ] , [ 1 ] , [ 1 ] ] ) ) # Nx.Tensor &lt; f32 0.3333333432674408 &gt; iex&gt; Axon.Metrics . accuracy ( Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) , Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 0 , 1 ] ] ) ) # Nx.Tensor &lt; f32 0.6666666865348816 &gt; iex&gt; Axon.Metrics . accuracy ( Nx . tensor ( [ [ 0 , 1 , 0 ] , [ 1 , 0 , 0 ] ] ) , Nx . tensor ( [ [ 0 , 1 , 0 ] , [ 0 , 1 , 0 ] ] ) ) # Nx.Tensor &lt; f32 0.5 &gt;","ref":"Axon.Metrics.html#accuracy/2","title":"Axon.Metrics.accuracy/2","type":"function"},{"doc":"Computes the number of false negative predictions with respect to given targets. Options :threshold - threshold for truth value of predictions. Defaults to 0.5 . Examples iex&gt; y_true = Nx . tensor ( [ 1 , 0 , 1 , 1 , 0 , 1 , 0 ] ) iex&gt; y_pred = Nx . tensor ( [ 0.8 , 0.6 , 0.4 , 0.2 , 0.8 , 0.2 , 0.2 ] ) iex&gt; Axon.Metrics . false_negatives ( y_true , y_pred ) # Nx.Tensor &lt; u64 3 &gt;","ref":"Axon.Metrics.html#false_negatives/3","title":"Axon.Metrics.false_negatives/3","type":"function"},{"doc":"Computes the number of false positive predictions with respect to given targets. Options :threshold - threshold for truth value of predictions. Defaults to 0.5 . Examples iex&gt; y_true = Nx . tensor ( [ 1 , 0 , 1 , 1 , 0 , 1 , 0 ] ) iex&gt; y_pred = Nx . tensor ( [ 0.8 , 0.6 , 0.4 , 0.2 , 0.8 , 0.2 , 0.2 ] ) iex&gt; Axon.Metrics . false_positives ( y_true , y_pred ) # Nx.Tensor &lt; u64 2 &gt;","ref":"Axon.Metrics.html#false_positives/3","title":"Axon.Metrics.false_positives/3","type":"function"},{"doc":"Calculates the mean absolute error of predictions with respect to targets. $$l_i = \\sum_i |\\hat{y_i} - y_i|$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Examples iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Metrics . mean_absolute_error ( y_true , y_pred ) # Nx.Tensor &lt; f32 0.5 &gt;","ref":"Axon.Metrics.html#mean_absolute_error/2","title":"Axon.Metrics.mean_absolute_error/2","type":"function"},{"doc":"Computes the precision of the given predictions with respect to the given targets. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :threshold - threshold for truth value of the predictions. Defaults to 0.5 Examples iex&gt; Axon.Metrics . precision ( Nx . tensor ( [ 0 , 1 , 1 , 1 ] ) , Nx . tensor ( [ 1 , 0 , 1 , 1 ] ) ) # Nx.Tensor &lt; f32 0.6666666865348816 &gt;","ref":"Axon.Metrics.html#precision/3","title":"Axon.Metrics.precision/3","type":"function"},{"doc":"Computes the recall of the given predictions with respect to the given targets. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :threshold - threshold for truth value of the predictions. Defaults to 0.5 Examples iex&gt; Axon.Metrics . recall ( Nx . tensor ( [ 0 , 1 , 1 , 1 ] ) , Nx . tensor ( [ 1 , 0 , 1 , 1 ] ) ) # Nx.Tensor &lt; f32 0.6666666865348816 &gt;","ref":"Axon.Metrics.html#recall/3","title":"Axon.Metrics.recall/3","type":"function"},{"doc":"Returns a function which computes a running average given current average, new observation, and current iteration. Examples iex&gt; cur_avg = 0.5 iex&gt; iteration = 1 iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) iex&gt; avg_acc = Axon.Metrics . running_average ( &amp; Axon.Metrics . accuracy / 2 ) iex&gt; avg_acc . ( cur_avg , [ y_true , y_pred ] , iteration ) # Nx.Tensor &lt; f32 0.75 &gt;","ref":"Axon.Metrics.html#running_average/1","title":"Axon.Metrics.running_average/1","type":"function"},{"doc":"Returns a function which computes a running sum given current sum, new observation, and current iteration. Examples iex&gt; cur_sum = 12 iex&gt; iteration = 2 iex&gt; y_true = Nx . tensor ( [ 0 , 1 , 0 , 1 ] ) iex&gt; y_pred = Nx . tensor ( [ 1 , 1 , 0 , 1 ] ) iex&gt; fps = Axon.Metrics . running_sum ( &amp; Axon.Metrics . false_positives / 2 ) iex&gt; fps . ( cur_sum , [ y_true , y_pred ] , iteration ) # Nx.Tensor &lt; s64 13 &gt;","ref":"Axon.Metrics.html#running_sum/1","title":"Axon.Metrics.running_sum/1","type":"function"},{"doc":"Computes the sensitivity of the given predictions with respect to the given targets. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :threshold - threshold for truth value of the predictions. Defaults to 0.5 Examples iex&gt; Axon.Metrics . sensitivity ( Nx . tensor ( [ 0 , 1 , 1 , 1 ] ) , Nx . tensor ( [ 1 , 0 , 1 , 1 ] ) ) # Nx.Tensor &lt; f32 0.6666666865348816 &gt;","ref":"Axon.Metrics.html#sensitivity/3","title":"Axon.Metrics.sensitivity/3","type":"function"},{"doc":"Computes the specificity of the given predictions with respect to the given targets. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :threshold - threshold for truth value of the predictions. Defaults to 0.5 Examples iex&gt; Axon.Metrics . specificity ( Nx . tensor ( [ 0 , 1 , 1 , 1 ] ) , Nx . tensor ( [ 1 , 0 , 1 , 1 ] ) ) # Nx.Tensor &lt; f32 0.0 &gt;","ref":"Axon.Metrics.html#specificity/3","title":"Axon.Metrics.specificity/3","type":"function"},{"doc":"Computes the top-k categorical accuracy. Options k - The k in &quot;top-k&quot;. Defaults to 5. sparse - If y_true is a sparse tensor. Defaults to false . Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Examples iex&gt; Axon.Metrics . top_k_categorical_accuracy ( Nx . tensor ( [ 0 , 1 , 0 , 0 , 0 ] ) , Nx . tensor ( [ 0.1 , 0.4 , 0.3 , 0.7 , 0.1 ] ) , k : 2 ) # Nx.Tensor &lt; f32 1.0 &gt; iex&gt; Axon.Metrics . top_k_categorical_accuracy ( Nx . tensor ( [ [ 0 , 1 , 0 ] , [ 1 , 0 , 0 ] ] ) , Nx . tensor ( [ [ 0.1 , 0.4 , 0.7 ] , [ 0.1 , 0.4 , 0.7 ] ] ) , k : 2 ) # Nx.Tensor &lt; f32 0.5 &gt; iex&gt; Axon.Metrics . top_k_categorical_accuracy ( Nx . tensor ( [ [ 0 ] , [ 2 ] ] ) , Nx . tensor ( [ [ 0.1 , 0.4 , 0.7 ] , [ 0.1 , 0.4 , 0.7 ] ] ) , k : 2 , sparse : true ) # Nx.Tensor &lt; f32 0.5 &gt;","ref":"Axon.Metrics.html#top_k_categorical_accuracy/3","title":"Axon.Metrics.top_k_categorical_accuracy/3","type":"function"},{"doc":"Computes the number of true negative predictions with respect to given targets. Options :threshold - threshold for truth value of predictions. Defaults to 0.5 . Examples iex&gt; y_true = Nx . tensor ( [ 1 , 0 , 1 , 1 , 0 , 1 , 0 ] ) iex&gt; y_pred = Nx . tensor ( [ 0.8 , 0.6 , 0.4 , 0.2 , 0.8 , 0.2 , 0.2 ] ) iex&gt; Axon.Metrics . true_negatives ( y_true , y_pred ) # Nx.Tensor &lt; u64 1 &gt;","ref":"Axon.Metrics.html#true_negatives/3","title":"Axon.Metrics.true_negatives/3","type":"function"},{"doc":"Computes the number of true positive predictions with respect to given targets. Options :threshold - threshold for truth value of predictions. Defaults to 0.5 . Examples iex&gt; y_true = Nx . tensor ( [ 1 , 0 , 1 , 1 , 0 , 1 , 0 ] ) iex&gt; y_pred = Nx . tensor ( [ 0.8 , 0.6 , 0.4 , 0.2 , 0.8 , 0.2 , 0.2 ] ) iex&gt; Axon.Metrics . true_positives ( y_true , y_pred ) # Nx.Tensor &lt; u64 1 &gt;","ref":"Axon.Metrics.html#true_positives/3","title":"Axon.Metrics.true_positives/3","type":"function"},{"doc":"Implementations of common gradient-based optimization algorithms. All of the methods in this module are written in terms of the update methods defined in Axon.Updates . Axon treats optimizers as the tuple: { init_fn , update_fn } where init_fn returns an initial optimizer state and update_fn scales input gradients. init_fn accepts a model's parameters and attaches state to each parameter. update_fn accepts gradients, optimizer state, and current model parameters and returns updated optimizer state and gradients. Custom optimizers are often created via the Axon.Updates API. Example Consider the following usage of the Adam optimizer in a basic update function (assuming objective and the dataset are defined elsewhere): defmodule Learning do import Nx.Defn defn init ( params , init_fn ) do init_fn . ( params ) end defn update ( params , optimizer_state , inputs , targets , update_fn ) do { loss , gradient } = value_and_grad ( params , &amp; objective ( &amp;1 , inputs , targets ) ) { scaled_updates , new_optimizer_state } = update_fn . ( gradient , optimizer_state , params ) { Axon.Updates . apply_updates ( params , scaled_updates ) , new_optimizer_state , loss } end end model_params = Nx . random_uniform ( { 784 , 10 } ) { init_fn , update_fn } = Axon.Optimizers . adam ( 0.005 ) optimizer_state = Learning . init ( params , init_fn ) { new_params , new_optimizer_state , loss } = Learning . update ( params , optimizer_state , inputs , targets , update_fn ) For a simpler approach, you can also use optimizers with the training API: model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , Axon.Optimizers . adam ( 0.005 ) ) |&gt; Axon.Loop . run ( data , epochs : 10 , compiler : EXLA )","ref":"Axon.Optimizers.html","title":"Axon.Optimizers","type":"module"},{"doc":"Adabelief optimizer. Options :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 0.0 :eps_root - numerical stability term. Defaults to 1.0e-16 References AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients","ref":"Axon.Optimizers.html#adabelief/2","title":"Axon.Optimizers.adabelief/2","type":"function"},{"doc":"Adagrad optimizer. Options :eps - numerical stability term. Defaults to 1.0e-7 References Adaptive Subgradient Methods for Online Learning and Stochastic Optimization","ref":"Axon.Optimizers.html#adagrad/2","title":"Axon.Optimizers.adagrad/2","type":"function"},{"doc":"Adam optimizer. Options :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 1.0e-8 :eps_root - numerical stability term. Defaults to 1.0e-15 References Adam: A Method for Stochastic Optimization","ref":"Axon.Optimizers.html#adam/2","title":"Axon.Optimizers.adam/2","type":"function"},{"doc":"Adam with weight decay optimizer. Options :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 1.0e-8 :eps_root - numerical stability term. Defaults to 0.0 :decay - weight decay. Defaults to 0.0","ref":"Axon.Optimizers.html#adamw/2","title":"Axon.Optimizers.adamw/2","type":"function"},{"doc":"Lamb optimizer. Options :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 1.0e-8 :eps_root - numerical stability term. Defaults to 0.0 :decay - weight decay. Defaults to 0.0 :min_norm - minimum norm value. Defaults to 0.0 References Large Batch Optimization for Deep Learning: Training BERT in 76 minutes","ref":"Axon.Optimizers.html#lamb/2","title":"Axon.Optimizers.lamb/2","type":"function"},{"doc":"Noisy SGD optimizer. Options :eta - used to compute variance of noise distribution. Defaults to 0.1 :gamma - used to compute variance of noise distribution. Defaults to 0.55","ref":"Axon.Optimizers.html#noisy_sgd/2","title":"Axon.Optimizers.noisy_sgd/2","type":"function"},{"doc":"Rectified Adam optimizer. Options :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 1.0e-8 :eps_root - numerical stability term. Defaults to 0.0 :threshold - threshold term. Defaults to 5.0 References On the Variance of Adaptive Learning Rate and Beyond","ref":"Axon.Optimizers.html#radam/2","title":"Axon.Optimizers.radam/2","type":"function"},{"doc":"RMSProp optimizer. Options :centered - whether to scale by centered root of EMA of squares. Defaults to false :momentum - momentum term. If set, uses SGD with momentum and decay set to value of this term. :nesterov - whether or not to use nesterov momentum. Defaults to false :initial_scale - initial value of EMA. Defaults to 0.0 :decay - EMA decay rate. Defaults to 0.9 :eps - numerical stability term. Defaults to 1.0e-8","ref":"Axon.Optimizers.html#rmsprop/2","title":"Axon.Optimizers.rmsprop/2","type":"function"},{"doc":"SGD optimizer. Options :momentum - momentum term. If set, uses SGD with momentum and decay set to value of this term. :nesterov - whether or not to use nesterov momentum. Defaults to false","ref":"Axon.Optimizers.html#sgd/2","title":"Axon.Optimizers.sgd/2","type":"function"},{"doc":"Yogi optimizer. Options :initial_accumulator_value - initial value for first and second moment. Defaults to 0.0 :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 1.0e-8 :eps_root - numerical stability term. Defaults to 0.0 References Adaptive Methods for Nonconvex Optimization","ref":"Axon.Optimizers.html#yogi/2","title":"Axon.Optimizers.yogi/2","type":"function"},{"doc":"Parameter Schedules. Parameter schedules are often used to anneal hyperparameters such as the learning rate during the training process. Schedules provide a mapping from the current time step to a learning rate or another hyperparameter. Choosing a good learning rate and consequently a good learning rate schedule is typically a process of trial and error. Learning rates should be relatively small such that the learning curve does not oscillate violently during the training process, but not so small that learning proceeds too slowly. Using a schedule slowly decreases oscillations during the training process such that, as the model converges, training also becomes more stable. All of the functions in this module are implemented as numerical functions and can be JIT or AOT compiled with any supported Nx compiler.","ref":"Axon.Schedules.html","title":"Axon.Schedules","type":"module"},{"doc":"Constant schedule. $$\\gamma(t) = \\gamma_0$$ Options :init_value - initial value. $\\gamma_0$ in above formulation. Defaults to 1.0e-2","ref":"Axon.Schedules.html#constant/1","title":"Axon.Schedules.constant/1","type":"function"},{"doc":"Cosine decay schedule. $$\\gamma(t) = \\gamma_0 (1 - \\alpha) (\\frac{1}{2}(1 + \\cos{\\pi \\frac{t}{k}})) + \\alpha$$ Options :init_value - initial value. $\\gamma_0$ in above formulation. Defaults to 1.0e-2 :decay_steps - number of steps to apply decay for. $k$ in above formulation. Defaults to 10 :alpha - minimum value of multiplier adjusting learning rate. $\\alpha$ in above formulation. Defaults to 0.0 References SGDR: Stochastic Gradient Descent with Warm Restarts","ref":"Axon.Schedules.html#cosine_decay/1","title":"Axon.Schedules.cosine_decay/1","type":"function"},{"doc":"Exponential decay schedule. $$\\gamma(t) = \\gamma_0 * r^{\\frac{t}{k}}$$ Options :init_value - initial value. $\\gamma$ in above formulation. Defaults to 1.0e-2 :decay_rate - rate of decay. $r$ in above formulation. Defaults to 0.95 :transition_steps - steps per transition. $k$ in above formulation. Defaults to 10 :transition_begin - step to begin transition. Defaults to 0 :staircase - discretize outputs. Defaults to false","ref":"Axon.Schedules.html#exponential_decay/1","title":"Axon.Schedules.exponential_decay/1","type":"function"},{"doc":"Polynomial schedule. $$\\gamma(t) = (\\gamma_0 - \\gamma_n) * (1 - \\frac{t}{k})^p$$ Options :init_value - initial value. $\\gamma_0$ in above formulation. Defaults to 1.0e-2 :end_value - end value of annealed scalar. $\\gamma_n$ in above formulation. Defaults to 1.0e-3 :power - power of polynomial. $p$ in above formulation. Defaults to 2 :transition_steps - number of steps over which annealing takes place. $k$ in above formulation. Defaults to 10","ref":"Axon.Schedules.html#polynomial_decay/1","title":"Axon.Schedules.polynomial_decay/1","type":"function"},{"doc":"Parameter update methods. Update methods transform the input tensor in some way, usually by scaling or shifting the input with respect to some input state. Update methods are composed to create more advanced optimization methods such as AdaGrad or Adam. Each update returns a tuple: { init_fn , update_fn } Which represent a state initialization and state update function respectively. While each method in the Updates API is a regular Elixir function, the two methods they return are implemented as defn , so they can be accelerated using any Nx backend or compiler. Update methods are just combinators that can be arbitrarily composed to create complex optimizers. For example, the Adam optimizer in Axon.Optimizers is implemented as: def adam ( learning_rate , opts \\\\ [ ] ) do Updates . scale_by_adam ( opts ) |&gt; Updates . scale ( - learning_rate ) end Updates are maps of updates, often associated with parameters of the same names. Using Axon.Updates.apply_updates/3 will merge updates and parameters by adding associated parameters and updates, and ensuring any given model state is preserved. Custom combinators You can create your own combinators using the stateless/2 and stateful/3 primitives. Every update method in this module is implemented in terms of one of these two primitives. stateless/2 represents a stateless update: def scale ( combinator \\\\ Axon.Updates . identity ( ) , step_size ) do stateless ( combinator , &amp; apply_scale ( &amp;1 , &amp;2 , step_size ) ) end defnp apply_scale ( x , _params , step ) do transform ( { x , step } , fn { updates , step } -&gt; deep_new ( updates , fn x -&gt; Nx . multiply ( x , step ) end ) end ) end Notice how the function given to stateless/2 is defined within defn . This is what allows the anonymous functions returned by Axon.Updates to be used inside defn . stateful/3 represents a stateful update and follows the same pattern: def my_stateful_update ( updates ) do Axon.Updates . stateful ( updates , &amp; init_my_update / 1 , &amp; apply_my_update / 2 ) end defnp init_my_update ( params ) do state = zeros_like ( params ) %{ state : state } end defnp apply_my_update ( updates , state ) do new_state = deep_new ( state , fn v -&gt; Nx . add ( v , 0.01 ) end ) updates = transform ( { updates , new_state } , fn { updates , state } -&gt; deep_merge ( updates , state , fn g , z -&gt; Nx . multiply ( g , z ) end ) end ) { updates , %{ state : new_state } } end State associated with individual parameters should have keys that match the keys of the parameter. For example, if you have parameters %{kernel: kernel} with associated states mu and nu representing the first and second moments, your state should look something like: %{ mu : %{ kernel : kernel_mu } nu : %{ kernel : kernel_nu } }","ref":"Axon.Updates.html","title":"Axon.Updates","type":"module"},{"doc":"Adds decayed weights to updates. Commonly used as a regularization strategy. Options * ` :decay ` - Rate of decay . Defaults to ` 0.0 ` .","ref":"Axon.Updates.html#add_decayed_weights/1","title":"Axon.Updates.add_decayed_weights/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#add_decayed_weights/2","title":"Axon.Updates.add_decayed_weights/2","type":"function"},{"doc":"Adds random Gaussian noise to the input. Options * ` :eta ` - Controls amount of noise to add . Defaults to ` 0.01 ` . * ` :gamma ` - Controls amount of noise to add . Defaults to ` 0.55 ` .","ref":"Axon.Updates.html#add_noise/1","title":"Axon.Updates.add_noise/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#add_noise/2","title":"Axon.Updates.add_noise/2","type":"function"},{"doc":"Applies updates to params and updates state parameters with given state map.","ref":"Axon.Updates.html#apply_updates/3","title":"Axon.Updates.apply_updates/3","type":"function"},{"doc":"Centralizes input by shifting updates by their mean.","ref":"Axon.Updates.html#centralize/1","title":"Axon.Updates.centralize/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#centralize/2","title":"Axon.Updates.centralize/2","type":"function"},{"doc":"Clips input between -delta and delta. Options :delta - maximum absolute value of the input. Defaults to 2.0","ref":"Axon.Updates.html#clip/1","title":"Axon.Updates.clip/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#clip/2","title":"Axon.Updates.clip/2","type":"function"},{"doc":"Clips input using input global norm. Options :max_norm - maximum norm value of input. Defaults to 1.0","ref":"Axon.Updates.html#clip_by_global_norm/1","title":"Axon.Updates.clip_by_global_norm/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#clip_by_global_norm/2","title":"Axon.Updates.clip_by_global_norm/2","type":"function"},{"doc":"Composes two updates. This is useful for extending optimizers without having to reimplement them. For example, you can implement gradient centralization: import Axon.Updates Axon.Updates . compose ( Axon.Updates . centralize ( ) , Axon.Optimizers . rmsprop ( ) ) This is equivalent to: Axon.Updates . centralize ( ) |&gt; Axon.Updates . scale_by_rms ( )","ref":"Axon.Updates.html#compose/2","title":"Axon.Updates.compose/2","type":"function"},{"doc":"Returns the identity update. This is often as the initial update in many functions in this module.","ref":"Axon.Updates.html#identity/0","title":"Axon.Updates.identity/0","type":"function"},{"doc":"","ref":"Axon.Updates.html#identity/1","title":"Axon.Updates.identity/1","type":"function"},{"doc":"Scales input by a fixed step size. $$f(x_i) = \\alpha x_i$$","ref":"Axon.Updates.html#scale/2","title":"Axon.Updates.scale/2","type":"function"},{"doc":"Scales input according to Adam algorithm. Options * ` :b1 ` - first moment decay . Defaults to ` 0.9 ` * ` :b2 ` - second moment decay . Defaults to ` 0.999 ` * ` :eps ` - numerical stability term . Defaults to ` 1.0e-8 ` * ` :eps_root ` - numerical stability term . Defaults to ` 1.0e-15 ` References Adam: A Method for Stochastic Optimization","ref":"Axon.Updates.html#scale_by_adam/1","title":"Axon.Updates.scale_by_adam/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_adam/2","title":"Axon.Updates.scale_by_adam/2","type":"function"},{"doc":"Scales input according to the AdaBelief algorithm. Options * ` :b1 ` - first moment decay . Defaults to ` 0.9 ` . * ` :b2 ` - second moment decay . Defaults to ` 0.999 ` . * ` :eps ` - numerical stability term . Defaults to ` 0.0 ` . * ` :eps_root ` - numerical stability term . Defaults to ` 1.0e-16 ` . References AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients","ref":"Axon.Updates.html#scale_by_belief/1","title":"Axon.Updates.scale_by_belief/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_belief/2","title":"Axon.Updates.scale_by_belief/2","type":"function"},{"doc":"Scale input according to the Rectified Adam algorithm. Options * ` :b1 ` - first moment decay . Defaults to ` 0.9 ` * ` :b2 ` - second moment decay . Defaults to ` 0.999 ` * ` :eps ` - numerical stability term . Defaults to ` 1.0e-8 ` * ` :eps_root ` - numerical stability term . Defaults to ` 0.0 ` * ` :threshold ` - threshold for variance . Defaults to ` 5.0 ` References On the Variance of the Adaptive Learning Rate and Beyond","ref":"Axon.Updates.html#scale_by_radam/1","title":"Axon.Updates.scale_by_radam/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_radam/2","title":"Axon.Updates.scale_by_radam/2","type":"function"},{"doc":"Scales input by the root of the EMA of squared inputs. Options * ` :decay ` - EMA decay rate . Defaults to ` 0.9 ` . * ` :eps ` - numerical stability term . Defaults to ` 1.0e-8 ` . References Overview of mini-batch gradient descent","ref":"Axon.Updates.html#scale_by_rms/1","title":"Axon.Updates.scale_by_rms/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_rms/2","title":"Axon.Updates.scale_by_rms/2","type":"function"},{"doc":"Scales input by the root of all prior squared inputs. Options * ` :eps ` - numerical stability term . Defaults to ` 1.0e-7 `","ref":"Axon.Updates.html#scale_by_rss/1","title":"Axon.Updates.scale_by_rss/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_rss/2","title":"Axon.Updates.scale_by_rss/2","type":"function"},{"doc":"Scales input using the given schedule function. This can be useful for implementing learning rate schedules. The number of update iterations is tracked by an internal counter. You might need to update the schedule to operate on per-batch schedule rather than per-epoch.","ref":"Axon.Updates.html#scale_by_schedule/2","title":"Axon.Updates.scale_by_schedule/2","type":"function"},{"doc":"Scales input by a tunable learning rate which can be manipulated by external APIs such as Axon's Loop API. $$f(x_i) = \\alpha x_i$$","ref":"Axon.Updates.html#scale_by_state/1","title":"Axon.Updates.scale_by_state/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_state/2","title":"Axon.Updates.scale_by_state/2","type":"function"},{"doc":"Scales input by the root of the centered EMA of squared inputs. Options * ` :decay ` - EMA decay rate . Defaults to ` 0.9 ` . * ` :eps ` - numerical stability term . Defaults to ` 1.0e-8 ` . References Overview of mini-batch gradient descent","ref":"Axon.Updates.html#scale_by_stddev/1","title":"Axon.Updates.scale_by_stddev/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_stddev/2","title":"Axon.Updates.scale_by_stddev/2","type":"function"},{"doc":"Scale by trust ratio. Options * ` :min_norm ` - Min norm to clip . Defaults to ` 0.0 ` . * ` :trust_coefficient ` - Trust coefficient . Defaults to ` 1.0 ` . * ` :eps ` - Numerical stability term . Defaults to ` 0.0 ` .","ref":"Axon.Updates.html#scale_by_trust_ratio/1","title":"Axon.Updates.scale_by_trust_ratio/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_trust_ratio/2","title":"Axon.Updates.scale_by_trust_ratio/2","type":"function"},{"doc":"Scale input according to the Yogi algorithm. Options * ` :initial_accumulator_value ` - Initial state accumulator value . * ` :b1 ` - first moment decay . Defaults to ` 0.9 ` * ` :b2 ` - second moment decay . Defaults to ` 0.999 ` * ` :eps ` - numerical stability term . Defaults to ` 1.0e-8 ` * ` :eps_root ` - numerical stability term . Defaults to ` 0.0 ` References * [ Adaptive Methods for Nonconvex Optimization ] ( https :// proceedings . neurips . cc / paper / 2018 / file / 90365351 ccc7437a1309dc64e4db32a3 - Paper . pdf )","ref":"Axon.Updates.html#scale_by_yogi/1","title":"Axon.Updates.scale_by_yogi/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_yogi/2","title":"Axon.Updates.scale_by_yogi/2","type":"function"},{"doc":"Represents a stateful update. Stateful updates require some update state, such as momentum or RMS of previous updates. Therefore you must implement some initialization function as well as an update function.","ref":"Axon.Updates.html#stateful/3","title":"Axon.Updates.stateful/3","type":"function"},{"doc":"Represents a stateless update. Stateless updates do not depend on an update state and thus only require an implementation of an update function.","ref":"Axon.Updates.html#stateless/2","title":"Axon.Updates.stateless/2","type":"function"},{"doc":"Trace inputs with past inputs. Options :decay - decay rate for tracing past updates. Defaults to 0.9 :nesterov - whether to use Nesterov momentum. Defaults to false","ref":"Axon.Updates.html#trace/1","title":"Axon.Updates.trace/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#trace/2","title":"Axon.Updates.trace/2","type":"function"},{"doc":"Abstraction for modeling a reduction of a dataset with an accumulated state for a number of epochs. Inspired heavily by PyTorch Ignite . The main abstraction is the %Loop{} struct, which controls a nested reduction of the form: Enum . reduce ( 1 .. max_epochs , state , fn epoch , state -&gt; Enum . reduce ( data , state , &amp; batch_step / 2 ) end ) data is assumed to be an Enumerable or Stream of input data which is handled by a processing function, batch_step . The purpose of the loop abstraction is to take away much of the boilerplate used in solving machine learning tasks. Tasks such as normalizing a dataset, hyperparameter optimization, or training machine learning models boil down to writing one function: defn batch_step ( batch , state ) do # ...do something with batch... updated_state end For tasks such as training a neural network, state will encapsulate things such as model and optimizer state. For supervised learning tasks, batch_step might look something like: defn batch_step ( { inputs , targets } , state ) do %{ parameters : params , optimizer_state : optim_state } = state gradients = grad ( params , objective_fn . ( &amp;1 , inputs , targets ) ) { updates , new_optim_state } = optimizer . ( optim_state , params , gradients ) new_params = apply_updates ( params , updates ) %{ parameters : new_params , optimizer_state : optim_state } end batch_step takes a batch of {input, target} pairs and the current state, and updates the model parameters based on the gradients received from some arbitrary objective function. This function will run in a nested loop, iterating over the entire dataset for N epochs before finally returning the trained model state. By defining 1 function, we've created a training loop that works for most machine learning models. In actuality, the loop abstraction accumulates a struct, Axon.Loop.State , which looks like (assuming container is a generic Elixir container of tensors, e.g. map, tuple, etc.): % State { epoch : integer ( ) , max_epoch : integer ( ) , iteration : integer ( ) , max_iteration : integer ( ) , metrics : map ( string ( ) , container ( ) ) , times : map ( integer ( ) , integer ( ) ) , step_state : container ( ) } batch_step takes in the batch and the step state field and returns a step_state , which is a generic container of state accumulated at each iteration. The rest of the fields in the state struct are updated automatically behind the scenes. The loop must start from some initial step state, thus most tasks must also provide an additional initialization function to provide some starting point for the step state. For machine learning tasks, the initialization function will return things like initial model parameters and optimizer state. Typically, the final output of the loop is the accumulated final state; however, you may optionally apply an output transform to extract specific values at the end of the loop. For example, Axon.Loop.trainer/4 by default extracts trained model state: output_transform = fn state -&gt; state . step_state [ :model_state ] end Initialize and Step The core of the Axon loop are the init and step functions. The initialization is an arity-0 function which provides an initial step state: init = fn -&gt; %{ params : Axon . init ( model ) } end While the step function is the batch_step function mentioned earlier: step = fn data , state -&gt; new_state = # ...do something... new_state end Metrics Often times you want to compute metrics associated with your training iterations. To accomplish this, you can attach metrics to each Axon.Loop . Assuming a batch_step function which looks like: defn batch_step ( { inputs , targets } , state ) do %{ parameters : params , optimizer_state : optim_state } = state gradients = grad ( params , objective_fn . ( &amp;1 , inputs , targets ) ) { updates , new_optim_state } = optimizer . ( optim_state , params , gradients ) new_params = apply_updates ( params , updates ) # Shown for simplicity, you can optimize this by calculating preds # along with the gradient calculation preds = model_fn . ( params , inputs ) %{ y_true : targets , y_pred : preds , parameters : new_params , optimizer_state : optim_state } end You can attach metrics to this by using Axon.Loop.metric/4 : Axon.Loop . loop ( &amp; batch_step / 2 ) |&gt; Axon.Loop . metric ( &quot;Accuracy&quot; , :accuracy , fn %{ y_true : y_ , y_pred : y } -&gt; [ y_ , y ] end ) |&gt; Axon.Loop . run ( data ) Because metrics work directly on step_state , you typically need to provide an output transform to indicate which values should be passed to your metric function. By default, Axon assumes a supervised training task with the fields :y_true and :y_pred present in the step state. See Axon.Loop.metric/4 for more information. Metrics will be tracked in the loop state using the user-provided key. Metrics integrate seamlessly with the supervised metrics defined in Axon.Metrics . You can also use metrics to keep running averages of some values in the original dataset. Events and Handlers You can instrument several points in the loop using event handlers. By default, several events are fired when running a loop: events = [ :started , # After loop state initialization :epoch_started , # On epoch start :iteration_started , # On iteration start :iteration_completed , # On iteration complete :epoch_completed , # On epoch complete :epoch_halted , # On epoch halt, if early halted :halted , # On loop halt, if early halted :completed # On loop completion ] You can attach event handlers to events using Axon.Loop.handle/4 : loop |&gt; Axon.Loop . handle ( :iteration_completed , &amp; log_metrics / 1 , every : 100 ) |&gt; Axon.Loop . run ( data ) The above will trigger log_metrics/1 every 100 times the :iteration_completed event is fired. Event handlers must return a tuple {status, state} , where status is an atom with one of the following values: :continue # Continue epoch, continue looping :halt_epoch # Halt the epoch, continue looping :halt_loop # Halt looping And state is an updated Axon.Loop.State struct. Handler functions take as input the current loop state. It's important to note that event handlers are triggered in the order they are attached to the loop. If you have two handlers on the same event, they will trigger in order: loop |&gt; Axon.Loop . handle ( :epoch_completed , &amp; normalize_state / 1 ) # Runs first |&gt; Axon.Loop . handle ( :epoch_completed , &amp; log_state / 1 ) # Runs second You may provide filters to filter when event handlers trigger. See Axon.Loop.handle/4 for more details on valid filters. Factories Axon loops are typically created from one of the factory functions provided in this module: * ` Axon.Loop . loop / 3 ` - Creates a loop from step function and optional initialization functions and output transform functions . * ` Axon.Loop . trainer / 3 ` - Creates a supervised training loop from model , loss , and optimizer . * ` Axon.Loop . evaluator / 1 ` - Creates a supervised evaluator loop from model . Running loops In order to execute a loop, you should use Axon.Loop.run/3 : loop |&gt; Axon.Loop . run ( data , epochs : 10 ) Resuming loops At times you may want to resume a loop from some previous state. You can accomplish this with Axon.Loop.from_state/2 : loop |&gt; Axon.Loop . from_state ( state ) |&gt; Axon.Loop . run ( data )","ref":"Axon.Loop.html","title":"Axon.Loop","type":"module"},{"doc":"Adds a handler function which saves loop checkpoints on a given event, optionally with metric-based criteria. By default, loop checkpoints will be saved at the end of every epoch in the current working directory under the checkpoint/ path. Checkpoints are serialized representations of loop state obtained from Axon.Loop.serialize_state/2 . Serialization options will be forwarded to Axon.Loop.serialize_state/2 . You can customize checkpoint events by passing :event and :filter options: loop |&gt; Axon.Loop . checkpoint ( event : :iteration_completed , filter : [ every : 50 ] ) Checkpoints are saved under the checkpoint/ directory with a pattern of checkpoint_{epoch}.ckpt . You can customize the path and pattern with the :path and :file_pattern options: my_file_pattern = fn % Axon.Loop.State { epoch : epoch , iteration : iter } -&gt; &quot;checkpoint_ \#{ epoch } _ \#{ iter } &quot; end loop |&gt; Axon.Loop . checkpoint ( path : &quot;my_checkpoints&quot; , file_pattern : my_file_pattern ) If you'd like to only save checkpoints based on some metric criteria, you can specify the :criteria option. :criteria must be a valid key in metrics: loop |&gt; Axon.Loop . checkpoint ( criteria : &quot;validation_loss&quot; ) The default criteria mode is :min , meaning the min score metric will be considered &quot;best&quot; when deciding to save on a given event. Valid modes are :min and :max : loop |&gt; Axon.Loop . checkpoint ( criteria : &quot;validation_accuracy&quot; , mode : :max )","ref":"Axon.Loop.html#checkpoint/2","title":"Axon.Loop.checkpoint/2","type":"function"},{"doc":"Deserializes loop state from a binary. It is the opposite of Axon.Loop.serialize_state/2 . By default, the step state is deserialized using Nx.deserialize.2 ; however, this behavior can be changed if step state is an application specific container. For example, if you introduce your own data structure into step_state and you customized the serialization logic, Nx.deserialize/2 will not be sufficient for deserialization. - you must pass custom logic with :deserialize_step_state .","ref":"Axon.Loop.html#deserialize_state/2","title":"Axon.Loop.deserialize_state/2","type":"function"},{"doc":"Adds a handler function which halts a loop if the given metric does not improve between events. By default, this will run after each epoch and track the improvement of a given metric. You must specify a metric to monitor and the metric must be present in the loop state. Typically, this will be a validation metric: model |&gt; Axon.Loop . trainer ( loss , optim ) |&gt; Axon.Loop . metric ( :accuracy ) |&gt; Axon.Loop . validate ( val_data ) |&gt; Axon.Loop . early_stop ( &quot;validation_accuracy&quot; ) It's important to remember that handlers are executed in the order they are added to the loop. For example, if you'd like to checkpoint a loop after every epoch and use early stopping, most likely you want to add the checkpoint handler before the early stopping handler: model |&gt; Axon.Loop . trainer ( loss , optim ) |&gt; Axon.Loop . metric ( :accuracy ) |&gt; Axon.Loop . checkpoint ( ) |&gt; Axon.Loop . early_stop ( &quot;accuracy&quot; ) That will ensure checkpoint is always fired, even if the loop exited early.","ref":"Axon.Loop.html#early_stop/3","title":"Axon.Loop.early_stop/3","type":"function"},{"doc":"Creates a supervised evaluation step from a model and model state. This function is intended for more fine-grained control over the loop creation process. It returns a tuple of {init_fn, step_fn} where init_fn returns an initial step state and step_fn performs a single evaluation step.","ref":"Axon.Loop.html#eval_step/1","title":"Axon.Loop.eval_step/1","type":"function"},{"doc":"Creates a supervised evaluator from a model and model state. An evaluator can be used for things such as testing and validation of models after or during training. It assumes model is an Axon struct, container of structs, or a tuple of init / apply functions. model_state must be a container usable from within model . The evaluator returns a step state of the form: %{ y_true : labels , y_pred : predictions } Such that you can attach any number of supervised metrics to the evaluation loop: model |&gt; Axon.Loop . evaluator ( ) |&gt; Axon.Loop . metric ( &quot;Accuracy&quot; , :accuracy ) Applies an output transform which returns the map of metrics accumulated over the given loop.","ref":"Axon.Loop.html#evaluator/1","title":"Axon.Loop.evaluator/1","type":"function"},{"doc":"Attaches state to the given loop in order to resume looping from a previous state. It's important to note that a loop's attached state takes precedence over defined initialization functions. Given initialization function: defn init_state ( ) , do : %{ foo : 1 , bar : 2 } And an attached state: state = % State { step_state : %{ foo : 2 , bar : 3 } } init_state/0 will never execute, and instead the initial step state of %{foo: 2, bar: 3} will be used.","ref":"Axon.Loop.html#from_state/2","title":"Axon.Loop.from_state/2","type":"function"},{"doc":"Adds a handler function to the loop which will be triggered on event with an optional filter. Events take place at different points during loop execution. The default events are: events = [ :started , # After loop state initialization :epoch_started , # On epoch start :iteration_started , # On iteration start :iteration_completed , # On iteration complete :epoch_completed , # On epoch complete :epoch_halted , # On epoch halt, if early halted :halted , # On loop halt, if early halted :completed # On loop completion ] Generally, event handlers are side-effecting operations which provide some sort of inspection into the loop's progress. It's important to note that if you define multiple handlers to be triggered on the same event, they will execute in order from when they were attached to the training loop: loop |&gt; Axon.Loop . handle ( :epoch_started , &amp; normalize_step_state / 1 ) # executes first |&gt; Axon.Loop . handle ( :epoch_started , &amp; log_step_state / 1 ) # executes second Thus, if you have separate handlers which alter or depend on loop state, you need to ensure they are ordered correctly, or combined into a single event handler for maximum control over execution. event must be an atom representing the event to trigger handler or a list of atoms indicating handler should be triggered on multiple events. event may be :all which indicates the handler should be triggered on every event during loop processing. handler must be an arity-1 function which takes as input loop state and returns {status, state} , where status is an atom with one of the following values: :continue # Continue epoch, continue looping :halt_epoch # Halt the epoch, continue looping :halt_loop # Halt looping filter is an atom representing a valid filter predicate, a keyword of predicate-value pairs, or a function which takes loop state and returns a true , indicating the handler should run, or false , indicating the handler should not run. Valid predicates are: :always # Always trigger event :once # Trigger on first event firing Valid predicate-value pairs are: every : N # Trigger every `N` event only : N # Trigger on `N` event","ref":"Axon.Loop.html#handle/4","title":"Axon.Loop.handle/4","type":"function"},{"doc":"Adds a handler function which logs the given message produced by message_fn to the given IO device every event satisfying filter . In most cases, this is useful for inspecting the contents of the loop state at intermediate stages. For example, the default trainer loop factory attaches IO logging of epoch, batch, loss and metrics. It's also possible to log loop state to files by changing the given IO device. By default, the IO device is :stdio . message_fn should take the loop state and return a binary representing the message to be written to the IO device.","ref":"Axon.Loop.html#log/5","title":"Axon.Loop.log/5","type":"function"},{"doc":"Creates a loop from step_fn , an optional init_fn , and an optional output_transform . step_fn is an arity-2 function which takes a batch and state and returns an updated step state: defn batch_step ( batch , step_state ) do step_state + 1 end init_fn by default is an identity function which forwards its initial arguments as the model state. You should define a custom initialization function if you require a different behavior: defn init_step_state ( state ) do Map . merge ( %{ foo : 1 } , state ) end You may use state in conjunction with initialization functions in init_fn . For example, train_step/3 uses initial state as initial model parameters to allow initializing models from partial parameterizations. step_batch/2 and init_step_state/1 are typically called from within Nx.Defn.jit/3 . While JIT-compilation will work with anonymous functions, def , and defn , it is recommended that you use the stricter defn to define both functions in order to avoid bugs or cryptic errors. output_transform/1 applies a transformation on the final accumulated loop state. This is useful for extracting specific fields from a loop and piping them into additional functions.","ref":"Axon.Loop.html#loop/3","title":"Axon.Loop.loop/3","type":"function"},{"doc":"Adds a metric of the given name to the loop. A metric is a function which tracks or measures some value with respect to values in the step state. For example, when training classification models, it's common to track the model's accuracy during training: loop |&gt; Axon.Loop . metric ( :accuracy , &quot;Accuracy&quot; ) By default, metrics assume a supervised learning task and extract the fields [:y_true, :y_pred] from the step state. If you wish to work on a different value, you can use an output transform. An output transform is a list of keys to extract from the output state, or a function which returns a flattened list of values to pass to the given metric function. Values received from output transforms are passed to the given metric using: value = output_transform . ( step_state ) apply ( metric , value ) Thus, even if you want your metric to work on a container, your output transform must return a list. metric must be an atom which matches the name of a metric in Axon.Metrics , or an arbitrary function which returns a tensor or container. name must be a string or atom used to store the computed metric in the loop state. If names conflict, the last attached metric will take precedence: loop |&gt; Axon.Loop . metric ( :mean_squared_error , &quot;Error&quot; ) # Will be overwritten |&gt; Axon.Loop . metric ( :mean_absolute_error , &quot;Error&quot; ) # Will be used By default, metrics keep a running average of the metric calculation. You can override this behavior by changing accumulate : loop |&gt; Axon.Loop . metric ( :true_negatives , &quot;tn&quot; , :running_sum ) Accumulation function can be one of the accumulation combinators in Axon.Metrics or an arity-3 function of the form: accumulate(acc, obs, i) :: new_acc .","ref":"Axon.Loop.html#metric/5","title":"Axon.Loop.metric/5","type":"function"},{"doc":"Adds a handler function which reduces the learning rate by the given factor if the given metric does not improve between events. By default, this will run after each epoch and track the improvement of a given metric. You must specify a metric to monitor and the metric must be present in the loop state. Typically, this will be a validation metric: model |&gt; Axon.Loop . trainer ( loss , optim ) |&gt; Axon.Loop . metric ( :accuracy ) |&gt; Axon.Loop . validate ( model , val_data ) |&gt; Axon.Loop . reduce_lr_on_plateau ( &quot;accuracy&quot; , mode : :max ) Options :event - event to fire handler on. Defaults to :epoch_completed . :filter - event filter to attach to handler. Defaults to :always . :patience - number of given events to wait for improvement. Defaults to 3 . :mode - whether given metric is being minimized or maximized. Defaults to :min . :factor - factor to decrease learning rate by. Defaults to 0.1 .","ref":"Axon.Loop.html#reduce_lr_on_plateau/3","title":"Axon.Loop.reduce_lr_on_plateau/3","type":"function"},{"doc":"Runs the given loop on data with the given options. loop must be a valid Axon.Loop struct built from one of the loop factories provided in this module. data must be an Enumerable or Stream which yields batches of data on each iteration. Options :epochs - max epochs to run loop for. Must be non-negative integer. Defaults to 1 . :iterations - max iterations to run each epoch. Must be non-negative integer. Defaults to -1 or no max iterations. :jit_compile? - whether or not to JIT compile initialization and step functions. JIT compilation must be used for gradient computations. Defaults to true. :debug - run loop in debug mode to trace loop progress. Defaults to false. Additional options are forwarded to Nx.Defn.jit as JIT-options. If no JIT options are set, the default options set with Nx.Defn.default_options are used.","ref":"Axon.Loop.html#run/4","title":"Axon.Loop.run/4","type":"function"},{"doc":"Serializes loop state to a binary for saving and loading loop from previous states. You can consider the serialized state to be a checkpoint of all state at a given iteration and epoch. By default, the step state is serialized using Nx.serialize/2 ; however, this behavior can be changed if step state is an application specific container. For example, if you introduce your own data structure into step_state, Nx.serialize/2 will not be sufficient for serialization - you must pass custom serialization as an option with :serialize_step_state . Additional opts controls serialization options such as compression. It is forwarded to :erlang.term_to_binary/2 .","ref":"Axon.Loop.html#serialize_state/2","title":"Axon.Loop.serialize_state/2","type":"function"},{"doc":"Creates a supervised train step from a model, loss function, and optimizer. This function is intended for more fine-grained control over the loop creation process. It returns a tuple of {init_fn, step_fn} where init_fn is an initialization function which returns an initial step state and step_fn is a supervised train step constructed from model , loss , and optimizer . model must be an Axon struct, a valid defn container of Axon structs, or a {init_fn, apply_fn} -tuple where init_fn is an arity-2 function which initializes the model state and apply_fn is an arity-2 function which applies the forward pass of the model. The forward pass of the model must return a map with keys :prediction and :state representing the model's prediction and updated state for layers which aggregate state during training. loss must be an atom which matches a function in Axon.Losses , a list of {loss, weight} tuples representing a basic weighted loss function for multi-output models, or an arity-2 function representing a custom loss function. optimizer must be an atom matching the name of a valid optimizer in Axon.Optimizers , or a {init_fn, update_fn} tuple where init_fn is an arity-1 function which initializes the optimizer state from attached parameters and update_fn is an arity-3 function which scales gradient updates with respect to input parameters, optimizer state, and gradients. See Axon.Updates for more information on building optimizers.","ref":"Axon.Loop.html#train_step/5","title":"Axon.Loop.train_step/5","type":"function"},{"doc":"Creates a supervised training loop from a model, loss function, and optimizer. This function is useful for training models on most standard supervised learning tasks. It assumes data consists of tuples of input-target pairs, e.g. [{x0, y0}, {x1, y1}, ..., {xN, yN}] where x0 and y0 are batched tensors or containers of batched tensors. It defines an initialization function which first initializes model state using the given model and then initializes optimizer state using the initial model state. The step function uses a differentiable objective function defined with respect to the model parameters, input data, and target data using the given loss function. It then updates model parameters using the given optimizer in order to minimize loss with respect to the model parameters. model must be an Axon struct, a valid defn container of Axon structs, or a {init_fn, apply_fn} -tuple where init_fn is an arity-2 function which initializes the model state and apply_fn is an arity-2 function which applies the forward pass of the model. loss must be an atom which matches a function in Axon.Losses , a list of {loss, weight} tuples representing a basic weighted loss function for multi-output models, or an arity-2 function representing a custom loss function. optimizer must be an atom matching the name of a valid optimizer in Axon.Optimizers , or a {init_fn, update_fn} tuple where init_fn is an arity-1 function which initializes the optimizer state from attached parameters and update_fn is an arity-3 function which scales gradient updates with respect to input parameters, optimizer state, and gradients. See Axon.Updates for more information on building optimizers. This function creates a step function which outputs a map consisting of the following fields for step_state : %{ y_pred : tensor ( ) | container ( tensor ( ) ) , # Model predictions for use in metrics y_true : tensor ( ) | container ( tensor ( ) ) , # True labels for use in metrics loss : tensor ( ) , # Running average of loss over epoch model_state : container ( tensor ( ) ) , # Model parameters and state optimizer_state : container ( tensor ( ) ) # Optimizer state associated with each parameter } Examples Basic usage data = Stream . zip ( input , target ) model = Axon . input ( &quot;input&quot; , shape : { nil , 32 } ) |&gt; Axon . dense ( 1 , activation : :sigmoid ) model |&gt; Axon.Loop . trainer ( :binary_cross_entropy , :adam ) |&gt; Axon.Loop . run ( data ) Customizing Optimizer model |&gt; Axon.Loop . trainer ( :binary_cross_entropy , Axon.Optimizers . adam ( 0.05 ) ) |&gt; Axon.Loop . run ( data ) Custom loss loss_fn = fn y_true , y_pred -&gt; Nx . cos ( y_true , y_pred ) end model |&gt; Axon.Loop . trainer ( loss_fn , Axon.Optimizers . rmsprop ( 0.01 ) ) |&gt; Axon.Loop . run ( data ) Multiple objectives with multi-output model model = { Axon . input ( &quot;input_0&quot; , shape : { nil , 1 } ) , Axon . input ( &quot;input_1&quot; , shape : { nil , 2 } ) } loss_weights = [ mean_squared_error : 0.5 , mean_absolute_error : 0.5 ] model |&gt; Axon.Loop . trainer ( loss_weights ) |&gt; Axon.Loop . run ( data ) Options :log - training loss and metric log interval. Set to 0 to silence training logs. Defaults to 50","ref":"Axon.Loop.html#trainer/5","title":"Axon.Loop.trainer/5","type":"function"},{"doc":"Adds a handler function which tests the performance of model against the given validation set. This handler assumes the loop state matches the state initialized in a supervised training loop. Typically, you'd call this immediately after creating a supervised training loop: model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . validate ( model , validation_data ) Please note that you must pass the same (or an equivalent) model into this method so it can be used during the validation loop. The metrics which are computed are those which are present BEFORE the validation handler was added to the loop. For the following loop: model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . metric ( :mean_absolute_error ) |&gt; Axon.Loop . validate ( model , validation_data ) |&gt; Axon.Loop . metric ( :binary_cross_entropy ) only :mean_absolute_error will be computed at validation time. The returned loop state is altered to contain validation metrics for use in later handlers such as early stopping and model checkpoints. Since the order of execution of event handlers is in the same order they are declared in the training loop, you MUST call this method before any other handler which expects or may use validation metrics. By default the validation loop runs after every epoch; however, you can customize it by overriding the default event and event filters: model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . metric ( :mean_absolute_error ) |&gt; Axon.Loop . validate ( model , validation_data , :iteration_completed , every : 10_000 ) |&gt; Axon.Loop . metric ( :binary_cross_entropy )","ref":"Axon.Loop.html#validate/5","title":"Axon.Loop.validate/5","type":"function"},{"doc":"Accumulated state in an Axon.Loop. Loop state is a struct: % State { epoch : integer ( ) , max_epoch : integer ( ) , iteration : integer ( ) , max_iteration : integer ( ) , metrics : map ( string ( ) , container ( ) ) , times : map ( integer ( ) , integer ( ) ) , step_state : container ( ) , handler_metadata : container ( ) } epoch is the current epoch, starting at 0, of the nested loop. Defaults to 0. max_epoch is the maximum number of epochs the loop should run for. Defaults to 1. iteration is the current iteration of the inner loop. In supervised settings, this will be the current batch. Defaults to 0. max_iteration is the maximum number of iterations the loop should run a given epoch for. Defaults to -1 (no max). metrics is a map of %{&quot;metric_name&quot; =&gt; value} which accumulates metrics over the course of loop processing. Defaults to an empty map. times is a map of %{epoch_number =&gt; value} which maps a given epoch to the processing time. Defaults to an empty map. step_state is the step state as defined by the loop's processing initialization and update functions. step_state is a required field. handler_metadata is a metadata field for storing loop handler metadata. For example, loop checkpoints with specific metric criteria can store previous best metrics in the handler meta for use between iterations.","ref":"Axon.Loop.State.html","title":"Axon.Loop.State","type":"module"},{"doc":"","ref":"Axon.CompileError.html","title":"Axon.CompileError","type":"exception"},{"doc":"Callback implementation for Exception.message/1 .","ref":"Axon.CompileError.html#message/1","title":"Axon.CompileError.message/1","type":"function"},{"doc":"Axon is a library for creating and training neural networks in Elixir. The Axon guides are a collection of Livebooks designed to introduce Axon's APIs and design decisions from the bottom-up. After working through the guides, you will feel comfortable and confident working with Axon and using Axon for your next deep learning problem.","ref":"guides.html","title":"Axon Guides","type":"extras"},{"doc":"Your first Axon model Sequential models Complex models Multi-input / multi-output models Custom layers Model hooks","ref":"guides.html#model-creation","title":"Axon Guides - Model Creation","type":"extras"},{"doc":"Accelerating Axon Training and inference mode","ref":"guides.html#model-execution","title":"Axon Guides - Model Execution","type":"extras"},{"doc":"Your first training loop Instrumenting loops with metrics Your first evalutaion loop Using loop event handlers Custom models, loss functions, and optimizers Writing custom metrics Writing custom event handlers","ref":"guides.html#training-and-evaluation","title":"Axon Guides - Training and Evaluation","type":"extras"},{"doc":"Converting ONNX models to Axon","ref":"guides.html#serialization","title":"Axon Guides - Serialization","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } , { :kino , &quot;~&gt; 0.7.0&quot; } ] ) :ok","ref":"your_first_axon_model.html","title":"Your first Axon model","type":"extras"},{"doc":"Axon is a library for creating and training neural networks in Elixir. Everything in Axon centers around the %Axon{} struct which represents an instance of an Axon model. Models are just graphs which represent the transformation and flow of input data to a desired output. Really, you can think of models as representing a single computation or function. An Axon model, when executed, takes data as input and returns transformed data as output. All Axon models start with a declaration of input nodes. These are the root nodes of your computation graph, and correspond to the actual input data you want to send to Axon: input = Axon . input ( &quot;data&quot; ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;data&quot; nodes : 1 &gt; Technically speaking, input is now a valid Axon model which you can inspect, execute, and initialize. You can visualize how data flows through the graph using Axon.Display.as_graph/2 : template = Nx . template ( { 2 , 8 } , :f32 ) Axon.Display . as_graph ( input , template ) graph TD ; 3 [ / &quot;data (:input) {2, 8}&quot; / ] ; ; Notice the execution flow is just a single node, because your graph only consists of an input node! You pass data in and the model spits the same data back out, without any intermediate transformations. You can see this in action by actually executing your model. You can build the %Axon{} struct into it's initialization and forward functions by calling Axon.build/2 . This pattern of &quot;lowering&quot; or transforming the %Axon{} data structure into other functions or representations is very common in Axon. By simply traversing the data structure, you can create useful functions, execution visualizations, and more! { init_fn , predict_fn } = Axon . build ( input ) { # Function &lt; 137.55749718 / 2 in Nx.Defn . wrap_arity / 2 &gt; , # Function &lt; 137.55749718 / 2 in Nx.Defn . wrap_arity / 2 &gt; } Notice that Axon.build/2 returns a tuple of {init_fn, predict_fn} . init_fn has the signature: init_fn . ( template :: map ( tensor ) | tensor , initial_params :: map ) :: map ( tensor ) while predict_fn has the signature: predict_fn . ( params :: map ( tensor ) , input :: map ( tensor ) | tensor ) init_fn returns all of your model's trainable parameters and state. You need to pass a template of the expected inputs because the shape of certain model parameters often depend on the shape of model inputs. You also need to pass any initial parameters you want your model to start with. This is useful for things like transfer learning, which you can read about in another guide. predict_fn returns transformed inputs from your model's trainable parameters and the given inputs. params = init_fn . ( Nx . template ( { 1 , 8 } , :f32 ) , %{ } ) %{ } In this example, you use Nx.template/2 to create a template tensor , which is a placeholder that does not actually consume any memory. Templates are useful for initialization because you don't actually need to know anything about your inputs other than their shape and type. Notice init_fn returned an empty map because your model does not have any trainable parameters. This should make sense because it's just an input layer. Now you can pass these trainable parameters to predict_fn along with some input to actually execute your model: predict_fn . ( params , Nx . iota ( { 1 , 8 } , type : :f32 ) ) # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ 0.0 , 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 6.0 , 7.0 ] ] &gt; And your model just returned the given input, as expected!","ref":"your_first_axon_model.html#your-first-model","title":"Your first Axon model - Your first model","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } , { :kino , &quot;~&gt; 0.7.0&quot; } ] ) :ok","ref":"sequential_models.html","title":"Sequential models","type":"extras"},{"doc":"In the last guide , you created a simple identity model which just returned the input. Of course, you would never actually use Axon for such purposes. You want to create real neural networks! In equivalent frameworks in the Python ecosystem such as Keras and PyTorch, there is a concept of sequential models . Sequential models are named after the sequential nature in which data flows through them. Sequential models transform the input with sequential, successive transformations. If you're an experienced Elixir programmer, this paradigm of sequential transformations might sound a lot like what happens when using the pipe ( |&gt; ) operator. In Elixir, it's common to see code blocks like: list |&gt; Enum . map ( fn x -&gt; x + 1 end ) |&gt; Enum . filter ( &amp; rem ( &amp;1 , 2 ) == 0 ) |&gt; Enum . count ( ) The snippet above passes list through a sequence of transformations. You can apply this same paradigm in Axon to create sequential models. In fact, creating sequential models is so natural with Elixir's pipe operator, that Axon does not need a distinct sequential construct. To create a sequential model, you just pass Axon models through successive transformations in the Axon API: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 32 ) |&gt; Axon . activation ( :relu ) |&gt; Axon . dropout ( rate : 0.5 ) |&gt; Axon . dense ( 1 ) |&gt; Axon . activation ( :softmax ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;softmax_0&quot; nodes : 6 &gt; If you visualize this model, it's easy to see how data flows sequentially through it: template = Nx . template ( { 2 , 16 } , :f32 ) Axon.Display . as_graph ( model , template ) graph TD ; 3 [ / &quot;data (:input) {2, 16}&quot; / ] ; 6 [ &quot;dense_0 (:dense) {2, 32}&quot; ] ; 7 [ &quot;relu_0 (:relu) {2, 32}&quot; ] ; 8 [ &quot;dropout_0 (:dropout) {2, 32}&quot; ] ; 11 [ &quot;dense_1 (:dense) {2, 1}&quot; ] ; 12 [ &quot;softmax_0 (:softmax) {2, 1}&quot; ] ; 11 -- &gt; 12 ; 8 -- &gt; 11 ; 7 -- &gt; 8 ; 6 -- &gt; 7 ; 3 -- &gt; 6 ; Your model is more involved and as a result so is the execution graph! Now, using the same constructs from the last section, you can build and run your model: { init_fn , predict_fn } = Axon . build ( model ) { # Function &lt; 137.55749718 / 2 in Nx.Defn . wrap_arity / 2 &gt; , # Function &lt; 137.55749718 / 2 in Nx.Defn . wrap_arity / 2 &gt; } params = init_fn . ( template , %{ } ) %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 32 ] [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 16 ] [ 32 ] [ [ - 0.25727564096450806 , - 0.31299564242362976 , - 0.1557893306016922 , - 0.3321501314640045 , 0.34875044226646423 , 0.15635445713996887 , 0.25805917382240295 , 0.316285640001297 , 0.29047688841819763 , - 0.09108144044876099 , 0.2781231701374054 , 0.21326711773872375 , - 0.29581472277641296 , - 0.3105146288871765 , - 0.11265464127063751 , 0.054490894079208374 , - 0.22294805943965912 , 0.23276928067207336 , 0.06426036357879639 , 0.12059605121612549 , - 0.24530324339866638 , 0.061366915702819824 , 0.17463091015815735 , - 0.2774006724357605 , 0.2621242105960846 , 0.19262376427650452 , - 0.10884760320186615 , - 0.3156566321849823 , 0.104307621717453 , - 0.22591334581375122 , - 0.09672778844833374 , - 0.18450938165187836 ] , [ - 0.32328563928604126 , - 0.3434811234474182 , - 0.3464450538158417 , 0.14756330847740173 , 0.010595977306365967 , 0.32808688282966614 , - 0.3048470616340637 , 0.011142522096633911 , 0.10394474864006042 , 0.04501914978027344 , - 0.26296690106391907 , - 0.1051199734210968 , - 0.0060880184173583984 , 0.22103646397590637 , - 0.3040429651737213 , ... ] , ... ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.0 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 32 ] [ 1 ] [ [ - 0.379288911819458 ] , [ - 0.05532142519950867 ] , [ - 0.07836392521858215 ] , [ 0.41381680965423584 ] , [ 0.33221137523651123 ] , [ 0.23515504598617554 ] , [ - 0.40667685866355896 ] , [ - 0.3503745198249817 ] , [ 0.2631032466888428 ] , [ - 0.13176566362380981 ] , [ - 0.3811171054840088 ] , [ 0.24656128883361816 ] , [ 0.17257028818130493 ] , [ 0.3528350591659546 ] , [ 0.4112042784690857 ] , [ 0.056196123361587524 ] , [ 0.138421893119812 ] , [ - 0.38378745317459106 ] , [ - 0.044070273637771606 ] , [ 0.11507803201675415 ] , [ - 0.3125251233577728 ] , [ - 0.11389034986495972 ] , [ - 0.27444711327552795 ] , [ - 0.30974721908569336 ] , [ - 0.3695589303970337 ] , [ 0.3146793246269226 ] , [ 0.005854517221450806 ] , [ - 0.03735968470573425 ] , [ 0.02763468027114868 ] , [ - 0.10707724094390869 ] , [ 0.10824829339981079 ] , [ 0.29013824462890625 ] ] &gt; } } Wow! Notice that this model actually has trainable parameters. You can see that the parameter map is just a regular Elixir map. Each top-level entry maps to a layer with a key corresponding to that layer's name and a value corresponding to that layer's trainable parameters. Each layer's individual trainable parameters are given layer-specific names and map directly to Nx tensors. Now you can use these params with your predict_fn : predict_fn . ( params , Nx . iota ( { 2 , 16 } , type : :f32 ) ) # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] [ [ 1.0 ] , [ 1.0 ] ] &gt; And voila! You've successfully created and used a sequential model in Axon!","ref":"sequential_models.html#creating-a-sequential-model","title":"Sequential models - Creating a sequential model","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } , { :kino , &quot;~&gt; 0.7.0&quot; } ] ) :ok","ref":"complex_models.html","title":"Complex models","type":"extras"},{"doc":"Not all models you'd want to create fit cleanly in the sequential paradigm. Some models require a more flexible API. Fortunately, because Axon models are just Elixir data structures, you can manipulate them and decompose architectures as you would any other Elixir program: input = Axon . input ( &quot;data&quot; ) x1 = input |&gt; Axon . dense ( 32 ) x2 = input |&gt; Axon . dense ( 64 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 32 ) out = Axon . add ( x1 , x2 ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;add_0&quot; nodes : 7 &gt; In the snippet above, your model branches input into x1 and x2 . Each branch performs a different set of transformations; however, at the end the branches are merged with an Axon.add/3 . You might sometimes see layers like Axon.add/3 called combinators . Really they're just layers that operate on multiple Axon models at once - typically to merge some branches together. out represents your final Axon model. If you visualize this model, you can see the full effect of the branching in this model: template = Nx . template ( { 2 , 8 } , :f32 ) Axon.Display . as_graph ( out , template ) graph TD ; 3 [ / &quot;data (:input) {2, 8}&quot; / ] ; 6 [ &quot;dense_0 (:dense) {2, 32}&quot; ] ; 9 [ &quot;dense_1 (:dense) {2, 64}&quot; ] ; 10 [ &quot;relu_0 (:relu) {2, 64}&quot; ] ; 13 [ &quot;dense_2 (:dense) {2, 32}&quot; ] ; 14 [ &quot;container_0 (:container) {{2, 32}, {2, 32}}&quot; ] ; 15 [ &quot;add_0 (:add) {2, 32}&quot; ] ; 14 -- &gt; 15 ; 13 -- &gt; 14 ; 6 -- &gt; 14 ; 10 -- &gt; 13 ; 9 -- &gt; 10 ; 3 -- &gt; 9 ; 3 -- &gt; 6 ; And you can use Axon.build/2 on out as you would any other Axon model: { init_fn , predict_fn } = Axon . build ( out ) { # Function &lt; 135.51955502 / 2 in Nx.Defn.Compiler . fun / 2 &gt; , # Function &lt; 135.51955502 / 2 in Nx.Defn.Compiler . fun / 2 &gt; } params = init_fn . ( template , %{ } ) predict_fn . ( params , Nx . iota ( { 2 , 8 } , type : :f32 ) ) # Nx.Tensor &lt; f32 [ 2 ] [ 32 ] [ [ - 3.4256787300109863 , - 0.866683840751648 , - 0.2629307508468628 , 3.2555718421936035 , 2.2740533351898193 , 3.0403499603271484 , - 2.7904915809631348 , 3.4799132347106934 , - 4.16396951675415 , - 4.545778274536133 , 3.146249532699585 , - 3.0786540508270264 , 3.4500746726989746 , 1.1419837474822998 , - 0.7993628978729248 , 2.3798861503601074 , 4.787802696228027 , 1.290929913520813 , 1.8274409770965576 , - 1.5016275644302368 , 3.441028118133545 , - 1.8077948093414307 , 0.25549376010894775 , - 2.555987596511841 , - 4.643674850463867 , 2.164360523223877 , - 0.30402517318725586 , - 2.54134464263916 , - 2.699089527130127 , 4.074007511138916 , - 0.7711544036865234 , - 3.988246202468872 ] , [ - 11.235082626342773 , - 1.5991168022155762 , - 4.076810836791992 , 11.091293334960938 , 4.669280052185059 , 12.756690979003906 , - 1.4954360723495483 , 4.8143310546875 , - 14.211947441101074 , - 11.360504150390625 , 6.239661693572998 , - 0.9994411468505859 , 8.645132064819336 , - 0.5422897338867188 , - 1.4019453525543213 , 9.633858680725098 , 10.077424049377441 , - 0.3623824119567871 , ... ] ] &gt; As your architectures grow in complexity, you might find yourself reaching for better abstractions to organize your model creation code. For example, PyTorch models are often organized into nn.Module . The equivalent of an nn.Module in Axon is a regular Elixir function. If you're translating models from PyTorch to Axon, it's natural to create one Elixir function per nn.Module . You should write your models as you would write any other Elixir code - you don't need to worry about any framework specific constructs: defmodule MyModel do def model ( ) do Axon . input ( &quot;data&quot; ) |&gt; conv_block ( ) |&gt; Axon . flatten ( ) |&gt; dense_block ( ) |&gt; dense_block ( ) |&gt; Axon . dense ( 1 ) end defp conv_block ( input ) do residual = input x = input |&gt; Axon . conv ( 3 , padding : :same ) |&gt; Axon . mish ( ) x |&gt; Axon . add ( residual ) |&gt; Axon . max_pool ( kernel_size : { 2 , 2 } ) end defp dense_block ( input ) do input |&gt; Axon . dense ( 32 ) |&gt; Axon . relu ( ) end end { :module , MyModel , &lt;&lt; 70 , 79 , 82 , 49 , 0 , 0 , 8 , ... &gt;&gt; , { :dense_block , 1 } } model = MyModel . model ( ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;dense_2&quot; nodes : 12 &gt; template = Nx . template ( { 1 , 28 , 28 , 3 } , :f32 ) Axon.Display . as_graph ( model , template ) graph TD ; 16 [ / &quot;data (:input) {1, 28, 28, 3}&quot; / ] ; 19 [ &quot;conv_0 (:conv) {1, 28, 28, 3}&quot; ] ; 20 [ &quot;mish_0 (:mish) {1, 28, 28, 3}&quot; ] ; 21 [ &quot;container_0 (:container) {{1, 28, 28, 3}, {1, 28, 28, 3}}&quot; ] ; 22 [ &quot;add_0 (:add) {1, 28, 28, 3}&quot; ] ; 23 [ &quot;max_pool_0 (:max_pool) {1, 14, 14, 3}&quot; ] ; 24 [ &quot;flatten_0 (:flatten) {1, 588}&quot; ] ; 27 [ &quot;dense_0 (:dense) {1, 32}&quot; ] ; 28 [ &quot;relu_0 (:relu) {1, 32}&quot; ] ; 31 [ &quot;dense_1 (:dense) {1, 32}&quot; ] ; 32 [ &quot;relu_1 (:relu) {1, 32}&quot; ] ; 35 [ &quot;dense_2 (:dense) {1, 1}&quot; ] ; 32 -- &gt; 35 ; 31 -- &gt; 32 ; 28 -- &gt; 31 ; 27 -- &gt; 28 ; 24 -- &gt; 27 ; 23 -- &gt; 24 ; 22 -- &gt; 23 ; 21 -- &gt; 22 ; 16 -- &gt; 21 ; 20 -- &gt; 21 ; 19 -- &gt; 20 ; 16 -- &gt; 19 ;","ref":"complex_models.html#creating-more-complex-models","title":"Complex models - Creating more complex models","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } , { :kino , &quot;~&gt; 0.7.0&quot; } ] ) :ok","ref":"multi_input_multi_output_models.html","title":"Multi-input / multi-output models","type":"extras"},{"doc":"Sometimes your application necessitates the use of multiple inputs. To use multiple inputs in an Axon model, you just need to declare multiple inputs in your graph: input_1 = Axon . input ( &quot;input_1&quot; ) input_2 = Axon . input ( &quot;input_2&quot; ) out = Axon . add ( input_1 , input_2 ) # Axon &lt; inputs : %{ &quot;input_1&quot; =&gt; nil , &quot;input_2&quot; =&gt; nil } outputs : &quot;add_0&quot; nodes : 4 &gt; Notice when you inspect the model, it tells you what your models inputs are up front. You can also get metadata about your model inputs programmatically with Axon.get_inputs/1 : Axon . get_inputs ( out ) %{ &quot;input_1&quot; =&gt; nil , &quot;input_2&quot; =&gt; nil } Each input is uniquely named, so you can pass inputs by-name into inspection and execution functions with a map: inputs = %{ &quot;input_1&quot; =&gt; Nx . template ( { 2 , 8 } , :f32 ) , &quot;input_2&quot; =&gt; Nx . template ( { 2 , 8 } , :f32 ) } Axon.Display . as_graph ( out , inputs ) graph TD ; 3 [ / &quot;input_1 (:input) {2, 8}&quot; / ] ; 4 [ / &quot;input_2 (:input) {2, 8}&quot; / ] ; 5 [ &quot;container_0 (:container) {{2, 8}, {2, 8}}&quot; ] ; 6 [ &quot;add_0 (:add) {2, 8}&quot; ] ; 5 -- &gt; 6 ; 4 -- &gt; 5 ; 3 -- &gt; 5 ; { init_fn , predict_fn } = Axon . build ( out ) params = init_fn . ( inputs , %{ } ) %{ } inputs = %{ &quot;input_1&quot; =&gt; Nx . iota ( { 2 , 8 } , type : :f32 ) , &quot;input_2&quot; =&gt; Nx . iota ( { 2 , 8 } , type : :f32 ) } predict_fn . ( params , inputs ) # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ 0.0 , 2.0 , 4.0 , 6.0 , 8.0 , 10.0 , 12.0 , 14.0 ] , [ 16.0 , 18.0 , 20.0 , 22.0 , 24.0 , 26.0 , 28.0 , 30.0 ] ] &gt; If you forget a required input, Axon will raise: predict_fn . ( params , %{ &quot;input_1&quot; =&gt; Nx . iota ( { 2 , 8 } , type : :f32 ) } )","ref":"multi_input_multi_output_models.html#creating-multi-input-models","title":"Multi-input / multi-output models - Creating multi-input models","type":"extras"},{"doc":"Depending on your application, you might also want your model to have multiple outputs. You can achieve this by using Axon.container/2 to wrap multiple nodes into any supported Nx container: inp = Axon . input ( &quot;data&quot; ) x1 = inp |&gt; Axon . dense ( 32 ) |&gt; Axon . relu ( ) x2 = inp |&gt; Axon . dense ( 64 ) |&gt; Axon . relu ( ) out = Axon . container ( { x1 , x2 } ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;container_0&quot; nodes : 6 &gt; template = Nx . template ( { 2 , 8 } , :f32 ) Axon.Display . as_graph ( out , template ) graph TD ; 7 [ / &quot;data (:input) {2, 8}&quot; / ] ; 10 [ &quot;dense_0 (:dense) {2, 32}&quot; ] ; 11 [ &quot;relu_0 (:relu) {2, 32}&quot; ] ; 14 [ &quot;dense_1 (:dense) {2, 64}&quot; ] ; 15 [ &quot;relu_1 (:relu) {2, 64}&quot; ] ; 16 [ &quot;container_0 (:container) {{2, 32}, {2, 64}}&quot; ] ; 15 -- &gt; 16 ; 11 -- &gt; 16 ; 14 -- &gt; 15 ; 7 -- &gt; 14 ; 10 -- &gt; 11 ; 7 -- &gt; 10 ; When executed, containers will return a data structure which matches their input structure: { init_fn , predict_fn } = Axon . build ( out ) params = init_fn . ( template , %{ } ) predict_fn . ( params , Nx . iota ( { 2 , 8 } , type : :f32 ) ) { # Nx.Tensor &lt; f32 [ 2 ] [ 32 ] [ [ 0.0 , 0.0 , 3.111135482788086 , 0.48920655250549316 , 0.0 , 0.5125713348388672 , 0.0 , 0.0 , 1.482532262802124 , 0.0 , 0.0 , 0.0 , 0.0 , 3.103637933731079 , 0.46897295117378235 , 2.6465413570404053 , 2.837477445602417 , 0.6159781217575073 , 1.3220927715301514 , 0.0 , 0.24302834272384644 , 3.4662821292877197 , 0.40560781955718994 , 0.0 , 0.0 , 0.2682836055755615 , 3.5352964401245117 , 0.0 , 0.6591103672981262 , 2.5643503665924072 , 0.0 , 0.0 ] , [ 0.0 , 0.0 , 4.642599105834961 , 0.0 , 0.0 , 1.8978865146636963 , 2.2522430419921875 , 0.0 , 1.2110804319381714 , 2.5524141788482666 , 0.0 , 0.742849588394165 , 0.0 , 8.30776596069336 , 5.09386682510376 , 4.69991397857666 , 5.195588111877441 , ... ] ] &gt; , # Nx.Tensor &lt; f32 [ 2 ] [ 64 ] [ [ 0.0 , 0.0 , 0.7948622107505798 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 2.3980231285095215 , 5.2512712478637695 , 1.5820361375808716 , 0.0 , 2.6624603271484375 , 0.0 , 0.0 , 0.0 , 1.6954007148742676 , 0.017102837562561035 , 0.7754535675048828 , 0.0 , 1.891753911972046 , 0.0 , 2.7824556827545166 , 0.0 , 0.5906356573104858 , 0.0 , 0.0 , 1.288651466369629 , 0.6939071416854858 , 0.8427785038948059 , 1.5664646625518799 , 0.38097164034843445 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.3193289637565613 , 0.0 , 0.0 , 0.35316526889801025 , 0.0 , 1.2567038536071777 , 0.7732977867126465 , 0.16440902650356293 , 0.0 , 1.9872947931289673 , ... ] , ... ] &gt; } You can output maps as well: out = Axon . container ( %{ x1 : x1 , x2 : x2 } ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;container_0&quot; nodes : 6 &gt; { init_fn , predict_fn } = Axon . build ( out ) params = init_fn . ( template , %{ } ) predict_fn . ( params , Nx . iota ( { 2 , 8 } , type : :f32 ) ) %{ x1 : # Nx.Tensor &lt; f32 [ 2 ] [ 32 ] [ [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 1.8718442916870117 , 0.0 , 1.813383936882019 , 0.0 , 0.0 , 0.0 , 0.0 , 3.0636630058288574 , 0.0 , 1.1350113153457642 , 1.7888737916946411 , 0.0658932775259018 , 0.0 , 0.4498137831687927 , 1.1311852931976318 , 3.2784717082977295 , 0.0 , 2.4505443572998047 , 3.346879005432129 , 0.0 , 0.0 , 2.614570140838623 , 0.0 , 0.0 , 0.8967163562774658 , 0.0 ] , [ 0.0 , 0.0 , 0.0 , 1.9045438766479492 , 0.0 , 0.0 , 7.110898971557617 , 0.09859625995159149 , 8.149545669555664 , 0.0 , 0.0 , 0.0 , 0.0 , 4.178244113922119 , 0.0 , 3.8360297679901123 , 6.177351474761963 , ... ] ] &gt; , x2 : # Nx.Tensor &lt; f32 [ 2 ] [ 64 ] [ [ 0.41670602560043335 , 0.0 , 0.0 , 0.0 , 1.338260531425476 , 0.0 , 0.5181264877319336 , 1.1024510860443115 , 0.0 , 0.0 , 1.485485553741455 , 0.0 , 0.0 , 1.9365136623382568 , 0.0 , 0.0 , 0.0 , 0.0 , 2.6925604343414307 , 0.6202171444892883 , 0.0 , 0.08886899054050446 , 0.0 , 1.3045244216918945 , 0.0 , 0.0545249879360199 , 0.0 , 1.2294358015060425 , 0.0 , 0.0 , 0.670710563659668 , 0.0 , 4.161868572235107 , 1.880513072013855 , 2.6189277172088623 , 0.5702207684516907 , 0.0 , 1.953904151916504 , 0.0 , 0.0 , 1.370330572128296 , 0.17245425283908844 , 1.9922431707382202 , 2.6845364570617676 , 0.3711611032485962 , 0.7940037250518799 , 0.0 , 2.12975811958313 , ... ] , ... ] &gt; } Containers even support arbitrary nesting: out = Axon . container ( { %{ x1 : { x1 , x2 } , x2 : %{ x1 : x1 , x2 : { x2 } } } } ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;container_0&quot; nodes : 6 &gt; { init_fn , predict_fn } = Axon . build ( out ) params = init_fn . ( template , %{ } ) predict_fn . ( params , Nx . iota ( { 2 , 8 } , type : :f32 ) ) { %{ x1 : { # Nx.Tensor &lt; f32 [ 2 ] [ 32 ] [ [ 3.9104199409484863 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 2.051666498184204 , 0.0 , 1.086042881011963 , 0.6107193827629089 , 0.5136545896530151 , 2.7927842140197754 , 0.0 , 0.0 , 0.0 , 0.0 , 2.472961902618408 , 0.13712915778160095 , 0.49807000160217285 , 1.7868735790252686 , 5.796293258666992 , 0.0 , 0.0 , 4.727283477783203 , 0.0 , 0.0 , 2.129516363143921 , 0.0 , 0.0 ] , [ 11.746908187866211 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 4.840534687042236 , 0.0 , 0.0 , 4.103122711181641 , 1.0597835779190063 , 8.971627235412598 , ... ] ] &gt; , # Nx.Tensor &lt; f32 [ 2 ] [ 64 ] [ [ 0.951026439666748 , 0.0 , 0.6895619034767151 , 0.12973949313163757 , 3.0561492443084717 , 0.0 , 0.21812109649181366 , 0.0 , 0.6377829313278198 , 0.0 , 0.0 , 0.0 , 0.0 , 1.6837494373321533 , 0.0 , 0.0 , 0.0 , 1.3907173871994019 , 0.0 , 0.0 , 0.21352148056030273 , 0.0 , 1.2145031690597534 , 0.0 , 3.080430507659912 , 0.0 , 3.9572620391845703 , 2.3347463607788086 , 0.5280991196632385 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 1.616438627243042 , 0.0 , 1.1335082054138184 , 2.228783369064331 , 0.0 , 1.0927692651748657 , 0.0 , 0.0 , 0.0 , 0.0 , 2.7719650268554688 , ... ] , ... ] &gt; } , x2 : %{ x1 : # Nx.Tensor &lt; f32 [ 2 ] [ 32 ] [ [ 3.9104199409484863 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 2.051666498184204 , 0.0 , 1.086042881011963 , 0.6107193827629089 , 0.5136545896530151 , 2.7927842140197754 , 0.0 , 0.0 , 0.0 , 0.0 , 2.472961902618408 , 0.13712915778160095 , 0.49807000160217285 , 1.7868735790252686 , 5.796293258666992 , 0.0 , 0.0 , 4.727283477783203 , 0.0 , 0.0 , 2.129516363143921 , 0.0 , 0.0 ] , [ 11.746908187866211 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 4.840534687042236 , 0.0 , 0.0 , 4.103122711181641 , 1.0597835779190063 , ... ] ] &gt; , x2 : { # Nx.Tensor &lt; f32 [ 2 ] [ 64 ] [ [ 0.951026439666748 , 0.0 , 0.6895619034767151 , 0.12973949313163757 , 3.0561492443084717 , 0.0 , 0.21812109649181366 , 0.0 , 0.6377829313278198 , 0.0 , 0.0 , 0.0 , 0.0 , 1.6837494373321533 , 0.0 , 0.0 , 0.0 , 1.3907173871994019 , 0.0 , 0.0 , 0.21352148056030273 , 0.0 , 1.2145031690597534 , 0.0 , 3.080430507659912 , 0.0 , 3.9572620391845703 , 2.3347463607788086 , 0.5280991196632385 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 1.616438627243042 , 0.0 , 1.1335082054138184 , 2.228783369064331 , 0.0 , 1.0927692651748657 , 0.0 , 0.0 , 0.0 , ... ] , ... ] &gt; } } } }","ref":"multi_input_multi_output_models.html#creating-multi-output-models","title":"Multi-input / multi-output models - Creating multi-output models","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } , { :kino , &quot;~&gt; 0.7.0&quot; } ] ) :ok","ref":"custom_layers.html","title":"Custom layers","type":"extras"},{"doc":"While Axon has a plethora of built-in layers, more than likely you'll run into a case where you need something not provided by the framework. In these instances, you can use custom layers . To Axon, layers are really just defn implementations with special Axon inputs. Every layer in Axon (including the built-in layers), are implemented with the Axon.layer/3 function. The API of Axon.layer/3 intentionally mirrors the API of Kernel.apply/2 . To declare a custom layer you need 2 things: A defn implementation Inputs The defn implementation looks like any other defn you'd write; however, it must always account for additional opts as an argument: defmodule CustomLayers do import Nx.Defn defn my_layer ( input , opts \\\\ [ ] ) do opts = keyword! ( opts , mode : :train , alpha : 1.0 ) input |&gt; Nx . sin ( ) |&gt; Nx . multiply ( opts [ :alpha ] ) end end { :module , CustomLayers , &lt;&lt; 70 , 79 , 82 , 49 , 0 , 0 , 11 , ... &gt;&gt; , { :my_layer , 2 } } Regardless of the options you configure your layer to accept, the defn implementation will always receive a :mode option indicating whether or not the model is running in training or inference mode. You can customize the behavior of your layer depending on the mode. With an implementation defined, you need only to call Axon.layer/3 to apply our custom layer to an Axon input: input = Axon . input ( &quot;data&quot; ) out = Axon . layer ( &amp; CustomLayers . my_layer / 2 , [ input ] ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;custom_0&quot; nodes : 2 &gt; Now you can inspect and execute your model as normal: template = Nx . template ( { 2 , 8 } , :f32 ) Axon.Display . as_graph ( out , template ) graph TD ; 3 [ / &quot;data (:input) {2, 8}&quot; / ] ; 4 [ &quot;custom_0 (:custom) {2, 8}&quot; ] ; 3 -- &gt; 4 ; Notice that by default custom layers render with a default operation marked as :custom . This can make it difficult to determine which layer is which during inspection. You can control the rendering by passing :op_name to Axon.layer/3 : out = Axon . layer ( &amp; CustomLayers . my_layer / 2 , [ input ] , op_name : :my_layer ) Axon.Display . as_graph ( out , template ) graph TD ; 3 [ / &quot;data (:input) {2, 8}&quot; / ] ; 5 [ &quot;my_layer_0 (:my_layer) {2, 8}&quot; ] ; 3 -- &gt; 5 ; You can also control the name of your layer via the :name option. All other options are forwarded to the layer implementation function: out = Axon . layer ( &amp; CustomLayers . my_layer / 2 , [ input ] , name : &quot;layer&quot; , op_name : :my_layer , alpha : 2.0 ) Axon.Display . as_graph ( out , template ) graph TD ; 3 [ / &quot;data (:input) {2, 8}&quot; / ] ; 6 [ &quot;layer (:my_layer) {2, 8}&quot; ] ; 3 -- &gt; 6 ; { init_fn , predict_fn } = Axon . build ( out ) params = init_fn . ( template , %{ } ) %{ } predict_fn . ( params , Nx . iota ( { 2 , 8 } , type : :f32 ) ) # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ 0.0 , 1.6829419136047363 , 1.8185948133468628 , 0.28224000334739685 , - 1.513604998588562 , - 1.9178485870361328 , - 0.558830976486206 , 1.3139731884002686 ] , [ 1.978716492652893 , 0.8242369890213013 , - 1.0880422592163086 , - 1.9999804496765137 , - 1.073145866394043 , 0.8403340578079224 , 1.9812147617340088 , 1.3005757331848145 ] ] &gt; Notice that this model does not have any trainable parameters because none of the layers have trainable parameters. You can introduce trainable parameters by passing inputs created with Axon.param/3 to Axon.layer/3 . For example, you can modify your original custom layer to take an additional trainable parameter: defmodule CustomLayers do import Nx.Defn defn my_layer ( input , alpha , _opts \\\\ [ ] ) do input |&gt; Nx . sin ( ) |&gt; Nx . multiply ( alpha ) end end { :module , CustomLayers , &lt;&lt; 70 , 79 , 82 , 49 , 0 , 0 , 11 , ... &gt;&gt; , { :my_layer , 3 } } And then construct the layer with a regular Axon input and a trainable parameter: alpha = Axon . param ( &quot;alpha&quot; , fn _ -&gt; { } end ) out = Axon . layer ( &amp; CustomLayers . my_layer / 3 , [ input , alpha ] , op_name : :my_layer ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;my_layer_0&quot; nodes : 2 &gt; { init_fn , predict_fn } = Axon . build ( out ) params = init_fn . ( template , %{ } ) %{ &quot;my_layer_0&quot; =&gt; %{ &quot;alpha&quot; =&gt; # Nx.Tensor &lt; f32 1.194254994392395 &gt; } } Notice how your model now initializes with a trainable parameter &quot;alpha&quot; for your custom layer. Each parameter requires a unique (per-layer) string name and a function which determines the parameter's shape from the layer's input shapes. If you plan on re-using custom layers in many locations, it's recommended that you wrap them in an Elixir function as an interface: defmodule CustomLayers do import Nx.Defn def my_layer ( % Axon { } = input , opts \\\\ [ ] ) do opts = Keyword . validate! ( opts , [ :name ] ) alpha = Axon . param ( &quot;alpha&quot; , fn _ -&gt; { } end ) Axon . layer ( &amp; my_layer_impl / 3 , [ input , alpha ] , name : opts [ :name ] , op_name : :my_layer ) end defnp my_layer_impl ( input , alpha , _opts \\\\ [ ] ) do input |&gt; Nx . sin ( ) |&gt; Nx . multiply ( alpha ) end end { :module , CustomLayers , &lt;&lt; 70 , 79 , 82 , 49 , 0 , 0 , 13 , ... &gt;&gt; , { :my_layer_impl , 3 } } out = input |&gt; CustomLayers . my_layer ( ) |&gt; CustomLayers . my_layer ( ) |&gt; Axon . dense ( 1 ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;dense_0&quot; nodes : 4 &gt; Axon.Display . as_graph ( out , template ) graph TD ; 3 [ / &quot;data (:input) {2, 8}&quot; / ] ; 10 [ &quot;my_layer_0 (:my_layer) {2, 8}&quot; ] ; 12 [ &quot;my_layer_1 (:my_layer) {2, 8}&quot; ] ; 15 [ &quot;dense_0 (:dense) {2, 1}&quot; ] ; 12 -- &gt; 15 ; 10 -- &gt; 12 ; 3 -- &gt; 10 ;","ref":"custom_layers.html#creating-custom-layers","title":"Custom layers - Creating custom layers","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } ] ) :ok","ref":"model_hooks.html","title":"Model hooks","type":"extras"},{"doc":"Sometimes it's useful to inspect or visualize the values of intermediate layers in your model during the forward or backward pass. For example, it's common to visualize the gradients of activation functions to ensure your model is learning in a stable manner. Axon supports this functionality via model hooks. Model hooks are a means of unidirectional communication with an executing model. Hooks are unidirectional in the sense that you can only receive information from your model, and not send information back. Hooks are attached per-layer and can execute at 4 different points in model execution: on the pre-forward, forward, or backward pass of the model or during model initialization. You can also configure the same hook to execute on all 3 events. You can attach hooks to models using Axon.attach_hook/3 : model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . attach_hook ( fn val -&gt; IO . inspect ( val , label : :dense_forward ) end , on : :forward ) |&gt; Axon . attach_hook ( fn val -&gt; IO . inspect ( val , label : :dense_init ) end , on : :initialize ) |&gt; Axon . relu ( ) |&gt; Axon . attach_hook ( fn val -&gt; IO . inspect ( val , label : :relu ) end , on : :forward ) { init_fn , predict_fn } = Axon . build ( model ) input = Nx . iota ( { 2 , 4 } , type : :f32 ) params = init_fn . ( input , %{ } ) dense_init : %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 8 ] [ [ - 0.40611347556114197 , - 0.1551232784986496 , 0.08485020697116852 , - 0.6748610734939575 , 0.04797258973121643 , - 0.059523195028305054 , 0.4092640280723572 , 0.1300794780254364 ] , [ - 0.3551754057407379 , 0.3159058094024658 , 0.25394684076309204 , 0.22510826587677002 , 0.2613920271396637 , - 0.15213526785373688 , - 0.15744848549365997 , - 0.46065202355384827 ] , [ - 0.5224899649620056 , 0.3639957010746002 , - 0.19676287472248077 , 0.5423932075500488 , - 0.4722306430339813 , 0.26447463035583496 , 0.18534891307353973 , - 0.6442952752113342 ] , [ - 0.5629043579101562 , 0.6370815634727478 , - 0.43325361609458923 , 0.5084872245788574 , - 0.1424017995595932 , 0.4865548312664032 , - 0.5839526057243347 , 0.09811079502105713 ] ] &gt; } %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 8 ] [ [ - 0.40611347556114197 , - 0.1551232784986496 , 0.08485020697116852 , - 0.6748610734939575 , 0.04797258973121643 , - 0.059523195028305054 , 0.4092640280723572 , 0.1300794780254364 ] , [ - 0.3551754057407379 , 0.3159058094024658 , 0.25394684076309204 , 0.22510826587677002 , 0.2613920271396637 , - 0.15213526785373688 , - 0.15744848549365997 , - 0.46065202355384827 ] , [ - 0.5224899649620056 , 0.3639957010746002 , - 0.19676287472248077 , 0.5423932075500488 , - 0.4722306430339813 , 0.26447463035583496 , 0.18534891307353973 , - 0.6442952752113342 ] , [ - 0.5629043579101562 , 0.6370815634727478 , - 0.43325361609458923 , 0.5084872245788574 , - 0.1424017995595932 , 0.4865548312664032 , - 0.5839526057243347 , 0.09811079502105713 ] ] &gt; } } Notice how during initialization the :dense_init hook fired and inspected the layer's parameters. Now when executing, you'll see outputs for :dense and :relu : predict_fn . ( params , input ) dense_forward : # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ - 3.0888683795928955 , 2.955142021179199 , - 1.4393397569656372 , 2.8353562355041504 , - 1.1102746725082397 , 1.8364784717559814 , - 1.538608431816101 , - 1.454910159111023 ] , [ - 10.475601196289062 , 7.602581024169922 , - 2.604217529296875 , 5.239866733551025 , - 2.331346035003662 , 3.993962526321411 , - 2.125761032104492 , - 4.961938381195068 ] ] &gt; relu : # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ 0.0 , 2.955142021179199 , 0.0 , 2.8353562355041504 , 0.0 , 1.8364784717559814 , 0.0 , 0.0 ] , [ 0.0 , 7.602581024169922 , 0.0 , 5.239866733551025 , 0.0 , 3.993962526321411 , 0.0 , 0.0 ] ] &gt; # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ 0.0 , 2.955142021179199 , 0.0 , 2.8353562355041504 , 0.0 , 1.8364784717559814 , 0.0 , 0.0 ] , [ 0.0 , 7.602581024169922 , 0.0 , 5.239866733551025 , 0.0 , 3.993962526321411 , 0.0 , 0.0 ] ] &gt; It's important to note that hooks execute in the order they were attached to a layer. If you attach 2 hooks to the same layer which execute different functions on the same event, they will run in order: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . attach_hook ( fn val -&gt; IO . inspect ( val , label : :hook1 ) end , on : :forward ) |&gt; Axon . attach_hook ( fn val -&gt; IO . inspect ( val , label : :hook2 ) end , on : :forward ) |&gt; Axon . relu ( ) { init_fn , predict_fn } = Axon . build ( model ) params = init_fn . ( input , %{ } ) predict_fn . ( params , input ) hook1 : # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ 1.3320910930633545 , 1.712153673171997 , - 2.0420351028442383 , 2.2541849613189697 , - 3.1382551193237305 , - 1.2241677045822144 , - 1.5477651357650757 , - 0.2126261293888092 ] , [ 2.1975531578063965 , 3.722827911376953 , - 1.6301460266113281 , 5.891226768493652 , - 10.79372787475586 , - 2.9982359409332275 , - 6.589874267578125 , 1.5387766361236572 ] ] &gt; hook2 : # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ 1.3320910930633545 , 1.712153673171997 , - 2.0420351028442383 , 2.2541849613189697 , - 3.1382551193237305 , - 1.2241677045822144 , - 1.5477651357650757 , - 0.2126261293888092 ] , [ 2.1975531578063965 , 3.722827911376953 , - 1.6301460266113281 , 5.891226768493652 , - 10.79372787475586 , - 2.9982359409332275 , - 6.589874267578125 , 1.5387766361236572 ] ] &gt; # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ 1.3320910930633545 , 1.712153673171997 , 0.0 , 2.2541849613189697 , 0.0 , 0.0 , 0.0 , 0.0 ] , [ 2.1975531578063965 , 3.722827911376953 , 0.0 , 5.891226768493652 , 0.0 , 0.0 , 0.0 , 1.5387766361236572 ] ] &gt; Notice that :hook1 fires before :hook2 . You can also specify a hook to fire on all events: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 , on : :all ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) { init_fn , predict_fn } = Axon . build ( model ) { # Function &lt; 136.40088443 / 2 in Nx.Defn . wrap_arity / 2 &gt; , # Function &lt; 136.40088443 / 2 in Nx.Defn . wrap_arity / 2 &gt; } On initialization: params = init_fn . ( input , %{ } ) %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 8 ] [ [ 0.6784419417381287 , 0.175045907497406 , 0.010701040737330914 , - 0.5537784695625305 , - 0.010694148950278759 , 0.7021086812019348 , - 0.3290281891822815 , - 0.6818609237670898 ] , [ - 0.6378231644630432 , - 0.5675055384635925 , 0.031453751027584076 , 0.4705190360546112 , - 0.002226108219474554 , 0.48611924052238464 , 0.5700677037239075 , 0.6729928851127625 ] , [ 0.4596043527126312 , - 0.6557875871658325 , - 0.07168347388505936 , - 0.37926459312438965 , - 0.20766735076904297 , 0.11274437606334686 , - 0.5166378617286682 , - 0.5115087032318115 ] , [ - 0.30842259526252747 , - 0.3418923616409302 , 0.3374936282634735 , 0.6272460222244263 , 0.6156857013702393 , 0.6739501357078552 , - 0.09081890434026718 , 0.706954836845398 ] ] &gt; } %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 8 ] [ [ 0.6784419417381287 , 0.175045907497406 , 0.010701040737330914 , - 0.5537784695625305 , - 0.010694148950278759 , 0.7021086812019348 , - 0.3290281891822815 , - 0.6818609237670898 ] , [ - 0.6378231644630432 , - 0.5675055384635925 , 0.031453751027584076 , 0.4705190360546112 , - 0.002226108219474554 , 0.48611924052238464 , 0.5700677037239075 , 0.6729928851127625 ] , [ 0.4596043527126312 , - 0.6557875871658325 , - 0.07168347388505936 , - 0.37926459312438965 , - 0.20766735076904297 , 0.11274437606334686 , - 0.5166378617286682 , - 0.5115087032318115 ] , [ - 0.30842259526252747 , - 0.3418923616409302 , 0.3374936282634735 , 0.6272460222244263 , 0.6156857013702393 , 0.6739501357078552 , - 0.09081890434026718 , 0.706954836845398 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.0 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 1 ] [ [ - 0.7136709690093994 ] , [ - 0.16328231990337372 ] , [ 0.08359552919864655 ] , [ 0.07646285742521286 ] , [ 0.7133787274360657 ] , [ - 0.00617210753262043 ] , [ 0.2241944670677185 ] , [ - 0.055933959782123566 ] ] &gt; } } On pre-forward and forward: predict_fn . ( params , input ) # Nx.Tensor &lt; f32 [ 2 ] [ 4 ] [ [ 0.0 , 1.0 , 2.0 , 3.0 ] , [ 4.0 , 5.0 , 6.0 , 7.0 ] ] &gt; # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ - 0.6438822746276855 , - 2.9047577381134033 , 0.9005677103996277 , 1.593727946281433 , 1.4294962882995605 , 2.7334585189819336 , - 0.7356647253036499 , 1.7708399295806885 ] , [ 0.12331989407539368 , - 8.465315818786621 , 2.132427453994751 , 2.2526159286499023 , 3.0098886489868164 , 10.633148193359375 , - 2.20133376121521 , 2.5171523094177246 ] ] &gt; # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ - 0.6438822746276855 , - 2.9047577381134033 , 0.9005677103996277 , 1.593727946281433 , 1.4294962882995605 , 2.7334585189819336 , - 0.7356647253036499 , 1.7708399295806885 ] , [ 0.12331989407539368 , - 8.465315818786621 , 2.132427453994751 , 2.2526159286499023 , 3.0098886489868164 , 10.633148193359375 , - 2.20133376121521 , 2.5171523094177246 ] ] &gt; # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] [ [ 1.100995421409607 ] , [ 2.2032604217529297 ] ] &gt; And on backwards: Nx.Defn . grad ( fn params -&gt; predict_fn . ( params , input ) end ) . ( params ) # Nx.Tensor &lt; f32 [ 2 ] [ 4 ] [ [ 0.0 , 1.0 , 2.0 , 3.0 ] , [ 4.0 , 5.0 , 6.0 , 7.0 ] ] &gt; # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ - 0.6438822746276855 , - 2.9047577381134033 , 0.9005677103996277 , 1.593727946281433 , 1.4294962882995605 , 2.7334585189819336 , - 0.7356647253036499 , 1.7708399295806885 ] , [ 0.12331989407539368 , - 8.465315818786621 , 2.132427453994751 , 2.2526159286499023 , 3.0098886489868164 , 10.633148193359375 , - 2.20133376121521 , 2.5171523094177246 ] ] &gt; # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ - 0.6438822746276855 , - 2.9047577381134033 , 0.9005677103996277 , 1.593727946281433 , 1.4294962882995605 , 2.7334585189819336 , - 0.7356647253036499 , 1.7708399295806885 ] , [ 0.12331989407539368 , - 8.465315818786621 , 2.132427453994751 , 2.2526159286499023 , 3.0098886489868164 , 10.633148193359375 , - 2.20133376121521 , 2.5171523094177246 ] ] &gt; %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ - 0.7136709690093994 , 0.0 , 0.1671910583972931 , 0.15292571485042572 , 1.4267574548721313 , - 0.01234421506524086 , 0.0 , - 0.11186791956424713 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 8 ] [ [ - 2.8546838760375977 , 0.0 , 0.3343821167945862 , 0.30585142970085144 , 2.8535149097442627 , - 0.02468843013048172 , 0.0 , - 0.22373583912849426 ] , [ - 3.568354845046997 , 0.0 , 0.5015732049942017 , 0.45877712965011597 , 4.280272483825684 , - 0.03703264519572258 , 0.0 , - 0.3356037735939026 ] , [ - 4.2820258140563965 , 0.0 , 0.6687642335891724 , 0.6117028594017029 , 5.707029819488525 , - 0.04937686026096344 , 0.0 , - 0.4474716782569885 ] , [ - 4.995697021484375 , 0.0 , 0.8359552621841431 , 0.7646285891532898 , 7.133787155151367 , - 0.0617210753262043 , 0.0 , - 0.5593395829200745 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 2.0 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 1 ] [ [ 0.12331989407539368 ] , [ 0.0 ] , [ 3.0329952239990234 ] , [ 3.846343994140625 ] , [ 4.439384937286377 ] , [ 13.366606712341309 ] , [ 0.0 ] , [ 4.287992477416992 ] ] &gt; } } Finally, you can specify hooks to only run when the model is built in a certain mode such as training and inference mode. You can read more about training and inference mode in Training and inference mode : model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 , on : :forward , mode : :train ) |&gt; Axon . relu ( ) { init_fn , predict_fn } = Axon . build ( model , mode : :train ) params = init_fn . ( input , %{ } ) %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 8 ] [ [ 0.13930729031562805 , 0.6213980913162231 , 0.5555388331413269 , - 0.18602639436721802 , 0.37516212463378906 , 0.025288991630077362 , 0.5311357378959656 , 0.2825106978416443 ] , [ - 0.14007511734962463 , - 0.1472432166337967 , - 0.011716545559465885 , 0.06804006546735764 , 0.4615606963634491 , - 0.024897094815969467 , - 0.2336975485086441 , 0.10019711405038834 ] , [ - 0.29539188742637634 , - 0.5487134456634521 , 0.41018739342689514 , - 0.49597275257110596 , 0.2970600426197052 , 0.4304136335849762 , 0.13961079716682434 , - 0.4316418170928955 ] , [ 0.5435506105422974 , - 0.056049738079309464 , 0.5059406161308289 , 0.29488587379455566 , 0.5656863451004028 , 0.43807661533355713 , - 0.5058187246322632 , - 0.6963644623756409 ] ] &gt; } } The model was built in training mode so the hook will run: predict_fn . ( params , input ) # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ 0.8997929096221924 , - 1.412819266319275 , 2.3264801502227783 , - 0.039247818291187286 , 2.752739906311035 , 2.150160074234009 , - 1.4719321727752686 , - 2.852180004119873 ] , [ 1.8893564939498901 , - 1.9352525472640991 , 8.166281700134277 , - 1.3155406713485718 , 9.550616264343262 , 5.625688552856445 , - 1.7470110654830933 , - 5.833373546600342 ] ] &gt; %{ prediction : # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ 0.8997929096221924 , 0.0 , 2.3264801502227783 , 0.0 , 2.752739906311035 , 2.150160074234009 , 0.0 , 0.0 ] , [ 1.8893564939498901 , 0.0 , 8.166281700134277 , 0.0 , 9.550616264343262 , 5.625688552856445 , 0.0 , 0.0 ] ] &gt; , state : %{ } } { init_fn , predict_fn } = Axon . build ( model , mode : :inference ) params = init_fn . ( input , %{ } ) %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 8 ] [ [ 0.4261569678783417 , - 0.6842133402824402 , - 0.13853907585144043 , 0.6665098667144775 , 0.6171062588691711 , 0.25513389706611633 , - 0.4866299033164978 , - 0.5819953680038452 ] , [ - 0.36037471890449524 , - 0.21852241456508636 , - 0.6355746388435364 , - 0.5705516934394836 , - 0.35449153184890747 , - 0.1527744084596634 , - 0.5036700367927551 , - 0.4164859354496002 ] , [ 0.6485253572463989 , 0.30033791065216064 , 0.35249730944633484 , - 0.31768497824668884 , 0.020564774051308632 , 0.147691547870636 , 0.6939279437065125 , 0.6060985922813416 ] , [ 0.006978582590818405 , 0.5333927869796753 , 0.30155065655708313 , - 0.09574121236801147 , 0.3447912037372589 , - 0.11081335693597794 , 0.5808792114257812 , 0.04360806941986084 ] ] &gt; } } The model was built in inference mode so the hook will not run: predict_fn . ( params , input ) # Nx.Tensor &lt; f32 [ 2 ] [ 8 ] [ [ 0.9576117396354675 , 1.9823317527770996 , 0.9740719795227051 , 0.0 , 0.7210116386413574 , 0.0 , 2.6268234252929688 , 0.9265354871749878 ] , [ 3.842756509780884 , 1.706311583518982 , 0.49380895495414734 , 0.0 , 3.2328944206237793 , 0.36711934208869934 , 3.764852285385132 , 0.0 ] ] &gt;","ref":"model_hooks.html#creating-models-with-hooks","title":"Model hooks - Creating models with hooks","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :exla , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;exla&quot; } , { :torchx , github : &quot;elixir-nx/nx&quot; , sparse : &quot;torchx&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } , { :benchee , github : &quot;akoutmos/benchee&quot; , branch : :adding_table_support } , { :kino_benchee , github : &quot;livebook-dev/kino_benchee&quot; } , { :kino , &quot;~&gt; 0.7.0&quot; , override : true } ] ) :ok","ref":"accelerating_axon.html","title":"Accelerating Axon","type":"extras"},{"doc":"Axon is built entirely on top of Nx's numerical definitions defn . Functions declared with defn tell Nx to use just-in-time compilation to compile and execute the given numerical definition with an available Nx compiler. Numerical definitions enable acceleration on CPU/GPU/TPU via pluggable compilers. At the time of this writing, Nx has 2 officially supported compiler/backends on top of the default BinaryBackend : EXLA - Acceleration via Google's XLA project TorchX - Bindings to LibTorch By default, Nx and Axon run all computations using the BinaryBackend which is a pure Elixir implementation of various numerical routines. The BinaryBackend is guaranteed to run wherever an Elixir installation runs; however, it is very slow. Due to the computational expense of neural networks, you should basically never use the BinaryBackend and instead opt for one of the available accelerated libraries. There are several ways to make use of Nx compilers from within Axon. First, create a simple model for benchmarking purposes: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 32 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) |&gt; Axon . softmax ( ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;softmax_0&quot; nodes : 5 &gt; By default, Axon will respect the default defn compilation options. You can set compilation options globally or per-process: # Sets the global compilation options Nx.Defn . global_default_options ( compiler : EXLA ) # Sets the process-level compilation options Nx.Defn . default_options ( compiler : EXLA ) [ compiler : EXLA ] When you call Axon.build/2 , Axon automatically marks your initialization and forward functions as JIT compiled functions. When you invoke them, they will compile a specialized version of the function using your default compiler options: inputs = Nx . random_uniform ( { 2 , 128 } ) { init_fn , predict_fn } = Axon . build ( model ) params = init_fn . ( inputs , %{ } ) predict_fn . ( params , inputs ) 10 : 34 : 02.503 [ info ] XLA service 0x7fbd5468c170 initialized for platform Host ( this does not guarantee that XLA will be used ) . Devices : 10 : 34 : 02.785 [ info ] StreamExecutor device ( 0 ) : Host , Default Version # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] EXLA.Backend &lt; host : 0 , 0.184501844 . 1168769032.259095 &gt; [ [ 1.0 ] , [ 1.0 ] ] &gt; Notice that the inspected tensor indicates the computation has been dispatched to EXLA and the tensor's data points to an EXLA buffer. If you feel like setting the global or process-level compilation options is too intrusive, you can opt for more explicit behavior in a few ways. First, you can specify the JIT compiler when you build the model: # Set back to defaults Nx.Defn . global_default_options ( [ ] ) Nx.Defn . default_options ( [ ] ) [ compiler : EXLA ] { init_fn , predict_fn } = Axon . build ( model , compiler : EXLA ) params = init_fn . ( inputs , %{ } ) predict_fn . ( params , inputs ) # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] EXLA.Backend &lt; host : 0 , 0.184501844 . 1168769032.259101 &gt; [ [ 1.0 ] , [ 1.0 ] ] &gt; You can also instead JIT compile functions explicitly via the Nx.Defn.jit or compiler-specific JIT APIs. This is useful when running benchmarks against various backends: { init_fn , predict_fn } = Axon . build ( model ) # These will both JIT compile with EXLA exla_init_fn = Nx.Defn . jit ( init_fn , compiler : EXLA ) exla_predict_fn = EXLA . jit ( predict_fn ) # Function &lt; 136.40088443 / 2 in Nx.Defn . wrap_arity / 2 &gt; Benchee . run ( %{ &quot;elixir init&quot; =&gt; fn -&gt; init_fn . ( inputs , %{ } ) end , &quot;exla init&quot; =&gt; fn -&gt; exla_init_fn . ( inputs , %{ } ) end } , time : 10 , memory_time : 5 , warmup : 2 ) Warning : the benchmark elixir init is using an evaluated function . Evaluated functions perform slower than compiled functions . You can move the Benchee caller to a function in a module and invoke ` Mod . fun ( ) ` instead . Alternatively , you can move the benchmark into a benchmark . exs file and run mix run benchmark . exs Warning : the benchmark exla init is using an evaluated function . Evaluated functions perform slower than compiled functions . You can move the Benchee caller to a function in a module and invoke ` Mod . fun ( ) ` instead . Alternatively , you can move the benchmark into a benchmark . exs file and run mix run benchmark . exs Operating System : Linux CPU Information : Intel ( R ) Core ( TM ) i7 - 7600 U CPU @ 2.80 GHz Number of Available Cores : 4 Available memory : 24.95 GB Elixir 1.13 . 4 Erlang 25.0 . 4 Benchmark suite executing with the following configuration : warmup : 2 s time : 10 s memory time : 5 s reduction time : 0 ns parallel : 1 inputs : none specified Estimated total run time : 34 s Benchmarking elixir init ... Benchmarking exla init ... Name ips average deviation median 99 th % exla init 3.79 K 0.26 ms  100.40 % 0.24 ms 0.97 ms elixir init 0.52 K 1.91 ms  35.03 % 1.72 ms 3.72 ms Comparison : exla init 3.79 K elixir init 0.52 K - 7.25 x slower + 1.65 ms Memory usage statistics : Name Memory usage exla init 9.80 KB elixir init 644.63 KB - 65.80 x memory usage + 634.83 KB * * All measurements for memory usage were the same * * Benchee . run ( %{ &quot;elixir predict&quot; =&gt; fn -&gt; predict_fn . ( params , inputs ) end , &quot;exla predict&quot; =&gt; fn -&gt; exla_predict_fn . ( params , inputs ) end } , time : 10 , memory_time : 5 , warmup : 2 ) Warning : the benchmark elixir predict is using an evaluated function . Evaluated functions perform slower than compiled functions . You can move the Benchee caller to a function in a module and invoke ` Mod . fun ( ) ` instead . Alternatively , you can move the benchmark into a benchmark . exs file and run mix run benchmark . exs Warning : the benchmark exla predict is using an evaluated function . Evaluated functions perform slower than compiled functions . You can move the Benchee caller to a function in a module and invoke ` Mod . fun ( ) ` instead . Alternatively , you can move the benchmark into a benchmark . exs file and run mix run benchmark . exs Operating System : Linux CPU Information : Intel ( R ) Core ( TM ) i7 - 7600 U CPU @ 2.80 GHz Number of Available Cores : 4 Available memory : 24.95 GB Elixir 1.13 . 4 Erlang 25.0 . 4 Benchmark suite executing with the following configuration : warmup : 2 s time : 10 s memory time : 5 s reduction time : 0 ns parallel : 1 inputs : none specified Estimated total run time : 34 s Benchmarking elixir predict ... Benchmarking exla predict ... Name ips average deviation median 99 th % exla predict 2.32 K 0.43 ms  147.05 % 0.34 ms 1.61 ms elixir predict 0.28 K 3.53 ms  42.21 % 3.11 ms 7.26 ms Comparison : exla predict 2.32 K elixir predict 0.28 K - 8.20 x slower + 3.10 ms Memory usage statistics : Name Memory usage exla predict 10.95 KB elixir predict 91.09 KB - 8.32 x memory usage + 80.14 KB * * All measurements for memory usage were the same * * Notice how calls to EXLA variants are significantly faster than their Elixir counterparts. These speedups become more pronounced with more complex models and workflows. It's important to note that in order to use a given library as an Nx compiler, it must implement the Nx compilation behaviour. For example, you cannot invoke Torchx as an Nx compiler because it does not support JIT compilation at this time.","ref":"accelerating_axon.html#using-nx-compilers-in-axon","title":"Accelerating Axon - Using Nx Compilers in Axon","type":"extras"},{"doc":"In addition to JIT-compilation, Axon also supports the usage of Nx backends. Nx backends are slightly different than Nx compilers in the sense that they do not fuse calls within numerical definitions. Backends are more eager, sacrificing a bit of performance for convenience. Torchx and EXLA both support running via backends. Again, Axon will respect the global and process-level Nx backend configuration options. You can set the default backend using: # Global default backend Nx . global_default_backend ( Torchx.Backend ) # Process default backend Nx . default_backend ( Torchx.Backend ) { Nx.BinaryBackend , [ ] } Now when you invoke model functions, it will run them with the given backend: { init_fn , predict_fn } = Axon . build ( model ) params = init_fn . ( inputs , %{ } ) predict_fn . ( params , inputs ) # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] Torchx.Backend ( cpu ) [ [ 1.0 ] , [ 1.0 ] ] &gt; # Global default backend Nx . global_default_backend ( EXLA.Backend ) # Process default backend Nx . default_backend ( EXLA.Backend ) { Torchx.Backend , [ ] } { init_fn , predict_fn } = Axon . build ( model ) params = init_fn . ( inputs , %{ } ) predict_fn . ( params , inputs ) # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] EXLA.Backend &lt; host : 0 , 0.184501844 . 1169293320.110725 &gt; [ [ 1.0 ] , [ 1.0 ] ] &gt; Unlike with JIT-compilation, you must set the backend at the top-level in order to invoke it. You should be careful using multiple backends in the same project as attempting to mix tensors between backends may result in strange performance bugs or errors. With most larger models, using a JIT compiler will be more performant than using a backend.","ref":"accelerating_axon.html#using-nx-backends-in-axon","title":"Accelerating Axon - Using Nx Backends in Axon","type":"extras"},{"doc":"While Nx mostly tries to standardize behavior across compilers and backends, some behaviors are backend-specific. For example, the API for choosing an acceleration platform (e.g. CUDA/ROCm/TPU) is backend-specific. You should refer to your chosen compiler or backend's documentation for information on targeting various accelerators. Typically, you only need to change a few configuration options and your code will run as-is on a chosen accelerator.","ref":"accelerating_axon.html#a-note-on-cpus-gpus-tpus","title":"Accelerating Axon - A Note on CPUs/GPUs/TPUs","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } ] ) :ok","ref":"training_and_inference_mode.html","title":"Training and inference mode","type":"extras"},{"doc":"Some layers have different considerations and behavior when running during model training versus model inference. For example dropout layers are intended only to be used during training as a form of model regularization. Certain stateful layers like batch normalization keep a running-internal state which changes during training mode but remains fixed during inference mode. Axon supports mode-dependent execution behavior via the :mode option passed to all building, compilation, and execution methods. By default, all models build in inference mode. You can see this behavior by adding a dropout layer with a dropout rate of 1. In inference mode this layer will have no affect: inputs = Nx . iota ( { 2 , 8 } , type : :f32 ) model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 4 ) |&gt; Axon . sigmoid ( ) |&gt; Axon . dropout ( rate : 0.99 ) |&gt; Axon . dense ( 1 ) { init_fn , predict_fn } = Axon . build ( model ) params = init_fn . ( inputs , %{ } ) predict_fn . ( params , inputs ) # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] [ [ - 0.6138466000556946 ] , [ - 0.8409845232963562 ] ] &gt; You can also explicitly specify the mode: { init_fn , predict_fn } = Axon . build ( model , mode : :inference ) params = init_fn . ( inputs , %{ } ) predict_fn . ( params , inputs ) # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] [ [ 0.7551136016845703 ] , [ 0.448221355676651 ] ] &gt; It's important that you know which mode your model's were compiled for, as running a model built in :inference mode will behave drastically different than a model built in :train mode.","ref":"training_and_inference_mode.html#executing-models-in-inference-mode","title":"Training and inference mode - Executing models in inference mode","type":"extras"},{"doc":"By specifying mode: :train , you tell your models to execute in training mode. You can see the effects of this behavior here: { init_fn , predict_fn } = Axon . build ( model , mode : :train ) params = init_fn . ( inputs , %{ } ) predict_fn . ( params , inputs ) %{ prediction : # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] [ [ 0.0 ] , [ 0.0 ] ] &gt; , state : %{ } } First, notice that your model now returns a map with keys :prediction and :state . :prediction contains the actual model prediction, while :state contains the updated state for any stateful layers such as batch norm. When writing custom training loops, you should extract :state and use it in conjunction with the updates API to ensure your stateful layers are updated correctly. If your model has stateful layers, :state will look similar to your model's parameter map: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 4 ) |&gt; Axon . sigmoid ( ) |&gt; Axon . batch_norm ( ) |&gt; Axon . dense ( 1 ) { init_fn , predict_fn } = Axon . build ( model , mode : :train ) params = init_fn . ( inputs , %{ } ) predict_fn . ( params , inputs ) %{ prediction : # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] [ [ 0.03675001487135887 ] , [ - 0.03674999624490738 ] ] &gt; , state : %{ &quot;batch_norm_0&quot; =&gt; %{ &quot;mean&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.8784151673316956 , 0.7386987209320068 , 0.663623571395874 , 0.8947045803070068 ] &gt; , &quot;var&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.10050597041845322 , 0.11294332146644592 , 0.16061438620090485 , 0.10003116726875305 ] &gt; } } }","ref":"training_and_inference_mode.html#executing-models-in-training-mode","title":"Training and inference mode - Executing models in training mode","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } ] ) :ok","ref":"your_first_training_loop.html","title":"Your first training loop","type":"extras"},{"doc":"Axon generalizes the concept of training, evaluation, hyperparameter optimization, and more into the Axon.Loop API. Axon loops are a instrumented reductions over Elixir Streams - that basically means you can accumulate some state over an Elixir Stream and control different points in the loop execution. With Axon, you'll most commonly implement and work with supervised training loops. Because supervised training loops are so common in deep learning, Axon has a loop factory function which takes care of most of the boilerplate of creating a supervised training loop for you. In the beginning of your deep learning journey, you'll almost exclusively use Axon's loop factories to create and run loops. Axon's supervised training loop assumes you have an input stream of data with entries that look like: {batch_inputs, batch_labels} Each entry is a batch of input data with a corresponding batch of labels. You can simulate some real training data by constructing an Elixir stream: train_data = Stream . repeatedly ( fn -&gt; xs = Nx . random_normal ( { 8 , 1 } ) ys = Nx . sin ( xs ) { xs , ys } end ) # Function &lt; 50.127921642 / 2 in Stream . repeatedly / 1 &gt; The most basic supervised training loop in Axon requires 3 things: An Axon model A loss function An optimizer You can construct an Axon model using the knowledge you've gained from going through the model creation guides: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) # Axon &lt; inputs : %{ &quot;data&quot; =&gt; nil } outputs : &quot;dense_2&quot; nodes : 6 &gt; Axon comes with built-in loss functions and optimizers which you can use directly when constructing your training loop. To construct your training loop, you use Axon.Loop.trainer/3 : loop = Axon.Loop . trainer ( model , :mean_squared_error , :sgd ) # Axon.Loop &lt; handlers : %{ completed : [ ] , epoch_completed : [ { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 5.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , epoch_halted : [ ] , epoch_started : [ ] , halted : [ ] , iteration_completed : [ { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 3.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , iteration_started : [ ] , started : [ ] } , metrics : %{ &quot;loss&quot; =&gt; { # Function &lt; 12.17233431 / 3 in Axon.Metrics . running_average / 1 &gt; , # Function &lt; 6.20267452 / 2 in Axon.Loop . build_loss_fn / 1 &gt; } } , ... &gt; You'll notice that Axon.Loop.trainer/3 returns an %Axon.Loop{} data structure. This data structure contains information which Axon uses to control the execution of the loop. In order to run the loop, you need to explicitly pass it to Axon.Loop.run/4 : Axon.Loop . run ( loop , train_data , %{ } , iterations : 1000 ) Epoch : 0 , Batch : 1000 , loss : 0.0421094 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.18567155301570892 , - 0.24138866364955902 , 0.13732704520225525 , 0.2081741988658905 , 0.013805730268359184 , 0.18336650729179382 , 0.07754829525947571 , - 0.12579604983329773 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ 0.06517036259174347 , - 0.7166120409965515 , 0.649202823638916 , - 0.3636767566204071 , 0.33472830057144165 , - 0.6622008681297302 , - 0.6205887198448181 , - 0.1951046586036682 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.2652607262134552 , 0.1563350260257721 , - 0.12963515520095825 , - 0.15289783477783203 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ 0.5483533143997192 , 0.16270962357521057 , - 0.29001912474632263 , 0.16584330797195435 ] , [ - 0.3257339596748352 , 0.6900827884674072 , 0.17480286955833435 , - 0.5176011323928833 ] , [ - 0.5791758298873901 , 0.7136418223381042 , 0.2863248288631439 , 0.2406335324048996 ] , [ 0.5999854803085327 , - 0.09972921013832092 , 0.16846133768558502 , 0.21690420806407928 ] , [ 0.10213596373796463 , 0.01878557913005352 , 0.03252492845058441 , - 0.25937923789024353 ] , [ 0.4094444811344147 , - 0.48399242758750916 , 0.18455447256565094 , 0.40939682722091675 ] , [ 0.2809498906135559 , 0.7121831178665161 , 0.42944926023483276 , - 0.4959437847137451 ] , [ - 0.21076196432113647 , - 0.3021833896636963 , - 0.46126121282577515 , - 0.5571116805076599 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.3293934762477875 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ - 1.041453242301941 ] , [ 0.6521084308624268 ] , [ - 0.5688052773475647 ] , [ - 0.5789349675178528 ] ] &gt; } } Axon.Loop.run/4 expects a loop to execute, some data to loop over, and any initial state you explicitly want your loop to start with. Axon.Loop.run/4 will then iterate over your data, executing a step function on each batch, and accumulating some generic loop state. In the case of a supervised training loop, this generic loop state actually represents training state including your model's trained parameters. Axon.Loop.run/4 also accepts options which control the loops execution. This includes :iterations which controls the number of iterations per epoch a loop should execute for, and :epochs which controls the number of epochs a loop should execute for: Axon.Loop . run ( loop , train_data , %{ } , epochs : 3 , iterations : 500 ) Epoch : 0 , Batch : 500 , loss : 0.0376754 Epoch : 1 , Batch : 500 , loss : 0.0300909 Epoch : 2 , Batch : 500 , loss : 0.0260511 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ - 0.09743800014257431 , 0.36350908875465393 , 0.23338767886161804 , 0.21299506723880768 , - 0.04753172770142555 , - 0.03144805133342743 , 0.0230794008821249 , - 0.17029045522212982 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ - 0.14422392845153809 , - 0.3840259611606598 , 0.7611677050590515 , 0.1216919794678688 , - 0.4270862638950348 , 0.43146076798439026 , - 0.3569082021713257 , 0.4051334857940674 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.21392156183719635 , 0.02405611053109169 , 0.2970339059829712 , 0.02390623465180397 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.12441369146108627 , 0.44625332951545715 , - 0.2095455527305603 , - 0.28127536177635193 ] , [ 0.6052687764167786 , 0.1358352154493332 , - 0.24579593539237976 , 0.6278529167175293 ] , [ - 0.5855410695075989 , 0.014370989985764027 , 0.4479483664035797 , - 0.07460466772317886 ] , [ 0.5286814570426941 , - 0.6323351263999939 , 0.4167028069496155 , - 0.4724753797054291 ] , [ - 0.3705250918865204 , 0.41602230072021484 , - 0.626926600933075 , - 0.03850430250167847 ] , [ 0.22140666842460632 , - 0.6492624878883362 , 0.09525017440319061 , 0.3179352283477783 ] , [ - 0.27787405252456665 , 0.43634578585624695 , 0.2430884689092636 , 0.18133315443992615 ] , [ 0.4248749911785126 , - 0.059922583401203156 , - 0.09462974965572357 , 0.57406085729599 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.015223611146211624 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ - 0.6736029386520386 ] , [ - 0.019722800701856613 ] , [ 0.932664692401886 ] , [ - 0.9208926558494568 ] ] &gt; } } You may have noticed that by default Axon.Loop.trainer/3 configures your loop to log information about training progress every 50 iterations. You can control this when constructing your supervised training loop with the :log option: model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd , log : 100 ) |&gt; Axon.Loop . run ( train_data , %{ } , iterations : 1000 ) Epoch : 0 , Batch : 1000 , loss : 0.0700251 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ - 0.10562735795974731 , 0.3525764048099518 , - 0.0731351301074028 , 0.3316117525100708 , - 0.08621923625469208 , 0.15377338230609894 , 0.02795499749481678 , 0.19813594222068787 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ 0.46547073125839233 , - 0.3838779926300049 , 0.06413891166448593 , 0.6604263186454773 , 0.09603694081306458 , - 0.3142688274383545 , - 0.0673874095082283 , - 0.1551232486963272 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.16770508885383606 , - 0.11785938590765 , - 0.08730955421924591 , 0.18854482471942902 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.32443270087242126 , 0.33927711844444275 , 0.5110990405082703 , - 0.34353166818618774 ] , [ 0.6843343377113342 , - 0.09189904481172562 , 0.4550926983356476 , - 0.27025723457336426 ] , [ 0.029612643644213676 , 0.3680649697780609 , 0.5105444192886353 , - 0.1120513379573822 ] , [ - 0.12359219789505005 , - 0.2177252620458603 , - 0.2753210961818695 , 0.7462171912193298 ] , [ 0.2723115086555481 , 0.39580288529396057 , - 0.41799622774124146 , 0.003858723910525441 ] , [ 0.21861012279987335 , - 0.37737029790878296 , - 0.5444738268852234 , - 0.12978340685367584 ] , [ 0.12569139897823334 , 0.09505560994148254 , 0.13603702187538147 , 0.20154744386672974 ] , [ 0.4721740484237671 , 0.27258655428886414 , - 0.6905713677406311 , 0.09732398390769958 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.2536466121673584 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ - 0.9850672483444214 ] , [ - 0.5319440960884094 ] , [ - 0.8099393844604492 ] , [ 0.6502916216850281 ] ] &gt; } }","ref":"your_first_training_loop.html#creating-an-axon-training-loop","title":"Your first training loop - Creating an Axon training loop","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } ] ) :ok","ref":"instrumenting_loops_with_metrics.html","title":"Instrumenting loops with metrics","type":"extras"},{"doc":"Often times when executing a loop you want to keep track of various metrics such as accuracy or precision. For training loops, Axon by default only tracks loss; however, you can instrument the loop with additional built-in metrics. For example, you might want to track mean-absolute error on top of a mean-squared error loss: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) loop = model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . metric ( :mean_absolute_error ) # Axon.Loop &lt; handlers : %{ completed : [ ] , epoch_completed : [ { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 5.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , epoch_halted : [ ] , epoch_started : [ ] , halted : [ ] , iteration_completed : [ { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 3.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , iteration_started : [ ] , started : [ ] } , metrics : %{ &quot;loss&quot; =&gt; { # Function &lt; 12.6031754 / 3 in Axon.Metrics . running_average / 1 &gt; , # Function &lt; 6.20267452 / 2 in Axon.Loop . build_loss_fn / 1 &gt; } , &quot;mean_absolute_error&quot; =&gt; { # Function &lt; 12.6031754 / 3 in Axon.Metrics . running_average / 1 &gt; , :mean_absolute_error } } , ... &gt; When specifying a metric, you can specify an atom which maps to any of the metrics defined in Axon.Metrics . You can also define custom metrics. For more information on custom metrics, see Writing custom metrics . When you run a loop with metrics, Axon will aggregate that metric over the course of the loop execution. For training loops, Axon will also report the aggregate metric in the training logs: train_data = Stream . repeatedly ( fn -&gt; xs = Nx . random_normal ( { 8 , 1 } ) ys = Nx . sin ( xs ) { xs , ys } end ) Axon.Loop . run ( loop , train_data , %{ } , iterations : 1000 ) Epoch : 0 , Batch : 1000 , loss : 0.0646209 mean_absolute_error : 0.1720028 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ - 0.2462722808122635 , 0.18984302878379822 , 0.0016971784643828869 , 0.19568635523319244 , 0.33571094274520874 , 0.07703055441379547 , 0.29576605558395386 , 0.14511419832706451 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ - 0.7807592749595642 , - 0.17303702235221863 , 0.43004679679870605 , - 0.46043306589126587 , - 0.6577866077423096 , 0.7490359544754028 , - 0.5164405703544617 , - 0.77418452501297 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.027583779767155647 , 0.4279942214488983 , - 0.10632428526878357 , - 0.05149337649345398 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.5688502192497253 , - 0.49978527426719666 , 0.0660838857293129 , 0.30804139375686646 ] , [ 0.21578946709632874 , 0.4183472990989685 , 0.530754566192627 , 0.1742597073316574 ] , [ - 0.17872463166713715 , - 0.08955764025449753 , - 0.7048909664154053 , 0.053243234753608704 ] , [ - 0.41064000129699707 , 0.3491946756839752 , 0.3753710091114044 , 0.6630277037620544 ] , [ - 0.1781950145959854 , 0.5766432881355286 , 0.5829672813415527 , - 0.34879636764526367 ] , [ - 0.026939965784549713 , - 0.44429031014442444 , - 0.12619371712207794 , 0.0030224998481571674 ] , [ 0.411702424287796 , 0.3330642879009247 , - 0.5062007308006287 , - 0.0731467455625534 ] , [ - 0.41474586725234985 , 0.23881299793720245 , 0.3847745358943939 , - 0.5769480466842651 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.8004998564720154 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ - 0.40993982553482056 ] , [ - 1.0208697319030762 ] , [ 0.18116380274295807 ] , [ - 0.8320646286010742 ] ] &gt; } } By default, the metric will have a name which matches the string form of the given metric. You can give metrics semantic meaning by providing an explicit name: model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . metric ( :mean_absolute_error , &quot;model error&quot; ) |&gt; Axon.Loop . run ( train_data , %{ } , iterations : 1000 ) Epoch : 0 , Batch : 1000 , loss : 0.0559179 model error : 0.1430965 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ - 0.2884136438369751 , - 0.016403740271925926 , 0.30548375844955444 , 0.2799474000930786 , - 0.017874717712402344 , 0.3168976306915283 , - 0.10385002940893173 , - 0.18653006851673126 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ - 0.44000443816185 , 0.6495574712753296 , - 0.5427255034446716 , - 0.795007050037384 , - 0.0035864184610545635 , - 0.5102121233940125 , 0.10152970999479294 , - 0.3913733959197998 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ - 0.24588409066200256 , - 0.05674195662140846 , - 0.08545850962400436 , 0.27886852622032166 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ 0.6334101557731628 , - 0.44550418853759766 , 0.34385600686073303 , 0.24886265397071838 ] , [ - 0.5474148988723755 , 0.09881290793418884 , 0.14616712927818298 , 0.8087677359580994 ] , [ - 0.15381869673728943 , 0.5322079658508301 , - 0.6275551915168762 , - 0.4207017421722412 ] , [ 0.4673740863800049 , 0.5706797242164612 , 0.44344833493232727 , - 0.5382705926895142 ] , [ 0.6662552356719971 , - 0.3875215947628021 , - 0.5359503626823425 , - 0.6198058724403381 ] , [ - 0.2842515707015991 , 0.2379448264837265 , 0.581102728843689 , - 0.5942302346229553 ] , [ 0.039275627583265305 , 0.6341984272003174 , - 0.10589496046304703 , - 0.3522306978702545 ] , [ 0.4015151560306549 , - 0.15162920951843262 , - 0.3449919819831848 , 0.21970798075199127 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.26691529154777527 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ 0.7088357210159302 ] , [ - 0.9271859526634216 ] , [ - 0.1610293984413147 ] , [ 0.6011591553688049 ] ] &gt; } } Axon's default aggregation behavior is to aggregate metrics with a running average; however, you can customize this behavior by specifying an explicit accumulation function. Built-in accumulation functions are :running_average and :running_sum : model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . metric ( :mean_absolute_error , &quot;total error&quot; , :running_sum ) |&gt; Axon.Loop . run ( train_data , %{ } , iterations : 1000 ) Epoch : 0 , Batch : 1000 , loss : 0.0645265 total error : 158.5873566 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.013307658955454826 , 0.08766761422157288 , - 0.0048030223697423935 , - 0.07024712860584259 , 0.261692613363266 , 0.0028863451443612576 , - 0.12552864849567413 , 0.10552618652582169 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ - 0.1647171825170517 , - 0.4144238233566284 , - 0.09969457238912582 , - 0.6063833832740784 , 0.7182243466377258 , - 0.3485015034675598 , - 0.29005324840545654 , - 0.5282242298126221 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.021465059369802475 , - 0.16003911197185516 , 0.6696521043777466 , - 0.15482725203037262 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ 0.3359515964984894 , - 0.21561087667942047 , - 0.48400720953941345 , - 0.3186679184436798 ] , [ - 0.08509980887174606 , - 0.031951334327459335 , - 0.6084564924240112 , - 0.39506790041923523 ] , [ 0.003889488521963358 , - 0.12886928021907806 , 0.5679722428321838 , 0.22699925303459167 ] , [ - 0.315458744764328 , 0.5626247525215149 , - 0.4241454303264618 , - 0.11212264746427536 ] , [ 0.6759291291236877 , - 0.6508319973945618 , 0.3511318564414978 , 0.17946019768714905 ] , [ - 0.7148906588554382 , 0.45404312014579773 , 0.4150676727294922 , 0.33603984117507935 ] , [ 0.398037314414978 , 0.5080180764198303 , 0.6770725250244141 , - 0.5274750590324402 ] , [ 0.5072763562202454 , - 0.7351003289222717 , - 0.583225429058075 , - 0.2974703013896942 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ - 0.8310347199440002 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ 0.28011587262153625 ] , [ 0.542819082736969 ] , [ 1.2814348936080933 ] , [ - 0.5193246603012085 ] ] &gt; } }","ref":"instrumenting_loops_with_metrics.html#adding-metrics-to-training-loops","title":"Instrumenting loops with metrics - Adding metrics to training loops","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } ] ) :ok","ref":"your_first_evaluation_loop.html","title":"Your first evaluation loop","type":"extras"},{"doc":"Once you have a trained model, it's necessary to test the trained model on some test data. Axon's loop abstraction is general enough to work for both training and evaluating models. Just as Axon implements a canned Axon.Loop.trainer/3 factory, it also implements a canned Axon.Loop.evaluator/1 factory. Axon.Loop.evaluator/1 creates an evaluation loop which you can instrument with metrics to measure the performance of a trained model on test data. First, you need a trained model: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) train_loop = Axon.Loop . trainer ( model , :mean_squared_error , :sgd ) data = Stream . repeatedly ( fn -&gt; xs = Nx . random_normal ( { 8 , 1 } ) ys = Nx . sin ( xs ) { xs , ys } end ) trained_model_state = Axon.Loop . run ( train_loop , data , %{ } , iterations : 1000 ) Epoch : 0 , Batch : 1000 , loss : 0.0348526 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.12334823608398438 , 0.23830991983413696 , 0.07463178038597107 , - 0.18479900062084198 , - 0.2544017434120178 , - 0.1100262850522995 , 0.04137010499835014 , 0.22781872749328613 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ - 0.7397015690803528 , 0.8709579110145569 , - 0.33129510283470154 , - 0.4521639347076416 , - 0.5752679109573364 , 0.5516160726547241 , - 0.1265108585357666 , - 0.5665484666824341 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 7.311657827813178e-5 , - 0.027584673836827278 , 0.20344746112823486 , 0.1330498605966568 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.19199007749557495 , 0.15660767257213593 , 0.5446576476097107 , 0.07457015663385391 ] , [ 0.034533075988292694 , - 0.10262273252010345 , 0.05103863775730133 , 0.5708968639373779 ] , [ - 0.4212855398654938 , - 0.47742989659309387 , 0.18940746784210205 , - 0.40659299492836 ] , [ 0.2127801775932312 , - 0.07477620989084244 , - 0.11274989694356918 , 0.4552466869354248 ] , [ - 0.13839538395404816 , 0.09832656383514404 , - 0.16157560050487518 , 0.7074514627456665 ] , [ - 0.6366024017333984 , 0.3754875361919403 , - 0.6808919906616211 , - 0.209626242518425 ] , [ 0.595952033996582 , 0.6973875164985657 , 0.4453340172767639 , 0.6247327327728271 ] , [ - 0.6312451958656311 , 0.33275362849235535 , 0.5079866051673889 , - 0.2508215010166168 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.17476916313171387 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ 0.8893225193023682 ] , [ - 0.4548797905445099 ] , [ - 0.8288624286651611 ] , [ 0.8321414589881897 ] ] &gt; } } Running loops with Axon.Loop.trainer/3 returns a trained model state which you can use to evaluate your model. To construct an evaluation loop, you just call Axon.Loop.evaluator/1 with your pre-trained model: test_loop = Axon.Loop . evaluator ( model ) # Axon.Loop &lt; handlers : %{ completed : [ ] , epoch_completed : [ ] , epoch_halted : [ ] , epoch_started : [ ] , halted : [ ] , iteration_completed : [ { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 5.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , iteration_started : [ ] , started : [ ] } , metrics : %{ } , ... &gt; Next, you'll need to instrument your test loop with the metrics you'd like to aggregate: test_loop = test_loop |&gt; Axon.Loop . metric ( :mean_absolute_error ) # Axon.Loop &lt; handlers : %{ completed : [ ] , epoch_completed : [ ] , epoch_halted : [ ] , epoch_started : [ ] , halted : [ ] , iteration_completed : [ { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 5.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , iteration_started : [ ] , started : [ ] } , metrics : %{ &quot;mean_absolute_error&quot; =&gt; { # Function &lt; 12.6031754 / 3 in Axon.Metrics . running_average / 1 &gt; , :mean_absolute_error } } , ... &gt; Finally, you can run your loop on test data. Because you want to test your trained model, you need to provide your model's initial state to the test loop: Axon.Loop . run ( test_loop , data , trained_model_state , iterations : 1000 ) Batch : 1000 , mean_absolute_error : 0.0955574 %{ 0 =&gt; %{ &quot;mean_absolute_error&quot; =&gt; # Nx.Tensor &lt; f32 0.09555738419294357 &gt; } }","ref":"your_first_evaluation_loop.html#creating-an-axon-evaluation-loop","title":"Your first evaluation loop - Creating an Axon evaluation loop","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } ] ) :ok","ref":"using_loop_event_handlers.html","title":"Using loop event handlers","type":"extras"},{"doc":"Often times you want more fine-grained control over things that happen during loop execution. For example, you might want to save loop state to a file every 500 iterations, or log some output to :stdout at the end of every epoch. Axon loops allow more fine-grained control via events and event handlers. Axon fires a number of events during loop execution which allow you to instrument various points in the loop execution cycle. You can attach event handlers to any of these events: events = [ :started , # After loop state initialization :epoch_started , # On epoch start :iteration_started , # On iteration start :iteration_completed , # On iteration complete :epoch_completed , # On epoch complete :epoch_halted , # On epoch halt, if early halted :halted , # On loop halt, if early halted :completed # On loop completion ] Axon packages a number of common loop event handlers for you out of the box. These handlers should cover most of the common event handlers you would need to write in practice. Axon also allows for custom event handlers. See Writing custom event handlers for more information. An event handler will take the current loop state at the time of the fired event, and alter or use it in someway before returning control back to the main loop execution. You can attach any of Axon's pre-packaged event handlers to a loop by using the function directly. For example, if you want to checkpoint loop state at the end of every epoch, you can use Axon.Loop.checkpoint/2 : model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) loop = model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . checkpoint ( event : :epoch_completed ) # Axon.Loop &lt; handlers : %{ completed : [ ] , epoch_completed : [ { # Function &lt; 14.20267452 / 1 in Axon.Loop . checkpoint / 2 &gt; , # Function &lt; 5.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } , { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 5.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , epoch_halted : [ ] , epoch_started : [ ] , halted : [ ] , iteration_completed : [ { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 3.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , iteration_started : [ ] , started : [ ] } , metrics : %{ &quot;loss&quot; =&gt; { # Function &lt; 12.6031754 / 3 in Axon.Metrics . running_average / 1 &gt; , # Function &lt; 6.20267452 / 2 in Axon.Loop . build_loss_fn / 1 &gt; } } , ... &gt; Now when you execute your loop, it will save a checkpoint at the end of every epoch: train_data = Stream . repeatedly ( fn -&gt; xs = Nx . random_normal ( { 8 , 1 } ) ys = Nx . sin ( xs ) { xs , ys } end ) Axon.Loop . run ( loop , train_data , %{ } , epochs : 5 , iterations : 100 ) Epoch : 0 , Batch : 100 , loss : 0.2462310 Epoch : 1 , Batch : 100 , loss : 0.1804814 Epoch : 2 , Batch : 100 , loss : 0.1452925 Epoch : 3 , Batch : 100 , loss : 0.1177117 Epoch : 4 , Batch : 100 , loss : 0.1008184 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.36853691935539246 , 0.24528849124908447 , 0.13193830847740173 , 0.03188902884721756 , - 0.06358373910188675 , 0.044517479836940765 , - 0.1203451156616211 , - 6.352089694701135e-4 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ 0.49448737502098083 , 0.5250089764595032 , 0.7132464051246643 , 0.47473379969596863 , - 0.043285828083753586 , - 0.14137212932109833 , - 0.07576408237218857 , - 0.48898136615753174 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.30324652791023254 , 0.0385407879948616 , - 0.16782516241073608 , 0.1984063982963562 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ 0.2536502778530121 , 0.375381737947464 , 0.7119463086128235 , - 0.14521682262420654 ] , [ 0.20504063367843628 , - 0.11605211347341537 , 0.49423739314079285 , - 0.03246872499585152 ] , [ - 0.13834621012210846 , - 0.2579476833343506 , 0.34836748242378235 , - 0.4670639634132385 ] , [ - 0.11925031989812851 , - 0.6655324697494507 , 0.5057039856910706 , 0.496115118265152 ] , [ 0.15856991708278656 , - 0.2239169478416443 , 0.5550385117530823 , - 0.3774339258670807 ] , [ - 0.326529860496521 , - 0.10192928463220596 , 0.2961374819278717 , 0.580808699131012 ] , [ 0.46179524064064026 , - 0.4794206917285919 , 0.47078272700309753 , - 0.5654175877571106 ] , [ - 0.501025915145874 , - 0.38049301505088806 , 0.3792027235031128 , 0.685397207736969 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ - 0.4034360647201538 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ 0.8062413334846497 ] , [ 0.6867087483406067 ] , [ 0.5137255787849426 ] , [ - 0.5783006548881531 ] ] &gt; } } You can also use event handlers for things as simple as implementing custom logging with the pre-packaged Axon.Loop.log/4 event handler: model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . log ( :epoch_completed , fn _state -&gt; &quot;epoch is over \\n &quot; end , :stdio ) |&gt; Axon.Loop . run ( train_data , %{ } , epochs : 5 , iterations : 100 ) Epoch : 0 , Batch : 100 , loss : 0.2134880 epoch is over Epoch : 1 , Batch : 100 , loss : 0.1604774 epoch is over Epoch : 2 , Batch : 100 , loss : 0.1294429 epoch is over Epoch : 3 , Batch : 100 , loss : 0.1087099 epoch is over Epoch : 4 , Batch : 100 , loss : 0.0940388 epoch is over %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.1741544008255005 , - 0.013307991437613964 , 0.0873112753033638 , - 0.04722493514418602 , - 0.12966567277908325 , 0.04596322402358055 , 0.3969370722770691 , - 0.04508184269070625 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ 0.31960299611091614 , - 0.5328841805458069 , - 0.24278149008750916 , - 0.47772416472435 , 0.21538947522640228 , - 0.2799384295940399 , 0.5947694778442383 , 0.0497460775077343 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.25857725739479065 , - 0.07283111661672592 , - 0.10656370222568512 , - 0.08234459906816483 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ 0.3983175754547119 , - 0.5524351596832275 , 0.36650899052619934 , - 0.23933114111423492 ] , [ 0.06517457216978073 , 0.2564122974872589 , 0.6227137446403503 , - 0.5661884546279907 ] , [ - 0.7012182474136353 , 0.054501600563526154 , - 0.6726318597793579 , 0.4774037301540375 ] , [ - 0.11393500864505768 , 0.1726256012916565 , - 0.6723376512527466 , 0.6044175028800964 ] , [ - 0.30502673983573914 , 0.7011693120002747 , 0.40034061670303345 , - 0.5748327374458313 ] , [ - 0.07724377512931824 , - 0.251364529132843 , - 0.6626797914505005 , - 0.20940908789634705 ] , [ 0.7290927767753601 , 0.08563250303268433 , - 0.047927819192409515 , - 0.04336162284016609 ] , [ - 0.34993213415145874 , 0.281339168548584 , - 0.49343380331993103 , - 0.2481663078069687 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ - 0.6856028437614441 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ 1.1966136693954468 ] , [ - 0.00546963419765234 ] , [ - 0.9349364042282104 ] , [ 0.9214714765548706 ] ] &gt; } } For even more fine-grained control over when event handlers fire, you can add filters. For example, if you only want to checkpoint loop state every 2 epochs, you can use a filter: model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . checkpoint ( event : :epoch_completed , filter : [ every : 2 ] ) |&gt; Axon.Loop . run ( train_data , %{ } , epochs : 5 , iterations : 100 ) Epoch : 0 , Batch : 100 , loss : 0.1791917 Epoch : 1 , Batch : 100 , loss : 0.1373887 Epoch : 2 , Batch : 100 , loss : 0.1156979 Epoch : 3 , Batch : 100 , loss : 0.0965481 Epoch : 4 , Batch : 100 , loss : 0.0865761 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.00938357226550579 , 0.16315333545207977 , 0.2767408788204193 , - 0.22733710706233978 , 0.2830233573913574 , - 0.10280115902423859 , - 0.07500249892473221 , 0.2947545647621155 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ 0.522411048412323 , 0.15686289966106415 , 0.30727216601371765 , 0.3295647203922272 , 0.38795727491378784 , 0.17159366607666016 , 0.7608513236045837 , 0.4526905119419098 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ - 0.024011338129639626 , 0.0 , - 0.00135718728415668 , - 0.0015321056125685573 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ 0.606391966342926 , - 0.08385708928108215 , 0.06838012486696243 , - 0.08704598248004913 ] , [ 0.5944894552230835 , - 0.17639528214931488 , 0.26653605699539185 , 0.35148826241493225 ] , [ - 0.06138936057686806 , - 0.024123376235365868 , 0.29706713557243347 , 0.5498997569084167 ] , [ 0.26888611912727356 , 0.024979088455438614 , - 0.653775155544281 , - 0.4111217260360718 ] , [ - 0.5042538046836853 , - 0.6867390871047974 , 0.13647332787513733 , 0.7193269729614258 ] , [ - 0.052732646465301514 , 0.099549300968647 , - 0.6970457434654236 , 0.3078557252883911 ] , [ - 0.261769562959671 , 0.17121906578540802 , - 0.08267408609390259 , - 0.2213396430015564 ] , [ - 0.09766292572021484 , - 0.5843542218208313 , 0.369784414768219 , 0.48434120416641235 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ - 0.6914201378822327 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ 0.96906977891922 ] , [ - 0.5032458901405334 ] , [ 0.9275273680686951 ] , [ 0.8574270606040955 ] ] &gt; } } Axon event handlers support both keyword and function filters. Keyword filters include keywords such as :every , :once , and :always . Function filters are arity-1 functions which accept the current loop state and return a boolean.","ref":"using_loop_event_handlers.html#adding-event-handlers-to-training-loops","title":"Using loop event handlers - Adding event handlers to training loops","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } ] ) :ok","ref":"custom_models_loss_optimizers.html","title":"Custom models, loss functions, and optimizers","type":"extras"},{"doc":"In the Your first training loop , you learned how to declare a supervised training loop using Axon.Loop.trainer/3 with a model, loss function, and optimizer. Your overall model and loop declaration looked something like this: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) loop = Axon.Loop . trainer ( model , :mean_squared_error , :sgd ) This example uses an %Axon{} struct to represent your model to train, and atoms to represent your loss function and optimizer. Some of your problems will require a bit more flexibility than this example affords. Fortunately, Axon.Loop.trainer/3 is designed for flexibility. For example, if your model cannot be cleanly represented as an %Axon{} model, you can instead opt instead to define custom initialization and forward functions to pass to Axon.Loop.trainer/3 . Actually, Axon.Loop.trainer/3 is doing this for you under the hood - the ability to pass an %Axon{} struct directly is just a convenience: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) lowered_model = { init_fn , predict_fn } = Axon . build ( model ) loop = Axon.Loop . trainer ( lowered_model , :mean_squared_error , :sgd ) # Axon.Loop &lt; handlers : %{ completed : [ ] , epoch_completed : [ { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 5.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , epoch_halted : [ ] , epoch_started : [ ] , halted : [ ] , iteration_completed : [ { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 3.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , iteration_started : [ ] , started : [ ] } , metrics : %{ &quot;loss&quot; =&gt; { # Function &lt; 12.6031754 / 3 in Axon.Metrics . running_average / 1 &gt; , # Function &lt; 6.20267452 / 2 in Axon.Loop . build_loss_fn / 1 &gt; } } , ... &gt; Notice that Axon.Loop.trainer/3 handles the &quot;lowered&quot; form of an Axon model without issue. When you pass an %Axon{} struct, the trainer factory converts it to a lowered representation for you. With this construct, you can build custom models entirely with Nx defn , or readily mix your Axon models into custom workflows without worrying about compatibility with the Axon.Loop API: defmodule CustomModel do import Nx.Defn defn custom_predict_fn ( model_predict_fn , params , input ) do %{ prediction : preds } = out = model_predict_fn . ( params , input ) %{ out | prediction : Nx . cos ( preds ) } end end { :module , CustomModel , &lt;&lt; 70 , 79 , 82 , 49 , 0 , 0 , 9 , ... &gt;&gt; , { :custom_predict_fn , 3 } } train_data = Stream . repeatedly ( fn -&gt; xs = Nx . random_normal ( { 8 , 1 } ) ys = Nx . sin ( xs ) { xs , ys } end ) { init_fn , predict_fn } = Axon . build ( model , mode : :train ) custom_predict_fn = &amp; CustomModel . custom_predict_fn ( predict_fn , &amp;1 , &amp;2 ) loop = Axon.Loop . trainer ( { init_fn , custom_predict_fn } , :mean_squared_error , :sgd ) Axon.Loop . run ( loop , train_data , %{ } , iterations : 500 ) Epoch : 0 , Batch : 500 , loss : 0.3053460 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ - 0.06573846191167831 , 0.37533989548683167 , - 0.014221129938960075 , - 0.0056641618721187115 , - 0.013241665437817574 , - 0.04930500313639641 , 0.03238297998905182 , 0.019304191693663597 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ - 0.3132522702217102 , - 0.9284062385559082 , 0.5041953921318054 , 0.09051526337862015 , 0.003381401300430298 , - 0.22686156630516052 , 0.506594181060791 , 0.46744370460510254 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.008441010490059853 , 0.0 , 0.5370790958404541 , 0.03584281727671623 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.3442431688308716 , - 0.33131587505340576 , - 0.03751888871192932 , - 0.5497395396232605 ] , [ - 0.4568001925945282 , - 0.5024663805961609 , 0.8712142109870911 , - 0.13484779000282288 ] , [ 0.7310590744018555 , - 0.34318023920059204 , 0.3977772295475006 , - 0.6045383214950562 ] , [ - 0.5255699157714844 , - 0.2829623818397522 , - 0.45367464423179626 , - 0.157784566283226 ] , [ - 0.47948920726776123 , 0.2930692136287689 , - 0.3784458339214325 , - 0.69244384765625 ] , [ 0.7052943706512451 , 0.015830136835575104 , - 0.02979498915374279 , 0.6160839796066284 ] , [ 0.3201732933521271 , - 0.1367085874080658 , - 0.17100055515766144 , 0.7335636019706726 ] , [ - 0.2825513482093811 , - 0.424674928188324 , - 0.3110836148262024 , 0.46001508831977844 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.6889857649803162 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ - 0.7191283106803894 ] , [ - 0.4222411513328552 ] , [ 1.122635006904602 ] , [ - 0.7385509014129639 ] ] &gt; } }","ref":"custom_models_loss_optimizers.html#using-custom-models-in-training-loops","title":"Custom models, loss functions, and optimizers - Using custom models in training loops","type":"extras"},{"doc":"Just as Axon.Loop.trainer/3 allows more flexibility with models, it also supports more flexible loss functions. In most cases, you can get away with using one of Axon's built-in loss functions by specifying an atom. Atoms map directly to a loss-function defined in Axon.Losses . Under the hood, Axon.Loop.trainer/3 is doing something like: loss_fn = &amp; apply ( Axon.Losses , loss_atom , [ &amp;1 , &amp;2 ] ) Rather than pass an atom, you can pass your own custom arity-2 function to Axon.Loop.trainer/3 . This arises most often in cases where you want to control some parameters of the loss function, such as the batch-level reduction: loss_fn = &amp; Axon.Losses . mean_squared_error ( &amp;1 , &amp;2 , reduction : :sum ) loop = Axon.Loop . trainer ( model , loss_fn , :sgd ) # Axon.Loop &lt; handlers : %{ completed : [ ] , epoch_completed : [ { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 5.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , epoch_halted : [ ] , epoch_started : [ ] , halted : [ ] , iteration_completed : [ { # Function &lt; 23.20267452 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 3.20267452 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , iteration_started : [ ] , started : [ ] } , metrics : %{ &quot;loss&quot; =&gt; { # Function &lt; 12.6031754 / 3 in Axon.Metrics . running_average / 1 &gt; , # Function &lt; 41.3316493 / 2 in :erl_eval . expr / 6 &gt; } } , ... &gt; You can also define your own custom loss functions, so long as they match the following spec: loss ( y_true :: tensor [ batch , ... ] | container ( tensor ) , y_preds :: tensor [ batch , ... ] | container ( tensor ) ) :: scalar This is useful for constructing loss functions when dealing with multi-output scenarios. For example, it's very easy to construct a custom loss function which is a weighted average of several loss functions on multiple inputs: train_data = Stream . repeatedly ( fn -&gt; xs = Nx . random_normal ( { 8 , 1 } ) y1 = Nx . sin ( xs ) y2 = Nx . cos ( xs ) { xs , { y1 , y2 } } end ) shared = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) y1 = Axon . dense ( shared , 1 ) y2 = Axon . dense ( shared , 1 ) model = Axon . container ( { y1 , y2 } ) custom_loss_fn = fn { y_true1 , y_true2 } , { y_pred1 , y_pred2 } -&gt; loss1 = Axon.Losses . mean_squared_error ( y_true1 , y_pred1 , reduction : :mean ) loss2 = Axon.Losses . mean_squared_error ( y_true2 , y_pred2 , reduction : :mean ) loss1 |&gt; Nx . multiply ( 0.4 ) |&gt; Nx . add ( Nx . multiply ( loss2 , 0.6 ) ) end model |&gt; Axon.Loop . trainer ( custom_loss_fn , :sgd ) |&gt; Axon.Loop . run ( train_data , %{ } , iterations : 1000 ) Epoch : 0 , Batch : 1000 , loss : 0.1098235 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.07738334685564041 , 0.04548311233520508 , 0.049238916486501694 , 0.38714033365249634 , - 0.030310271307826042 , - 0.07575170695781708 , 0.02918776497244835 , 0.15639683604240417 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ - 0.5250527858734131 , 0.9252119660377502 , - 0.7720071077346802 , 0.3685735762119293 , - 0.15688209235668182 , - 0.41163918375968933 , 0.7827479839324951 , 0.07295594364404678 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.012770675122737885 , 0.6008449792861938 , 0.29370757937431335 , - 0.05354489013552666 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.08783119916915894 , 0.4296257495880127 , 0.07153885811567307 , - 0.6921477317810059 ] , [ 0.15848888456821442 , - 0.4663836658000946 , 0.7126847505569458 , 0.0693722814321518 ] , [ - 0.24852830171585083 , - 0.7588720321655273 , - 0.5033655166625977 , 0.6524038314819336 ] , [ 0.2933746874332428 , 0.6656989455223083 , - 0.046741705387830734 , 0.44998466968536377 ] , [ 0.17215801775455475 , - 0.3072860836982727 , 0.2046997845172882 , - 0.7001357078552246 ] , [ 0.6354788541793823 , - 0.12706635892391205 , - 0.18666459619998932 , - 0.26693975925445557 ] , [ - 0.3737913966178894 , - 0.07344938814640045 , 0.22658668458461761 , - 0.37110695242881775 ] , [ 0.01989569514989853 , 0.39410898089408875 , - 0.30496707558631897 , - 0.4945743680000305 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ - 0.5888826251029968 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ 1.0239059925079346 ] , [ 0.25252565741539 ] , [ 0.8877795338630676 ] , [ - 0.13882321119308472 ] ] &gt; } , &quot;dense_3&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.2557465434074402 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ - 0.6269392371177673 ] , [ 1.1281259059906006 ] , [ - 0.503214418888092 ] , [ - 0.5435869693756104 ] ] &gt; } }","ref":"custom_models_loss_optimizers.html#using-custom-loss-functions-in-training-loops","title":"Custom models, loss functions, and optimizers - Using custom loss functions in training loops","type":"extras"},{"doc":"As you might expect, it's also possible to customize the optimizer passed to Axon.Loop.trainer/3 . If you read the Axon.Updates documentation, you'll learn that optimizers are actually represented as the tuple {init_fn, update_fn} where init_fn initializes optimizer state from model state and update_fn scales gradients from optimizer state, gradients, and model state. You likely won't have to implement a custom optimizer; however, you should know how to construct optimizers with different hyperparameters and how to apply different modifiers to different optimizers to customize the optimization process. When you specify an optimizer as an atom in Axon.Loop.trainer/3 , it maps directly to an optimizer declared in Axon.Optimizers . You can instead opt to declare your optimizer directly. This is most useful for controlling things like the learning rate and various optimizer hyperparameters: train_data = Stream . repeatedly ( fn -&gt; xs = Nx . random_normal ( { 8 , 1 } ) ys = Nx . sin ( xs ) { xs , ys } end ) model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) optimizer = { _init_optimizer_fn , _update_fn } = Axon.Optimizers . sgd ( 1.0e-3 ) model |&gt; Axon.Loop . trainer ( :mean_squared_error , optimizer ) |&gt; Axon.Loop . run ( train_data , %{ } , iterations : 1000 ) Epoch : 0 , Batch : 1000 , loss : 0.0992607 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.06136200204491615 , - 0.08278193324804306 , - 0.07280997931957245 , 0.08740464597940445 , 0.08663233369588852 , - 0.06915996968746185 , 0.03753892332315445 , 0.06512840837240219 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ 0.622833251953125 , 0.24778570234775543 , 0.4959430694580078 , - 0.604946494102478 , - 0.31578049063682556 , 0.09977878630161285 , 0.776294469833374 , 0.5804685950279236 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ - 0.012786266393959522 , 0.01057625561952591 , 0.10597240924835205 , 0.13692162930965424 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.46233609318733215 , - 0.7435348033905029 , - 0.10738609731197357 , 0.09911829978227615 ] , [ 0.5295257568359375 , 0.48769527673721313 , - 0.23950818181037903 , - 0.26084062457084656 ] , [ - 0.5117107033729553 , 0.2039143443107605 , - 0.12630638480186462 , - 0.41089773178100586 ] , [ - 0.6043668985366821 , 0.3961969316005707 , 0.5120400190353394 , - 0.6773409247398376 ] , [ 0.22123000025749207 , 0.7197521924972534 , 0.2679356038570404 , - 0.12402179092168808 ] , [ 0.4830038249492645 , 0.3629038631916046 , 0.49994897842407227 , - 0.25865232944488525 ] , [ 0.29824453592300415 , 0.29333528876304626 , - 0.05371938645839691 , 0.5230391621589661 ] , [ 0.5483304262161255 , 0.08283360302448273 , - 0.6959219574928284 , 0.6471460461616516 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.07759959995746613 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ - 0.036170706152915955 ] , [ - 0.5362256765365601 ] , [ - 0.6853286027908325 ] , [ 0.6693617701530457 ] ] &gt; } }","ref":"custom_models_loss_optimizers.html#using-custom-optimizers-in-training-loops","title":"Custom models, loss functions, and optimizers - Using custom optimizers in training loops","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } ] ) :ok","ref":"writing_custom_metrics.html","title":"Writing custom metrics","type":"extras"},{"doc":"When passing an atom to Axon.Loop.metric/5 , Axon dispatches the function to a built-in function in Axon.Metrics . If you find you'd like to use a metric that does not exist in Axon.Metrics , you can define a custom function: defmodule CustomMetric do import Nx.Defn defn my_weird_metric ( y_true , y_pred ) do Nx . atan2 ( y_true , y_pred ) |&gt; Nx . sum ( ) end end { :module , CustomMetric , &lt;&lt; 70 , 79 , 82 , 49 , 0 , 0 , 8 , ... &gt;&gt; , { :my_weird_metric , 2 } } Then you can pass that directly to Axon.Loop.metric/5 . You must provide a name for your custom metric: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) loop = model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . metric ( &amp; CustomMetric . my_weird_metric / 2 , &quot;my weird metric&quot; ) # Axon.Loop &lt; handlers : %{ completed : [ ] , epoch_completed : [ { # Function &lt; 23.77614421 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 5.77614421 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , epoch_halted : [ ] , epoch_started : [ ] , halted : [ ] , iteration_completed : [ { # Function &lt; 23.77614421 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 3.77614421 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , iteration_started : [ ] , started : [ ] } , metrics : %{ &quot;loss&quot; =&gt; { # Function &lt; 12.46375131 / 3 in Axon.Metrics . running_average / 1 &gt; , # Function &lt; 6.77614421 / 2 in Axon.Loop . build_loss_fn / 1 &gt; } , &quot;my weird metric&quot; =&gt; { # Function &lt; 12.46375131 / 3 in Axon.Metrics . running_average / 1 &gt; , &amp; CustomMetric . my_weird_metric / 2 } } , ... &gt; Then when running, Axon will invoke your custom metric function and accumulate it with the given aggregator: train_data = Stream . repeatedly ( fn -&gt; xs = Nx . random_normal ( { 8 , 1 } ) ys = Nx . sin ( xs ) { xs , ys } end ) Axon.Loop . run ( loop , train_data , %{ } , iterations : 1000 ) Epoch : 0 , Batch : 1000 , loss : 0.0468431 my weird metric : - 5.7462921 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.011475208215415478 , 0.23035769164562225 , 0.01538881566375494 , 0.08167446404695511 , 0.23642019927501678 , 0.10298296064138412 , 0.20279639959335327 , - 0.18916435539722443 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ 0.7426201105117798 , 0.734136700630188 , - 0.5648708343505859 , - 0.5230435132980347 , 0.3056533932685852 , 0.3383721709251404 , - 0.3518844544887543 , - 0.19460521638393402 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.2185358852148056 , 0.23043134808540344 , 0.0 , 0.2650437355041504 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ 0.19164204597473145 , - 0.26440876722335815 , 0.060297321528196335 , 0.004777891095727682 ] , [ 0.019263261929154396 , - 0.6267783045768738 , - 0.33454063534736633 , 0.33268266916275024 ] , [ - 0.18489953875541687 , 0.4653063714504242 , - 0.6056118607521057 , - 0.046012550592422485 ] , [ 0.5975558161735535 , - 0.237883061170578 , - 0.6522921919822693 , 0.019332828000187874 ] , [ - 0.7424253225326538 , 0.593705952167511 , 0.2551117241382599 , 0.26270362734794617 ] , [ 0.018434584140777588 , 0.15290242433547974 , 0.08793036639690399 , 0.1839984804391861 ] , [ 0.6048195958137512 , - 0.20294713973999023 , - 0.694927990436554 , - 0.45577046275138855 ] , [ - 0.628790020942688 , 0.21741150319576263 , - 0.08936657756567001 , 0.6170362234115601 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ - 0.03722470998764038 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ - 0.7919473648071289 ] , [ - 0.4341854751110077 ] , [ - 0.39114490151405334 ] , [ 0.9605273008346558 ] ] &gt; } } While the metric defaults are designed with supervised training loops in mind, they can be used for much more flexible purposes. By default, metrics look for the fields :y_true and :y_pred in the given loop's step state. They then apply the given metric function on those inputs. You can also define metrics which work on other fields. For example you can track the running average of a given parameter with a metric just by defining a custom output transform: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) output_transform = fn %{ model_state : model_state } -&gt; [ model_state [ &quot;dense_0&quot; ] [ &quot;kernel&quot; ] ] end loop = model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . metric ( &amp; Nx . mean / 1 , &quot;dense_0_kernel_mean&quot; , :running_average , output_transform ) |&gt; Axon.Loop . metric ( &amp; Nx . variance / 1 , &quot;dense_0_kernel_var&quot; , :running_average , output_transform ) # Axon.Loop &lt; handlers : %{ completed : [ ] , epoch_completed : [ { # Function &lt; 23.77614421 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 5.77614421 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , epoch_halted : [ ] , epoch_started : [ ] , halted : [ ] , iteration_completed : [ { # Function &lt; 23.77614421 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 3.77614421 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , iteration_started : [ ] , started : [ ] } , metrics : %{ &quot;dense_0_kernel_mean&quot; =&gt; { # Function &lt; 12.46375131 / 3 in Axon.Metrics . running_average / 1 &gt; , &amp; Nx . mean / 1 } , &quot;dense_0_kernel_var&quot; =&gt; { # Function &lt; 12.46375131 / 3 in Axon.Metrics . running_average / 1 &gt; , &amp; Nx . variance / 1 } , &quot;loss&quot; =&gt; { # Function &lt; 12.46375131 / 3 in Axon.Metrics . running_average / 1 &gt; , # Function &lt; 6.77614421 / 2 in Axon.Loop . build_loss_fn / 1 &gt; } } , ... &gt; Axon will apply your custom output transform to the loop's step state and forward the result to your custom metric function: train_data = Stream . repeatedly ( fn -&gt; xs = Nx . random_normal ( { 8 , 1 } ) ys = Nx . sin ( xs ) { xs , ys } end ) Axon.Loop . run ( loop , train_data , %{ } , iterations : 1000 ) Epoch : 0 , Batch : 1000 , dense_0_kernel_mean : 0.0807205 dense_0_kernel_var : 0.1448047 loss : 0.0626600 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ - 0.14429236948490143 , 0.3176318109035492 , 0.0036036474630236626 , 0.01434470433741808 , 0.21225003898143768 , - 0.1406097412109375 , 0.32469284534454346 , - 0.18893203139305115 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ 0.2918722331523895 , - 0.44978663325309753 , - 0.28219935297966003 , - 0.10681337863206863 , 0.5192054510116577 , 0.312747985124588 , - 0.15127503871917725 , 0.5638187527656555 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.0 , - 0.003864143043756485 , 0.5194356441497803 , 0.028363214805722237 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.6123268008232117 , 0.22753892838954926 , 0.12077417969703674 , 0.4875330626964569 ] , [ - 0.5840837359428406 , 0.2259720116853714 , 0.4917944371700287 , 0.22638437151908875 ] , [ - 0.22699439525604248 , - 0.6744257807731628 , - 0.2907045781612396 , 0.35300591588020325 ] , [ - 0.16367988288402557 , - 0.5971682071685791 , - 0.39346548914909363 , 0.5823913812637329 ] , [ - 0.5512545704841614 , - 0.6812713742256165 , - 0.5777145624160767 , - 0.653957188129425 ] , [ - 0.23620283603668213 , - 0.47966212034225464 , - 0.273225873708725 , 0.3827615976333618 ] , [ - 0.5591338276863098 , - 0.1730434000492096 , 0.25726518034935 , 0.7179149389266968 ] , [ 0.3902169167995453 , 0.6351881623268127 , - 0.602277398109436 , 0.40137141942977905 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.824558675289154 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ 0.9618374109268188 ] , [ - 0.028266794979572296 ] , [ - 1.1059081554412842 ] , [ - 0.7398673892021179 ] ] &gt; } } You can also define custom accumulation functions. Axon has definitions for computing running averages and running sums; however, you might find you need something like an exponential moving average: defmodule CustomAccumulator do import Nx.Defn defn running_ema ( acc , obs , _i , opts \\\\ [ ] ) do opts = keyword! ( opts , alpha : 0.9 ) obs * opts [ :alpha ] + acc * ( 1 - opts [ :alpha ] ) end end { :module , CustomAccumulator , &lt;&lt; 70 , 79 , 82 , 49 , 0 , 0 , 11 , ... &gt;&gt; , { :running_ema , 4 } } Your accumulator must be an arity-3 function which accepts the current accumulated value, the current observation, and the current iteration and returns the aggregated metric. You can pass a function direct as an accumulator in your metric: model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) output_transform = fn %{ model_state : model_state } -&gt; [ model_state [ &quot;dense_0&quot; ] [ &quot;kernel&quot; ] ] end loop = model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . metric ( &amp; Nx . mean / 1 , &quot;dense_0_kernel_ema_mean&quot; , &amp; CustomAccumulator . running_ema / 3 , output_transform ) # Axon.Loop &lt; handlers : %{ completed : [ ] , epoch_completed : [ { # Function &lt; 23.77614421 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 5.77614421 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , epoch_halted : [ ] , epoch_started : [ ] , halted : [ ] , iteration_completed : [ { # Function &lt; 23.77614421 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 3.77614421 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , iteration_started : [ ] , started : [ ] } , metrics : %{ &quot;dense_0_kernel_ema_mean&quot; =&gt; { # Function &lt; 12.77614421 / 3 in Axon.Loop . build_metric_fn / 3 &gt; , &amp; Nx . mean / 1 } , &quot;loss&quot; =&gt; { # Function &lt; 12.46375131 / 3 in Axon.Metrics . running_average / 1 &gt; , # Function &lt; 6.77614421 / 2 in Axon.Loop . build_loss_fn / 1 &gt; } } , ... &gt; Then when you run the loop, Axon will use your custom accumulator: train_data = Stream . repeatedly ( fn -&gt; xs = Nx . random_normal ( { 8 , 1 } ) ys = Nx . sin ( xs ) { xs , ys } end ) Axon.Loop . run ( loop , train_data , %{ } , iterations : 1000 ) Epoch : 0 , Batch : 1000 , dense_0_kernel_ema_mean : 0.2137861 loss : 0.0709054 %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.08160790055990219 , - 0.21322371065616608 , - 0.1431925743818283 , 0.2848915755748749 , - 0.007875560782849789 , 0.3923396170139313 , - 0.04444991424679756 , 0.23083189129829407 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ - 0.6269387006759644 , 0.3289071023464203 , 0.19450749456882477 , 0.7400281429290771 , 0.23878233134746552 , 0.36140456795692444 , 0.10503113269805908 , 0.3685782253742218 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.2350393682718277 , 0.06712433695793152 , - 0.03675961494445801 , - 0.06366443634033203 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.35826751589775085 , - 0.10699580609798431 , - 0.3681609034538269 , 0.08517063409090042 ] , [ - 0.7694831490516663 , 0.13644370436668396 , - 0.2390032261610031 , 0.6069303154945374 ] , [ - 0.6424086689949036 , 0.13374455273151398 , - 0.35404452681541443 , 0.6343701481819153 ] , [ - 0.09528166800737381 , 0.7048070430755615 , 0.13699916005134583 , 0.6482889652252197 ] , [ - 0.08044164627790451 , 0.010588583536446095 , 0.11140558868646622 , 0.33911004662513733 ] , [ 0.7361723780632019 , 0.757600724697113 , - 0.0011848200811073184 , 0.2799053192138672 ] , [ 0.3472788631916046 , - 0.5225644111633301 , 0.04859891161322594 , - 0.4931156039237976 ] , [ 0.09371320903301239 , 0.5478940606117249 , 0.5831385254859924 , - 0.21019525825977325 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ - 0.835706889629364 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ 1.0109968185424805 ] , [ 0.574639618396759 ] , [ - 0.01302765030413866 ] , [ - 0.008134203962981701 ] ] &gt; } }","ref":"writing_custom_metrics.html#writing-custom-metrics","title":"Writing custom metrics - Writing custom metrics","type":"extras"},{"doc":"Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :nx , &quot;~&gt; 0.3.0&quot; , github : &quot;elixir-nx/nx&quot; , sparse : &quot;nx&quot; , override : true } ] ) :ok","ref":"writing_custom_event_handlers.html","title":"Writing custom event handlers","type":"extras"},{"doc":"If you require functionality not offered by any of Axon's built-in event handlers, then you'll need to write a custom event handler. Custom event handlers are functions which accept loop state, perform some action, and then defer execution back to the main loop. For example, you can write custom loop handlers which visualize model outputs, communicate with an external Kino process, or simply halt the loop based on some criteria. All event handlers must accept an %Axon.Loop.State{} struct and return a tuple of {control_term, state} where control_term is one of :continue , :halt_epoch , or :halt_loop and state is the updated loop state: defmodule CustomEventHandler do alias Axon.Loop.State def my_weird_handler ( % State { } = state ) do IO . puts ( &quot;My weird handler: fired&quot; ) { :continue , state } end end { :module , CustomEventHandler , &lt;&lt; 70 , 79 , 82 , 49 , 0 , 0 , 6 , ... &gt;&gt; , { :my_weird_handler , 1 } } To register event handlers, you use Axon.Loop.handle/4 : model = Axon . input ( &quot;data&quot; ) |&gt; Axon . dense ( 8 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 4 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 1 ) loop = model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . handle ( :epoch_completed , &amp; CustomEventHandler . my_weird_handler / 1 ) # Axon.Loop &lt; handlers : %{ completed : [ ] , epoch_completed : [ { &amp; CustomEventHandler . my_weird_handler / 1 , # Function &lt; 5.33119226 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } , { # Function &lt; 23.33119226 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 5.33119226 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , epoch_halted : [ ] , epoch_started : [ ] , halted : [ ] , iteration_completed : [ { # Function &lt; 23.33119226 / 1 in Axon.Loop . log / 5 &gt; , # Function &lt; 3.33119226 / 1 in Axon.Loop . build_filter_fn / 1 &gt; } ] , iteration_started : [ ] , started : [ ] } , metrics : %{ &quot;loss&quot; =&gt; { # Function &lt; 12.46375131 / 3 in Axon.Metrics . running_average / 1 &gt; , # Function &lt; 6.33119226 / 2 in Axon.Loop . build_loss_fn / 1 &gt; } } , ... &gt; Axon will trigger your custom handler to run on the attached event: train_data = Stream . repeatedly ( fn -&gt; xs = Nx . random_normal ( { 8 , 1 } ) ys = Nx . sin ( xs ) { xs , ys } end ) Axon.Loop . run ( loop , train_data , %{ } , epochs : 5 , iterations : 100 ) Epoch : 0 , Batch : 100 , loss : 0.1905403 My weird handler : fired Epoch : 1 , Batch : 100 , loss : 0.1478554 My weird handler : fired Epoch : 2 , Batch : 100 , loss : 0.1184390 My weird handler : fired Epoch : 3 , Batch : 100 , loss : 0.0983292 My weird handler : fired Epoch : 4 , Batch : 100 , loss : 0.0845697 My weird handler : fired %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 0.014659373089671135 , 0.08941870182752609 , - 0.09661660343408585 , 0.2650177478790283 , - 0.06400775164365768 , - 0.07953602075576782 , 0.22094617784023285 , - 0.014790073968470097 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ 0.3581556975841522 , 0.38828182220458984 , - 0.3311854302883148 , - 0.4059808552265167 , 0.6334917545318604 , 0.17008493840694427 , - 0.5630434155464172 , 0.3790667653083801 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.3047839403152466 , - 0.025677276775240898 , 0.18113580346107483 , 0.19019420444965363 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.25477269291877747 , 0.28833284974098206 , - 0.25498083233833313 , 0.40912926197052 ] , [ - 0.387851357460022 , 0.009837300516664982 , - 0.48930269479751587 , - 0.6119663715362549 ] , [ 0.49769237637519836 , - 0.45746952295303345 , - 0.3886529505252838 , - 0.49895355105400085 ] , [ 0.6451961994171143 , 0.16054697334766388 , 0.27802371978759766 , - 0.15226426720619202 ] , [ 0.17125651240348816 , - 0.048851024359464645 , 0.19429178535938263 , 0.24933232367038727 ] , [ 0.5465306043624878 , - 0.15836869180202484 , 0.39782997965812683 , - 0.3635501563549042 ] , [ - 0.36660289764404297 , - 0.011948992498219013 , 0.48680511116981506 , 0.5263928174972534 ] , [ - 0.6284276843070984 , - 0.5880372524261475 , 0.004470183979719877 , - 0.4550755023956299 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 0.7117368578910828 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ - 0.7743457555770874 ] , [ 0.3977936804294586 ] , [ - 1.0638943910598755 ] , [ - 0.6494196653366089 ] ] &gt; } } You can use event handlers to early-stop a loop or loop epoch by returning a :halt_* control term. Halt control terms can be one of :halt_epoch or :halt_loop . :halt_epoch halts the current epoch and continues to the next. :halt_loop halts the loop altogether. defmodule CustomEventHandler do alias Axon.Loop.State def always_halts ( % State { } = state ) do IO . puts ( &quot;stopping loop&quot; ) { :halt_loop , state } end end { :module , CustomEventHandler , &lt;&lt; 70 , 79 , 82 , 49 , 0 , 0 , 6 , ... &gt;&gt; , { :always_halts , 1 } } The loop will immediately stop executing and return the current state at the time it was halted: model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . handle ( :epoch_completed , &amp; CustomEventHandler . always_halts / 1 ) |&gt; Axon.Loop . run ( train_data , %{ } , epochs : 5 , iterations : 100 ) Epoch : 0 , Batch : 100 , loss : 0.1967763 stopping loop %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ - 0.05958094820380211 , 0.08930676430463791 , - 0.006259916350245476 , 0.05067025125026703 , 0.10981185734272003 , - 0.011248357594013214 , - 0.007601946126669645 , 0.036958880722522736 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ 0.050393108278512955 , - 0.5486620664596558 , 0.6901980042457581 , 0.42280837893486023 , 0.6446300745010376 , 0.25207778811454773 , - 0.13566234707832336 , 0.26625606417655945 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ - 0.06729397922754288 , 0.14259757101535797 , - 0.0020351663697510958 , 0.16679106652736664 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.5964004397392273 , - 0.5631846785545349 , 0.15613533556461334 , 0.1943722516298294 ] , [ 0.19513694941997528 , - 0.24765732884407043 , - 0.06751974672079086 , 0.6707308292388916 ] , [ - 0.6826592087745667 , - 0.006577506195753813 , - 0.6097249984741211 , - 0.5801466703414917 ] , [ - 0.30076032876968384 , 0.34819719195365906 , - 0.5906499028205872 , - 0.37741175293922424 ] , [ 0.16266342997550964 , 0.7666646838188171 , 0.6456886529922485 , - 0.4589986801147461 ] , [ - 0.2686948776245117 , - 0.06113003194332123 , 0.22663049399852753 , - 0.12092678993940353 ] , [ - 0.5785921216011047 , - 0.641874372959137 , - 0.24317769706249237 , - 0.2897084951400757 ] , [ 0.14917287230491638 , 0.24462535977363586 , - 0.64858478307724 , - 0.5138146877288818 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ - 0.11649220436811447 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ 0.7849427461624146 ] , [ 0.5966104865074158 ] , [ - 0.5520159602165222 ] , [ - 0.4974740147590637 ] ] &gt; } } Note that halting an epoch will fire a different event than completing an epoch. So if you implement a custom handler to halt the loop when an epoch completes, it will never fire if the epoch always halts prematurely: defmodule CustomEventHandler do alias Axon.Loop.State def always_halts_epoch ( % State { } = state ) do IO . puts ( &quot; \\n stopping epoch&quot; ) { :halt_epoch , state } end def always_halts_loop ( % State { } = state ) do IO . puts ( &quot;stopping loop \\n &quot; ) { :halt_loop , state } end end { :module , CustomEventHandler , &lt;&lt; 70 , 79 , 82 , 49 , 0 , 0 , 7 , ... &gt;&gt; , { :always_halts_loop , 1 } } If you run these handlers in conjunction, the loop will not terminate prematurely: model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . handle ( :iteration_completed , &amp; CustomEventHandler . always_halts_epoch / 1 ) |&gt; Axon.Loop . handle ( :epoch_completed , &amp; CustomEventHandler . always_halts_loop / 1 ) |&gt; Axon.Loop . run ( train_data , %{ } , epochs : 5 , iterations : 100 ) Epoch : 0 , Batch : 0 , loss : 0.0000000 stopping epoch Epoch : 0 , Batch : 0 , loss : 0.7256396 stopping epoch Epoch : 0 , Batch : 0 , loss : 0.4574284 stopping epoch Epoch : 0 , Batch : 0 , loss : 0.4981923 stopping epoch Epoch : 0 , Batch : 0 , loss : 0.4377063 stopping epoch %{ &quot;dense_0&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 9.248655405826867e-4 , - 0.0038722341414541006 , - 0.0015197680331766605 , - 0.001993122510612011 , - 0.0015419051051139832 , - 0.004070846363902092 , 0.001461982261389494 , 0.0043989671394228935 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ 8 ] [ [ - 0.6537156701087952 , 0.2857331335544586 , - 0.339731365442276 , 0.46841081976890564 , - 0.5864744782447815 , - 0.364472359418869 , - 0.5385616421699524 , - 0.694677472114563 ] ] &gt; } , &quot;dense_1&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 0.0 , - 0.017093738541007042 , 0.00152371556032449 , - 0.0019599769730120897 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 8 ] [ 4 ] [ [ - 0.21336764097213745 , - 0.6211493611335754 , 0.676548957824707 , 0.3768426477909088 ] , [ - 0.24921125173568726 , 0.217195525765419 , 0.23704318702220917 , 0.1597728431224823 ] , [ - 0.12178827077150345 , - 0.4966273307800293 , - 0.283501535654068 , 0.00888047181069851 ] , [ - 0.19504092633724213 , 0.18697738647460938 , 0.14705461263656616 , 0.39286476373672485 ] , [ - 0.5945789813995361 , - 0.5958647727966309 , - 0.3320448100566864 , - 0.02747068926692009 ] , [ - 0.2157520055770874 , - 0.2990635335445404 , - 0.16008871793746948 , 0.4921063184738159 ] , [ - 0.529068648815155 , - 0.383655846118927 , - 0.07292155921459198 , - 0.2834954559803009 ] , [ - 0.3056498169898987 , - 0.28507867455482483 , 0.554026186466217 , - 0.24665579199790955 ] ] &gt; } , &quot;dense_2&quot; =&gt; %{ &quot;bias&quot; =&gt; # Nx.Tensor &lt; f32 [ 1 ] [ - 0.010511377826333046 ] &gt; , &quot;kernel&quot; =&gt; # Nx.Tensor &lt; f32 [ 4 ] [ 1 ] [ [ 0.9865502119064331 ] , [ - 0.686279296875 ] , [ - 0.15436960756778717 ] , [ 0.18355509638786316 ] ] &gt; } } You may access and update any portion of the loop state. Keep in mind that event handlers are not JIT-compiled, so you should be certain to manually JIT-compile any long-running or expensive operations.","ref":"writing_custom_event_handlers.html#writing-custom-event-handlers","title":"Writing custom event handlers - Writing custom event handlers","type":"extras"},{"doc":"Mix . install ( [ { :nx , &quot;~&gt; 0.3&quot; } , { :axon , &quot;~&gt; 0.2&quot; } , { :exla , &quot;~&gt; 0.3&quot; } , { :axon_onnx , &quot;~&gt; 0.2&quot; } , { :stb_image , &quot;~&gt; 0.5&quot; } , { :kino , &quot;~&gt; 0.7.0&quot; } ] , # change to &quot;cuda111&quot; for Nvidia GPU system_env : %{ &quot;XLA_TARGET&quot; =&gt; xla_target } )","ref":"onnx_to_axon.html","title":"Converting ONNX models to Axon","type":"extras"},{"doc":"Axon is a new machine learning capability, specific to Elixir. We would like to take advantage of a large amount of models that have been written in other languages and machine learning frameworks. Let's take a look at how we could use a model developed in another language. Converting models developed by data scientists into a production capable implementation is a challenge for all languages and frameworks. ONNX is an interchange format that allows models written in one language or framework to be converted into another language and framework. The source model must use constructs mapped into ONNX. Also, the destination framework must support the model's ONNX constructs. From an Elixir focus, we are interested in ONNX models that axon_onnx can convert into Axon models. Why is ONNX important to Axon? Elixir can get access to thousands of public models and your organization may have private models written in other languages and frameworks. Axon will be hard pressed to quickly repeat the countless person-hours spent on developing models in other languages like Tensorflow and PyTorch. However, if the model can be converted into ONNX and then into Axon, we can directly run the model in Elixir. Setting up our environment Axon runs on top of Nx (Numerical Elixir) . Nx has backends for both Google's XLA (via EXLA) and PyTorch (via Torchx). In this guide, we will use EXLA. We'll also convert from an ONNX model into an Axon model using axon_onnx . You can find all dependencies in the installation cell at the top of the notebook. In there, you will also find the XLA_TARGET environment variable whick you can set to &quot;cuda111&quot; or &quot;rocm&quot; if you have any of those GPUs available. Let's also configure Nx to store tensors in EXLA by default: Nx . default_backend ( EXLA.Backend ) We'll also need local access to ONNX files. For this notebook, the models/onnx folder contains the ONNX model file. This notebook assumes the output file location will be in models axon. Copy your ONNX model files into the models/onnx folder. This opinionated module presents a simple API for loading in an ONNX file and saving the converted Axon model in the provided directory. This API will allow us to save multiple models pretty quickly. defmodule OnnxToAxon do @moduledoc &quot;&quot;&quot; Helper module from ONNX to Axon. &quot;&quot;&quot; @doc &quot;&quot;&quot; Loads an ONNX model into Axon and saves the model ## Examples iex&gt; OnnxToAxon.onnx_axon(path_to_onnx_file, path_to_axon_dir) &quot;&quot;&quot; def onnx_axon ( path_to_onnx_file , path_to_axon_dir ) do axon_name = axon_name_from_onnx_path ( path_to_onnx_file ) path_to_axon = Path . join ( path_to_axon_dir , axon_name ) { model , parameters } = AxonOnnx . import ( path_to_onnx_file ) model_bytes = Axon . serialize ( model , parameters ) File . write! ( path_to_axon , model_bytes ) end defp axon_name_from_onnx_path ( onnx_path ) do model_root = onnx_path |&gt; Path . basename ( ) |&gt; Path . rootname ( ) &quot; \#{ model_root } .axon&quot; end end","ref":"onnx_to_axon.html#converting-an-onnx-model-into-axon","title":"Converting ONNX models to Axon - Converting an ONNX model into Axon","type":"extras"},{"doc":"For this example, we'll use a couple ONNX models that have been saved in the Huggingface Hub. The ONNX models were trained in Fast.ai (PyTorch) using the following notebooks: https://github.com/meanderingstream/fastai_course22/blob/main/saving-a-basic-fastai-model-in-onnx.ipynb https://github.com/meanderingstream/fastai_course22/blob/main/saving-cat-dog-breed-fastai-model-in-onnx.ipynb To repeat this notebook, the onnx files for this notebook can be found on huggingface hub. Download the onnx models from: https://huggingface.co/ScottMueller/Cats_v_Dogs.ONNX https://huggingface.co/ScottMueller/Cat_Dog_Breeds.ONNX Download the files and place them in a directory of your choice. By default, we will assume you downloaded them to the same directory as the notebook: File . cd! ( __DIR__ ) Now let's convert an ONNX model into Axon path_to_onnx_file = &quot;models/onnx/cats_v_dogs.onnx&quot; path_to_axon_dir = &quot;models/axon&quot; OnnxToAxon . onnx_axon ( path_to_onnx_file , path_to_axon_dir ) path_to_onnx_file = &quot;models/onnx/cat_dog_breeds.onnx&quot; path_to_axon_dir = &quot;models/axon&quot; OnnxToAxon . onnx_axon ( path_to_onnx_file , path_to_axon_dir )","ref":"onnx_to_axon.html#onnx-model","title":"Converting ONNX models to Axon - ONNX model","type":"extras"},{"doc":"To run inference on the model, you'll need 10 images focused on cats or dogs. You can download the images used in training the model at: &quot; https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz&quot; Or you can find or use your own images. In this notebook, we are going to use the local copies of the Oxford Pets dataset that was used in training the model. Let's load the Axon model. cats_v_dogs = File . read! ( &quot;models/axon/cats_v_dogs.axon&quot; ) { cats_v_dogs_model , cats_v_dogs_params } = Axon . deserialize ( cats_v_dogs ) We need a tensor representation of an image. Let's start by looking at samples of our data. File . read! ( &quot;data/oxford-iiit-pet/images/havanese_71.jpg&quot; ) |&gt; Kino.Image . new ( :jpeg ) To manipulate the images, we will use the StbImage library: { :ok , img } = StbImage . read_file ( &quot;data/oxford-iiit-pet/images/havanese_71.jpg&quot; ) % StbImage { data : binary , shape : shape , type : type } = StbImage . resize ( img , 224 , 224 ) Now let's work on a batch of images and convert them to tensors. Here are the images we will work with: file_names = [ &quot;havanese_71.jpg&quot; , &quot;yorkshire_terrier_9.jpg&quot; , &quot;Sphynx_206.jpg&quot; , &quot;Siamese_95.jpg&quot; , &quot;Egyptian_Mau_63.jpg&quot; , &quot;keeshond_175.jpg&quot; , &quot;samoyed_88.jpg&quot; , &quot;British_Shorthair_122.jpg&quot; , &quot;Russian_Blue_20.jpg&quot; , &quot;boxer_99.jpg&quot; ] Next we resize the images: resized_images = Enum . map ( file_names , fn file_name -&gt; ( &quot;data/oxford-iiit-pet/images/&quot; &lt;&gt; file_name ) |&gt; IO . inspect ( label : file_name ) |&gt; StbImage . read_file! ( ) |&gt; StbImage . resize ( 224 , 224 ) end ) And finally convert them into tensors by using StbImage.to_nx/1 . The created tensor will have three axes, named :height , :width , and :channel respectively. Our goal is to stack the tensors, then normalize and transpose their axes to the order expected by the neural network: img_tensors = resized_images |&gt; Enum . map ( &amp; StbImage . to_nx / 1 ) |&gt; Nx . stack ( name : :index ) |&gt; Nx . divide ( 255.0 ) |&gt; Nx . transpose ( axes : [ :index , :channels , :height , :width ] ) With our input data, it is finally time to work on predictions. First let's define a helper module: defmodule Predictions do @doc &quot;&quot;&quot; When provided a Tensor of single label predictions, returns the best vocabulary match for each row in the prediction tensor. ## Examples iex&gt; Predictions.sindle_label_prediction(path_to_onnx_file, path_to_axon_dir) [&quot;dog&quot;, &quot;cat&quot;, &quot;dog&quot;] &quot;&quot;&quot; def single_label_classification ( predictions_batch , vocabulary ) do IO . inspect ( Nx . shape ( predictions_batch ) , label : &quot;predictions batch shape&quot; ) for prediction_tensor &lt;- Nx . to_batched ( predictions_batch ) do { _prediction_value , prediction_label } = prediction_tensor |&gt; Nx . to_flat_list ( ) |&gt; Enum . zip ( vocabulary ) |&gt; Enum . max ( ) prediction_label end end end Now we deserialize the model { cats_v_dogs_model , cats_v_dogs_params } = Axon . deserialize ( cats_v_dogs ) run a prediction using the EXLA compiler for performance tensor_of_predictions = Axon . predict ( cats_v_dogs_model , cats_v_dogs_params , img_tensors , compiler : EXLA ) and finally retrieve the predicted label dog_cat_vocabulary = [ &quot;dog&quot; , &quot;cat&quot; ] Predictions . single_label_classification ( tensor_of_predictions , dog_cat_vocabulary ) Let's repeat the above process for the dog and cat breed model. cat_dog_vocabulary = [ &quot;abyssinian&quot; , &quot;american_bulldog&quot; , &quot;american_pit_bull_terrier&quot; , &quot;basset_hound&quot; , &quot;beagle&quot; , &quot;bengal&quot; , &quot;birman&quot; , &quot;bombay&quot; , &quot;boxer&quot; , &quot;british_shorthair&quot; , &quot;chihuahua&quot; , &quot;egyptian_mau&quot; , &quot;english_cocker_spaniel&quot; , &quot;english_setter&quot; , &quot;german_shorthaired&quot; , &quot;great_pyrenees&quot; , &quot;havanese&quot; , &quot;japanese_chin&quot; , &quot;keeshond&quot; , &quot;leonberger&quot; , &quot;maine_coon&quot; , &quot;miniature_pinscher&quot; , &quot;newfoundland&quot; , &quot;persian&quot; , &quot;pomeranian&quot; , &quot;pug&quot; , &quot;ragdoll&quot; , &quot;russian_blue&quot; , &quot;saint_bernard&quot; , &quot;samoyed&quot; , &quot;scottish_terrier&quot; , &quot;shiba_inu&quot; , &quot;siamese&quot; , &quot;sphynx&quot; , &quot;staffordshire_bull_terrier&quot; , &quot;wheaten_terrier&quot; , &quot;yorkshire_terrier&quot; ] cat_dog_breeds = File . read! ( &quot;models/axon/cat_dog_breeds.axon&quot; ) { cat_dog_breeds_model , cat_dog_breeds_params } = Axon . deserialize ( cat_dog_breeds ) Axon . predict ( cat_dog_breeds_model , cat_dog_breeds_params , img_tensors ) |&gt; Predictions . single_label_classification ( cat_dog_vocabulary ) For cat and dog breeds, the model performed pretty well, but it was not perfect.","ref":"onnx_to_axon.html#inference-on-onnx-derived-models","title":"Converting ONNX models to Axon - Inference on ONNX derived models","type":"extras"},{"doc":"Mix . install ( [ { :axon , &quot;~&gt; 0.3.0&quot; } , { :nx , &quot;~&gt; 0.4.0&quot; , override : true } , { :exla , &quot;~&gt; 0.4.0&quot; } , { :kino_vega_lite , &quot;~&gt; 0.1.6&quot; } ] ) Nx.Defn . default_options ( compiler : EXLA ) alias VegaLite , as : Vl","ref":"xor.html","title":"Modeling XOR with a neural network","type":"extras"},{"doc":"In this notebook we try to create a model and learn it the logical XOR . Even though XOR seems like a trivial operation, it cannot be modeled using a single dense layer ( single-layer perceptron ). The underlying reason is that the classes in XOR are not linearly separable. We cannot draw a straight line to separate the points $(0,0)$, $(1,1)$ from the points $(0,1)$, $(1,0)$. To model this properly, we need to turn to deep learning methods. Deep learning is capable of learning non-linear relationships like XOR.","ref":"xor.html#introduction","title":"Modeling XOR with a neural network - Introduction","type":"extras"},{"doc":"Let's start with the model. We need two inputs, since XOR has two operands. We then concatenate them into a single input vector with Axon.concatenate/3 . Then we have one hidden layer and one output layer, both of them dense. Note: the model is a sequential neural network. In Axon, we can conveniently create such a model by using the pipe operator ( |&gt; ) to add layers one by one. x1_input = Axon . input ( &quot;x1&quot; , shape : { nil , 1 } ) x2_input = Axon . input ( &quot;x2&quot; , shape : { nil , 1 } ) model = x1_input |&gt; Axon . concatenate ( x2_input ) |&gt; Axon . dense ( 8 , activation : :tanh ) |&gt; Axon . dense ( 1 , activation : :sigmoid )","ref":"xor.html#the-model","title":"Modeling XOR with a neural network - The model","type":"extras"},{"doc":"The next step is to prepare training data. Since we are modeling a well-defined operation, we can just generate random operands and compute the expected XOR result for them. The training works with batches of examples, so we repeatedly generate a whole batch of inputs and the expected result. batch_size = 32 data = Stream . repeatedly ( fn -&gt; x1 = Nx . random_uniform ( { batch_size , 1 } , 0 , 2 ) x2 = Nx . random_uniform ( { batch_size , 1 } , 0 , 2 ) y = Nx . logical_xor ( x1 , x2 ) { %{ &quot;x1&quot; =&gt; x1 , &quot;x2&quot; =&gt; x2 } , y } end ) Here's how a sample batch looks: Enum . at ( data , 0 )","ref":"xor.html#training-data","title":"Modeling XOR with a neural network - Training data","type":"extras"},{"doc":"It's time to train our model. In this case we use binary cross entropy for the loss and stochastic gradient descent as the optimizer. We use binary cross entropy because we can consider the task of computing XOR the same as a binary classification problem. We want our output to have a binary label 0 or 1 , and binary cross entropy is typically used in these cases. Having defined our training loop, we run it with Axon.Loop.run/4 . epochs = 10 params = model |&gt; Axon.Loop . trainer ( :binary_cross_entropy , :sgd ) |&gt; Axon.Loop . run ( data , %{ } , epochs : epochs , iterations : 1000 )","ref":"xor.html#training","title":"Modeling XOR with a neural network - Training","type":"extras"},{"doc":"Finally, we can test our model on sample data. Axon . predict ( model , params , %{ &quot;x1&quot; =&gt; Nx . tensor ( [ [ 0 ] ] ) , &quot;x2&quot; =&gt; Nx . tensor ( [ [ 1 ] ] ) } ) Try other combinations of $x_1$ and $x_2$ and see what the output is. To improve the model performance, you can increase the number of training epochs.","ref":"xor.html#trying-the-model","title":"Modeling XOR with a neural network - Trying the model","type":"extras"},{"doc":"The original XOR we modeled only works with binary values $0$ and $1$, however our model operates in continuous space. This means that we can give it $x 1 = 0.5$, $x_2 = 0.5$ as input and we expect _some output. We can use this to visualize the non-linear relationship between inputs $x_1$, $x_2$ and outputs that our model has learned. # The number of points per axis, determines the resolution n = 50 # We generate coordinates of in the (n x n) grid x1 = Nx . iota ( { n , n } , axis : 0 ) |&gt; Nx . divide ( n ) |&gt; Nx . reshape ( { :auto , 1 } ) x2 = Nx . iota ( { n , n } , axis : 1 ) |&gt; Nx . divide ( n ) |&gt; Nx . reshape ( { :auto , 1 } ) # The output is also a real number, but we round it into one of the two classes y = Axon . predict ( model , params , %{ &quot;x1&quot; =&gt; x1 , &quot;x2&quot; =&gt; x2 } ) |&gt; Nx . round ( ) Vl . new ( width : 300 , height : 300 ) |&gt; Vl . data_from_values ( x1 : Nx . to_flat_list ( x1 ) , x2 : Nx . to_flat_list ( x2 ) , y : Nx . to_flat_list ( y ) ) |&gt; Vl . mark ( :circle ) |&gt; Vl . encode_field ( :x , &quot;x1&quot; , type : :quantitative ) |&gt; Vl . encode_field ( :y , &quot;x2&quot; , type : :quantitative ) |&gt; Vl . encode_field ( :color , &quot;y&quot; , type : :nominal ) From the plot we can clearly see that during training our model learnt two clean boundaries to separate $(0,0)$, $(1,1)$ from $(0,1)$, $(1,0)$.","ref":"xor.html#visualizing-the-model-predictions","title":"Modeling XOR with a neural network - Visualizing the model predictions","type":"extras"},{"doc":"Mix . install ( [ { :axon , &quot;~&gt; 0.3.0&quot; } , { :nx , &quot;~&gt; 0.4.0&quot; , override : true } , { :exla , &quot;~&gt; 0.4.0&quot; } , { :req , &quot;~&gt; 0.3.1&quot; } ] )","ref":"mnist.html","title":"Classifying handwritten digits","type":"extras"},{"doc":"This livebook will walk you through training a basic neural network using Axon, accelerated by the EXLA compiler. We'll be working on the MNIST dataset which is a dataset of handwritten digits with corresponding labels. The goal is to train a model that correctly classifies these handwritten digits with a single label [0-9].","ref":"mnist.html#introduction","title":"Classifying handwritten digits - Introduction","type":"extras"},{"doc":"The MNIST dataset is available for free online. Using Req we'll download both training images and training labels. Both train_images and train_labels are compressed binary data. Fortunately, Req takes care of the decompression for us. You can read more about the format of the ubyte files here . Each file starts with a magic number and some metadata. We can use binary pattern matching to extract the information we want. In this case we extract the raw binary images and labels. base_url = &quot;https://storage.googleapis.com/cvdf-datasets/mnist/&quot; %{ body : train_images } = Req . get! ( base_url &lt;&gt; &quot;train-images-idx3-ubyte.gz&quot; ) %{ body : train_labels } = Req . get! ( base_url &lt;&gt; &quot;train-labels-idx1-ubyte.gz&quot; ) &lt;&lt; _ :: 32 , n_images :: 32 , n_rows :: 32 , n_cols :: 32 , images :: binary &gt;&gt; = train_images &lt;&lt; _ :: 32 , n_labels :: 32 , labels :: binary &gt;&gt; = train_labels We can easily read that binary data into a tensor using Nx.from_binary/2 . Nx.from_binary/2 expects a raw binary and a data type. In this case, both images and labels are stored as unsigned 8-bit integers. We can start by parsing our images: images = images |&gt; Nx . from_binary ( { :u , 8 } ) |&gt; Nx . reshape ( { n_images , 1 , n_rows , n_cols } , names : [ :images , :channels , :height , :width ] ) |&gt; Nx . divide ( 255 ) Nx.from_binary/2 returns a flat tensor. Using Nx.reshape/3 we can manipulate this flat tensor into meaningful dimensions. Notice we also normalized the tensor by dividing the input data by 255. This squeezes the data between 0 and 1 which often leads to better behavior when training models. Now, let's see what these images look like: images [ [ images : 0 .. 4 ] ] |&gt; Nx . to_heatmap ( ) In the reshape operation above, we give each dimension of the tensor a name. This makes it much easier to do things like slicing, and helps make your code easier to understand. Here we slice the images dimension of the images tensor to obtain the first 5 training images. Then, we convert them to a heatmap for easy visualization. It's common to train neural networks in batches (actually correctly called minibatches, but you'll see batch and minibatch used interchangeably). We can &quot;batch&quot; our images into batches of 32 like this: images = Nx . to_batched ( images , 32 ) Now, we'll need to get our labels into batches as well, but first we need to one-hot encode the labels. One-hot encoding converts input data from labels such as 3 , 5 , 7 , etc. into vectors of 0's and a single 1 at the correct labels index. As an example, a label of: 3 gets converted to: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] . targets = labels |&gt; Nx . from_binary ( { :u , 8 } ) |&gt; Nx . new_axis ( - 1 ) |&gt; Nx . equal ( Nx . tensor ( Enum . to_list ( 0 .. 9 ) ) ) |&gt; Nx . to_batched ( 32 )","ref":"mnist.html#retrieving-and-exploring-the-dataset","title":"Classifying handwritten digits - Retrieving and exploring the dataset","type":"extras"},{"doc":"Let's start by defining a simple model: model = Axon . input ( &quot;input&quot; , shape : { nil , 1 , 28 , 28 } ) |&gt; Axon . flatten ( ) |&gt; Axon . dense ( 128 , activation : :relu ) |&gt; Axon . dense ( 10 , activation : :softmax ) All Axon models start with an input layer to tell subsequent layers what shapes to expect. We then use Axon.flatten/2 which flattens the previous layer by squeezing all dimensions but the first dimension into a single dimension. Our model consists of 2 fully connected layers with 128 and 10 units respectively. The first layer uses :relu activation which returns max(0, input) element-wise. The final layer uses :softmax activation to return a probability distribution over the 10 labels [0 - 9].","ref":"mnist.html#defining-the-model","title":"Classifying handwritten digits - Defining the model","type":"extras"},{"doc":"In Axon we express the task of training using a declarative loop API. First, we need to specify a loss function and optimizer, there are many built-in variants to choose from. In this example, we'll use categorical cross-entropy and the Adam optimizer. We will also keep track of the accuracy metric. Finally, we run training loop passing our batched images and labels. We'll train for 10 epochs using the EXLA compiler. params = model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , :adam ) |&gt; Axon.Loop . metric ( :accuracy , &quot;Accuracy&quot; ) |&gt; Axon.Loop . run ( Stream . zip ( images , targets ) , %{ } , epochs : 10 , compiler : EXLA )","ref":"mnist.html#training","title":"Classifying handwritten digits - Training","type":"extras"},{"doc":"Now that we have the parameters from the training step, we can use them for predictions. For this the Axon.predict can be used. first_batch = Enum . at ( images , 0 ) output = Axon . predict ( model , params , first_batch ) For each image, the model outputs probability distribution. This informs us how certain the model is about its prediction. Let's see the most probable digit for each image: Nx . argmax ( output , axis : 1 ) If you look at the original images and you will see the predictions match the data!","ref":"mnist.html#prediction","title":"Classifying handwritten digits - Prediction","type":"extras"},{"doc":"Mix . install ( [ { :axon , &quot;~&gt; 0.3.0&quot; } , { :nx , &quot;~&gt; 0.4.0&quot; , sparse : &quot;nx&quot; , override : true } , { :exla , &quot;~&gt; 0.4.0&quot; , sparse : &quot;exla&quot; , override : true } , { :stb_image , &quot;~&gt; 0.5.2&quot; } , { :req , &quot;~&gt; 0.3.1&quot; } , { :kino , &quot;~&gt; 0.7.0&quot; } ] ) Nx . global_default_backend ( EXLA.Backend ) Nx.Defn . global_default_options ( compiler : EXLA )","ref":"horses_or_humans.html","title":"Classifying horses and humans","type":"extras"},{"doc":"In this notebook, we want to predict whether an image presents a horse or a human. To do this efficiently, we will build a Convolutional Neural Network (CNN) and compare the learning process with and without gradient centralization.","ref":"horses_or_humans.html#introduction","title":"Classifying horses and humans - Introduction","type":"extras"},{"doc":"We will be using the Horses or Humans Dataset . The dataset is available as a ZIP with image files, we will download it using req . Conveniently, req will unzip the files for us, we just need to convert the filenames from strings. %{ body : files } = Req . get! ( &quot;https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip&quot; ) files = for { name , binary } &lt;- files , do : { List . to_string ( name ) , binary } Note on batching We need to know how many images to include in a batch. A batch is a group of images to load into the GPU at a time. If the batch size is too big for your GPU, it will run out of memory, in such case you can reduce the batch size. It is generally optimal to utilize almost all of the GPU memory during training. It will take more time to train with a lower batch size. batch_size = 32 batches_per_epoch = div ( length ( files ) , batch_size )","ref":"horses_or_humans.html#loading-the-data","title":"Classifying horses and humans - Loading the data","type":"extras"},{"doc":"We'll have a really quick look at our data. Let's see what we are dealing with: { name , binary } = Enum . random ( files ) Kino.Markdown . new ( name ) |&gt; Kino . render ( ) Kino.Image . new ( binary , :png ) Reevaluate the cell a couple times to view different images. Note that the file names are either horse[N]-[M].png or human[N]-[M].png , so we can derive the expected class from that. While we are at it, look at this beautiful animation: names_to_animate = [ &quot;horse01&quot; , &quot;horse05&quot; , &quot;human01&quot; , &quot;human05&quot; ] images_to_animate = for { name , binary } &lt;- files , Enum . any? ( names_to_animate , &amp; String . contains? ( name , &amp;1 ) ) do Kino.Image . new ( binary , :png ) end Kino . animate ( 50 , images_to_animate , fn _i , [ image | images ] -&gt; { :cont , image , images } _i , [ ] -&gt; :halt end ) How many images are there? length ( files ) How many images will not be used for training? The remainder of the integer division will be ignored. files |&gt; length ( ) |&gt; rem ( batch_size )","ref":"horses_or_humans.html#a-look-at-the-data","title":"Classifying horses and humans - A look at the data","type":"extras"},{"doc":"First, we need to preprocess the data for our CNN. At the beginning of the process, we chunk images into batches. Then, we use the parse_file/1 function to load images and label them accurately. Finally, we &quot;augment&quot; the input, which means that we normalize data and flip the images along one of the axes. The last procedure helps a neural network to make predictions regardless of the orientation of the image. defmodule HorsesHumans.DataProcessing do import Nx.Defn def data_stream ( files , batch_size ) do files |&gt; Enum . shuffle ( ) |&gt; Stream . chunk_every ( batch_size , batch_size , :discard ) |&gt; Task . async_stream ( fn batch -&gt; { images , labels } = batch |&gt; Enum . map ( &amp; parse_file / 1 ) |&gt; Enum . unzip ( ) { Nx . stack ( images ) , Nx . stack ( labels ) } end , timeout : :infinity ) |&gt; Stream . map ( fn { :ok , { images , labels } } -&gt; { augment ( images ) , labels } end ) |&gt; Stream . cycle ( ) end defp parse_file ( { filename , binary } ) do label = if String . starts_with? ( filename , &quot;horses/&quot; ) , do : Nx . tensor ( [ 1 , 0 ] , type : { :u , 8 } ) , else : Nx . tensor ( [ 0 , 1 ] , type : { :u , 8 } ) image = binary |&gt; StbImage . read_binary! ( ) |&gt; StbImage . to_nx ( ) { image , label } end defnp augment ( images ) do # Normalize images = images / 255.0 # Optional vertical/horizontal flip u = Nx . random_uniform ( { } ) cond do u &lt; 0.25 -&gt; images u &lt; 0.5 -&gt; Nx . reverse ( images , axes : [ 2 ] ) u &lt; 0.75 -&gt; Nx . reverse ( images , axes : [ 3 ] ) true -&gt; Nx . reverse ( images , axes : [ 2 , 3 ] ) end end end","ref":"horses_or_humans.html#data-processing","title":"Classifying horses and humans - Data processing","type":"extras"},{"doc":"The next step is creating our model. In this notebook, we choose the classic Convolutional Neural Network architecture. Let's dive in to the core components of a CNN. Axon.conv/3 adds a convolutional layer, which is at the core of a CNN. A convolutional layer applies a filter function throughout the image, sliding a window with shape :kernel_size . As opposed to dense layers, a convolutional layer exploits weight sharing to better model data where locality matters. This feature is a natural fit for images. Figure 1: A step-by-step visualization of a convolution layer for kernel_size: {3, 3} Axon.max_pool/2 adds a downscaling operation that takes the maximum value from a subtensor according to :kernel_size . Figure 2: Max pooling operation for kernel_size: {2, 2} Axon.dropout/2 and Axon.spatial_dropout/2 add dropout layers which prevent a neural network from overfitting. Standard dropout drops a given rate of randomly chosen neurons during the training process. On the other hand, spatial dropout gets rid of whole feature maps. The graphical difference between dropout and spatial dropout is presented in a picture below. Figure 3: The difference between standard dropout and spatial dropout Knowing the relevant building blocks, let's build our network! It will have a convolutional part, composed of convolutional and pooling layers, this part should capture the spatial features of an image. Then at the end, we will add a dense layer with 512 neurons fed with all the spatial features, and a final two-neuron layer for as our classification output. model = Axon . input ( &quot;input&quot; , shape : { nil , 300 , 300 , 4 } ) |&gt; Axon . conv ( 16 , kernel_size : { 3 , 3 } , activation : :relu ) |&gt; Axon . max_pool ( kernel_size : { 2 , 2 } ) |&gt; Axon . conv ( 32 , kernel_size : { 3 , 3 } , activation : :relu ) |&gt; Axon . spatial_dropout ( rate : 0.5 ) |&gt; Axon . max_pool ( kernel_size : { 2 , 2 } ) |&gt; Axon . conv ( 64 , kernel_size : { 3 , 3 } , activation : :relu ) |&gt; Axon . spatial_dropout ( rate : 0.5 ) |&gt; Axon . max_pool ( kernel_size : { 2 , 2 } ) |&gt; Axon . conv ( 64 , kernel_size : { 3 , 3 } , activation : :relu ) |&gt; Axon . max_pool ( kernel_size : { 2 , 2 } ) |&gt; Axon . conv ( 64 , kernel_size : { 3 , 3 } , activation : :relu ) |&gt; Axon . max_pool ( kernel_size : { 2 , 2 } ) |&gt; Axon . flatten ( ) |&gt; Axon . dropout ( rate : 0.5 ) |&gt; Axon . dense ( 512 , activation : :relu ) |&gt; Axon . dense ( 2 , activation : :softmax )","ref":"horses_or_humans.html#building-the-model","title":"Classifying horses and humans - Building the model","type":"extras"},{"doc":"It's time to train our model. We specify the loss, optimizer and choose accuracy as our metric. We also set log: 1 to frequently update the training progress. We manually specify the number of iterations, such that each epoch goes through all of the baches once. data = HorsesHumans.DataProcessing . data_stream ( files , batch_size ) optimizer = Axon.Optimizers . adam ( 1.0e-4 ) params = model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , optimizer , :identity , log : 1 ) |&gt; Axon.Loop . metric ( :accuracy ) |&gt; Axon.Loop . run ( data , %{ } , epochs : 10 , iterations : batches_per_epoch )","ref":"horses_or_humans.html#training-the-model","title":"Classifying horses and humans - Training the model","type":"extras"},{"doc":"We can improve the training by applying gradient centralization. It is a technique with a similar purpose to batch normalization. For each loss gradient, we subtract a mean value to have a gradient with mean equal to zero. This process prevents gradients from exploding. centralized_optimizer = Axon.Updates . compose ( Axon.Updates . centralize ( ) , optimizer ) model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , centralized_optimizer , :identity , log : 1 ) |&gt; Axon.Loop . metric ( :accuracy ) |&gt; Axon.Loop . run ( data , %{ } , epochs : 10 , iterations : batches_per_epoch )","ref":"horses_or_humans.html#extra-gradient-centralization","title":"Classifying horses and humans - Extra: gradient centralization","type":"extras"},{"doc":"We can now use our trained model, let's try a couple examples. { name , binary } = Enum . random ( files ) Kino.Markdown . new ( name ) |&gt; Kino . render ( ) Kino.Image . new ( binary , :png ) |&gt; Kino . render ( ) input = binary |&gt; StbImage . read_binary! ( ) |&gt; StbImage . to_nx ( ) |&gt; Nx . new_axis ( 0 ) |&gt; Nx . divide ( 255.0 ) Axon . predict ( model , params , input ) Note: the model output refers to the probability that the image presents a horse and a human respectively. The website from where we loaded the dataset also includes a validation set, in case you want to experiment further!","ref":"horses_or_humans.html#inference","title":"Classifying horses and humans - Inference","type":"extras"},{"doc":"Mix . install ( [ { :axon , &quot;~&gt; 0.3.0&quot; } , { :nx , &quot;~&gt; 0.4.0&quot; , override : true } , { :exla , &quot;~&gt; 0.4.0&quot; } , { :req , &quot;~&gt; 0.3.1&quot; } ] ) Nx.Defn . default_options ( compiler : EXLA ) Nx . global_default_backend ( EXLA.Backend )","ref":"lstm_generation.html","title":"Generating text with LSTM","type":"extras"},{"doc":"Recurrent Neural Networks (RNNs) can be used as generative models. This means that in addition to being used for predictive models (making predictions) they can learn the sequences of a problem and then generate entirely new plausible sequences for the problem domain. Generative models like this are useful not only to study how well a model has learned a problem, but to learn more about the problem domain itself. In this example, we will discover how to create a generative model for text, character-by-character using Long Short-Term Memory (LSTM) recurrent neural networks in Elixir with Axon.","ref":"lstm_generation.html#introduction","title":"Generating text with LSTM - Introduction","type":"extras"},{"doc":"Using Project Gutenburg we can download a text books that are no longer protected under copywrite, so we can experiment with them. The one that we will use for this experiment is Alice's Adventures in Wonderland by Lewis Carroll . You can choose any other text or book that you like for this experiment. # Change the URL if you&#39;d like to experiment with other books download_url = &quot;https://www.gutenberg.org/files/11/11-0.txt&quot; book_text = Req . get! ( download_url ) . body First of all, we need to normalize the content of the book. We are only interested in the sequence of English characters, periods and new lines. Also currently we don't care about the capitalization and things like apostrophe so we can remove all other unknown characters and downcase everything. We can use a regular expression for that. We can also convert the string into a list of characters so we can handle them easier. You will understand exactly why a bit further. normalized_book_text = book_text |&gt; String . downcase ( ) |&gt; String . replace ( ~r/[^a-z \\. \\n ]/ , &quot;&quot; ) |&gt; String . to_charlist ( ) We converted the text to a list of characters, where each character is a number (specifically, a Unicode code point). Lowercase English characters are represented with numbers between 97 = a and 122 = z , a space is 32 = [ ] , a new line is 10 = \\n and the period is 46 = . . So we should have 26 + 3 (= 29) characters in total. Let's see if that's true. normalized_book_text |&gt; Enum . uniq ( ) |&gt; Enum . count ( ) Since we want to use this 29 characters as possible values for each input in our neural network, we can re-map them to values between 0 and 28. So each specific neuron will indicate a specific character. # Extract all then unique characters we have and sort them for clarity characters = normalized_book_text |&gt; Enum . uniq ( ) |&gt; Enum . sort ( ) characters_count = Enum . count ( characters ) # Create a mapping for every character char_to_idx = characters |&gt; Enum . with_index ( ) |&gt; Map . new ( ) # And a reverse mapping to convert back to characters idx_to_char = characters |&gt; Enum . with_index ( &amp; { &amp;2 , &amp;1 } ) |&gt; Map . new ( ) IO . puts ( &quot;Total book characters: \#{ Enum . count ( normalized_book_text ) } &quot; ) IO . puts ( &quot;Total unique characters: \#{ characters_count } &quot; ) Now we need to create our training and testing data sets. But how? Our goal is to teach the machine what comes after a sequence of characters (usually). For example given the following sequence &quot;Hello, My name i&quot; the computer should be able to guess that the next character is probably &quot;s&quot; . graph LR; A[Input: Hello my name i]--&gt;NN[Neural Network]--&gt;B[Output: s]; Let's choose an arbitrary sequence length and create a data set from the book text. All we need to do is read X amount of characters from the book as the input and then read 1 more as the designated output. After doing all that, we also want to convert every character to it's index using the char_to_idx mapping that we have created before. Neural networks work best if you scale your inputs and outputs. In this case we are going to scale everything between 0 and 1 by dividing them by the number of unique characters that we have. And for the final step we will reshape it so we can use the data in our LSTM model. sequence_length = 100 train_data = normalized_book_text |&gt; Enum . map ( &amp; Map . fetch! ( char_to_idx , &amp;1 ) ) |&gt; Enum . chunk_every ( sequence_length , 1 , :discard ) # We don&#39;t want the last chunk since we don&#39;t have a prediction for it. |&gt; Enum . drop ( - 1 ) |&gt; Nx . tensor ( ) |&gt; Nx . divide ( characters_count ) |&gt; Nx . reshape ( { :auto , sequence_length , 1 } ) For our train results, We will do the same. Drop the first sequence_length characters and then convert them to the mapping. Additionally, we will do one-hot encoding . The reason we want to use one-hot encoding is that in our model we don't want to only return a character as the output. We want it to return the probability of each character for the output. This way we can decide if certain probability is good or not or even we can decide between multiple possible outputs or even discard everything if the network is not confident enough. In Nx, you can achieve this encoding by using this snippet Nx . tensor ( [ [ 0 ] , [ 1 ] , [ 2 ] ] ) |&gt; Nx . equal ( Nx . iota ( { 1 , 3 } ) ) To sum it up, Here is how we generate the train results. train_results = normalized_book_text |&gt; Enum . drop ( sequence_length ) |&gt; Enum . map ( &amp; Map . fetch! ( char_to_idx , &amp;1 ) ) |&gt; Nx . tensor ( ) |&gt; Nx . reshape ( { :auto , 1 } ) |&gt; Nx . equal ( Nx . iota ( { 1 , characters_count } ) )","ref":"lstm_generation.html#preparation","title":"Generating text with LSTM - Preparation","type":"extras"},{"doc":"# As the input, we expect the sequence_length characters model = Axon . input ( &quot;input_chars&quot; , shape : { nil , sequence_length , 1 } ) # The LSTM layer of our network |&gt; Axon . lstm ( 256 ) # Selecting only the output from the LSTM Layer |&gt; then ( fn { out , _ } -&gt; out end ) # Since we only want the last sequence in LSTM we will slice it and # select the last one |&gt; Axon . nx ( fn t -&gt; t [ [ 0 .. - 1 // 1 , - 1 ] ] end ) # 20% dropout so we will not become too dependent on specific neurons |&gt; Axon . dropout ( rate : 0.2 ) # The output layer. One neuron for each character and using softmax, # as activation so every node represents a probability |&gt; Axon . dense ( characters_count , activation : :softmax )","ref":"lstm_generation.html#defining-the-model","title":"Generating text with LSTM - Defining the Model","type":"extras"},{"doc":"To train the network, we will use Axon's Loop API. It is pretty straightforward. For the loss function we can use categorical cross-entropy since we are dealing with categories (each character) in our output. For the optimizer we can use Adam . We will train our network for 20 epochs. Note that we are working with a fair amount data, so it may take a long time unless you run it on a GPU. batch_size = 128 train_batches = Nx . to_batched ( train_data , batch_size ) result_batches = Nx . to_batched ( train_results , batch_size ) IO . puts ( &quot;Total batches: \#{ Enum . count ( train_batches ) } &quot; ) params = model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , Axon.Optimizers . adam ( 0.001 ) ) |&gt; Axon.Loop . run ( Stream . zip ( train_batches , result_batches ) , %{ } , epochs : 20 , compiler : EXLA ) :ok","ref":"lstm_generation.html#training-the-network","title":"Generating text with LSTM - Training the network","type":"extras"},{"doc":"Now we have a trained neural network, so we can start generating text with it! We just need to pass the initial sequence as the input to the network and select the most probable output. Axon.predict/3 will give us the output layer and then using Nx.argmax/1 we get the most confident neuron index, then simply convert that index back to its Unicode representation. generate_fn = fn model , params , init_seq -&gt; # The initial sequence that we want the network to complete for us. init_seq = init_seq |&gt; String . trim ( ) |&gt; String . downcase ( ) |&gt; String . to_charlist ( ) |&gt; Enum . map ( &amp; Map . fetch! ( char_to_idx , &amp;1 ) ) Enum . reduce ( 1 .. 100 , init_seq , fn _ , seq -&gt; init_seq = seq |&gt; Enum . take ( - sequence_length ) |&gt; Nx . tensor ( ) |&gt; Nx . divide ( characters_count ) |&gt; Nx . reshape ( { 1 , sequence_length , 1 } ) char = Axon . predict ( model , params , init_seq ) |&gt; Nx . argmax ( ) |&gt; Nx . to_number ( ) seq ++ [ char ] end ) |&gt; Enum . map ( &amp; Map . fetch! ( idx_to_char , &amp;1 ) ) end # The initial sequence that we want the network to complete for us. init_seq = &quot;&quot;&quot; not like to drop the jar for fear of killing somebody underneath so managed to put it into one of the cupboards as she fell past it. &quot;&quot;&quot; generate_fn . ( model , params , init_seq ) |&gt; IO . puts ( )","ref":"lstm_generation.html#generating-text","title":"Generating text with LSTM - Generating text","type":"extras"},{"doc":"We can improve our network by stacking multiple LSTM layers together. We just need to change our model and re-train our network. new_model = Axon . input ( &quot;input_chars&quot; , shape : { nil , sequence_length , 1 } ) |&gt; Axon . lstm ( 256 ) |&gt; then ( fn { out , _ } -&gt; out end ) |&gt; Axon . dropout ( rate : 0.2 ) # This time we will pass all of the `out` to the next lstm layer. # We just need to slice the last one. |&gt; Axon . lstm ( 256 ) |&gt; then ( fn { out , _ } -&gt; out end ) |&gt; Axon . nx ( fn x -&gt; x [ [ 0 .. - 1 // 1 , - 1 ] ] end ) |&gt; Axon . dropout ( rate : 0.2 ) |&gt; Axon . dense ( characters_count , activation : :softmax ) Then we can train the network using the exact same code as before # Using a smaller batch size in this case will give the network more opportunity to learn batch_size = 64 train_batches = Nx . to_batched ( train_data , batch_size ) result_batches = Nx . to_batched ( train_results , batch_size ) IO . puts ( &quot;Total batches: \#{ Enum . count ( train_batches ) } &quot; ) new_params = new_model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , Axon.Optimizers . adam ( 0.001 ) ) |&gt; Axon.Loop . run ( Stream . zip ( train_batches , result_batches ) , %{ } , epochs : 50 , compiler : EXLA ) :ok","ref":"lstm_generation.html#multi-lstm-layers","title":"Generating text with LSTM - Multi LSTM layers","type":"extras"},{"doc":"generate_fn . ( new_model , new_params , init_seq ) |&gt; IO . puts ( ) As you may see, it improved a lot with this new model and the extensive training. This time it knows about rules like adding a space after period.","ref":"lstm_generation.html#generate-text-with-the-new-network","title":"Generating text with LSTM - Generate text with the new network","type":"extras"},{"doc":"The above example was written heavily inspired by this article by Jason Brownlee.","ref":"lstm_generation.html#references","title":"Generating text with LSTM - References","type":"extras"},{"doc":"Mix . install ( [ { :axon , &quot;~&gt; 0.3.0&quot; } , { :nx , &quot;~&gt; 0.4.0&quot; , override : true } , { :exla , &quot;~&gt; 0.4.0&quot; } , { :explorer , &quot;~&gt; 0.3.1&quot; } , { :kino , &quot;~&gt; 0.7.0&quot; } ] ) Nx.Defn . default_options ( compiler : EXLA ) Nx . global_default_backend ( EXLA.Backend ) alias Explorer . { DataFrame , Series }","ref":"credit_card_fraud.html","title":"Classifying fraudulent transactions","type":"extras"},{"doc":"This time we will examine the Credit Card Fraud Dataset. Due to confidentiality, the original data were preprocessed by principal component analysis (PCA), and then 31 principal components were selected for the final data set. The dataset is highly imbalanced. The positive class (frauds) account for 0.172% of all transactions. Eventually, we will create a classifier which has not only great accuracy but, what is even more important, a high recall and precision - two metrics that are much more indicative of performance with imbalanced classification problems.","ref":"credit_card_fraud.html#introduction","title":"Classifying fraudulent transactions - Introduction","type":"extras"},{"doc":"The first step is to prepare the data for training and evaluation. Please download the dataset in the CSV format from https://www.kaggle.com/mlg-ulb/creditcardfraud (this requires a Kaggla account). Once done, put the file path in the input below. data_path_input = Kino.Input . text ( &quot;Data path (CSV)&quot; ) Now, let's read the data into an Explorer.Dataframe : data_path = Kino.Input . read ( data_path_input ) df = DataFrame . from_csv! ( data_path , dtypes : [ { &quot;Time&quot; , :float } ] ) For further processing, we will need a couple helper functions. We will group them in a module for convenience. defmodule CredidCard.Data do import Nx.Defn def split_train_test ( df , portion ) do num_examples = DataFrame . n_rows ( df ) num_train = ceil ( portion * num_examples ) num_test = num_examples - num_train train = DataFrame . slice ( df , 0 , num_train ) test = DataFrame . slice ( df , num_train , num_test ) { train , test } end def split_features_targets ( df ) do features = DataFrame . select ( df , &amp; ( &amp;1 == &quot;Class&quot; ) , :drop ) targets = DataFrame . select ( df , &amp; ( &amp;1 == &quot;Class&quot; ) , :keep ) { features , targets } end def df_to_tensor ( df ) do df |&gt; DataFrame . names ( ) |&gt; Enum . map ( &amp; Series . to_tensor ( df [ &amp;1 ] ) ) |&gt; Nx . stack ( axis : 1 ) end defn normalize_features ( tensor ) do max = tensor |&gt; Nx . abs ( ) |&gt; Nx . reduce_max ( axes : [ 0 ] , keep_axes : true ) tensor / max end end With that, we can start converting the data into the desired format. First, we split the data into training and test data (in proportion 80% into a training set and 20% into a test set). { train_df , test_df } = CredidCard.Data . split_train_test ( df , 0.8 ) { DataFrame . n_rows ( train_df ) , DataFrame . n_rows ( test_df ) } Next, we separate features from labels and convert both to tensors. In case of features we additionally normalize each of them, dividing by the maximum absolute value of that feature. { train_features , train_targets } = CredidCard.Data . split_features_targets ( train_df ) { test_features , test_targets } = CredidCard.Data . split_features_targets ( test_df ) train_inputs = train_features |&gt; CredidCard.Data . df_to_tensor ( ) |&gt; CredidCard.Data . normalize_features ( ) test_inputs = test_features |&gt; CredidCard.Data . df_to_tensor ( ) |&gt; CredidCard.Data . normalize_features ( ) train_targets = CredidCard.Data . df_to_tensor ( train_targets ) test_targets = CredidCard.Data . df_to_tensor ( test_targets ) :ok","ref":"credit_card_fraud.html#data-processing","title":"Classifying fraudulent transactions - Data processing","type":"extras"},{"doc":"Our model for predicting whether a transaction was fraudulent or not is a dense neural network. It consists of two dense layers with 256 neurons, ReLU activation functions, one dropout layer, and a dense layer with one neuron (since the problem is a binary prediction) followed by a sigmoid activation function. model = Axon . input ( &quot;input&quot; ) |&gt; Axon . dense ( 256 ) |&gt; Axon . relu ( ) |&gt; Axon . dense ( 256 ) |&gt; Axon . relu ( ) |&gt; Axon . dropout ( rate : 0.3 ) |&gt; Axon . dense ( 1 ) |&gt; Axon . sigmoid ( )","ref":"credit_card_fraud.html#building-the-model","title":"Classifying fraudulent transactions - Building the model","type":"extras"},{"doc":"Now we have both data and model architecture prepared, it's time to train! Note the disproportion in the data samples: fraud = Nx . sum ( train_targets ) |&gt; Nx . to_number ( ) legit = Nx . size ( train_targets ) - fraud batched_train_inputs = Nx . to_batched ( train_inputs , 2048 ) batched_train_targets = Nx . to_batched ( train_targets , 2048 ) batched_train = Stream . zip ( batched_train_inputs , batched_train_targets ) IO . puts ( &quot;# of legit transactions (train): \#{ legit } &quot; ) IO . puts ( &quot;# of fraudulent transactions (train): \#{ fraud } &quot; ) IO . puts ( &quot;% fraudlent transactions (train): \#{ 100 * ( fraud / ( legit + fraud ) ) } %&quot; ) As always, we define our train loop. We are using binary cross-entropy as our loss function and Adam as the optimizer with a learning rate of 0.01. Then we immediately start the training passing our train portion of the dataset. loss = &amp; Axon.Losses . binary_cross_entropy ( &amp;1 , &amp;2 , negative_weight : 1 / legit , positive_weight : 1 / fraud , reduction : :mean ) optimizer = Axon.Optimizers . adam ( 1.0e-2 ) params = model |&gt; Axon.Loop . trainer ( loss , optimizer ) |&gt; Axon.Loop . run ( batched_train , %{ } , epochs : 30 , compiler : EXLA ) :ok","ref":"credit_card_fraud.html#training-our-model","title":"Classifying fraudulent transactions - Training our model","type":"extras"},{"doc":"After the training, there is only one thing left: testing. Here, we will focus on the number of true positive, true negative, false positive, and false negative values, but also on the likelihood of denying legit and fraudulent transactions. batched_test_inputs = Nx . to_batched ( test_inputs , 2048 ) batched_test_targets = Nx . to_batched ( test_targets , 2048 ) batched_test = Stream . zip ( batched_test_inputs , batched_test_targets ) summarize = fn % Axon.Loop.State { metrics : metrics } = state -&gt; legit_transactions_declined = Nx . to_number ( metrics [ &quot;fp&quot; ] ) legit_transactions_accepted = Nx . to_number ( metrics [ &quot;tn&quot; ] ) fraud_transactions_accepted = Nx . to_number ( metrics [ &quot;fn&quot; ] ) fraud_transactions_declined = Nx . to_number ( metrics [ &quot;tp&quot; ] ) total_fraud = fraud_transactions_declined + fraud_transactions_accepted total_legit = legit_transactions_declined + legit_transactions_accepted fraud_denial_percent = 100 * ( fraud_transactions_declined / total_fraud ) legit_denial_percent = 100 * ( legit_transactions_declined / total_legit ) IO . write ( &quot; \\n &quot; ) IO . puts ( &quot;Legit Transactions Declined: \#{ legit_transactions_declined } &quot; ) IO . puts ( &quot;Fraudulent Transactions Caught: \#{ fraud_transactions_declined } &quot; ) IO . puts ( &quot;Fraudulent Transactions Missed: \#{ fraud_transactions_accepted } &quot; ) IO . puts ( &quot;Likelihood of catching fraud: \#{ fraud_denial_percent } %&quot; ) IO . puts ( &quot;Likelihood of denying legit transaction: \#{ legit_denial_percent } %&quot; ) { :continue , state } end model |&gt; Axon.Loop . evaluator ( ) |&gt; Axon.Loop . metric ( :true_positives , &quot;tp&quot; , :running_sum ) |&gt; Axon.Loop . metric ( :true_negatives , &quot;tn&quot; , :running_sum ) |&gt; Axon.Loop . metric ( :false_positives , &quot;fp&quot; , :running_sum ) |&gt; Axon.Loop . metric ( :false_negatives , &quot;fn&quot; , :running_sum ) |&gt; Axon.Loop . handle ( :epoch_completed , summarize ) |&gt; Axon.Loop . run ( batched_test , params , compiler : EXLA ) :ok","ref":"credit_card_fraud.html#model-evaluation","title":"Classifying fraudulent transactions - Model evaluation","type":"extras"},{"doc":"Mix . install ( [ { :exla , &quot;~&gt; 0.4.0&quot; } , { :nx , &quot;~&gt; 0.4.0&quot; , override : true } , { :axon , &quot;~&gt; 0.3.0&quot; } , { :req , &quot;~&gt; 0.3.1&quot; } , { :kino , &quot;~&gt; 0.7.0&quot; } , { :scidata , &quot;~&gt; 0.1.9&quot; } , { :stb_image , &quot;~&gt; 0.5.2&quot; } , { :table_rex , &quot;~&gt; 3.1.1&quot; } ] )","ref":"mnist_autoencoder_using_kino.html","title":"MNIST Denoising Autoencoder using Kino for visualization","type":"extras"},{"doc":"The goal of this notebook is to build a Denoising Autoencoder from scratch using Livebook. This notebook is based on Training an Autoencoder on Fashion MNIST , but includes some tips on using Livebook to train the model and using Kino (Livebook's interactive widget library) to play with and visualize our results.","ref":"mnist_autoencoder_using_kino.html#introduction","title":"MNIST Denoising Autoencoder using Kino for visualization - Introduction","type":"extras"},{"doc":"An autoencoder learns to recreate data it's seen in the dataset. For this notebook, we're going to try something simple: generating images of digits using the MNIST digit recognition dataset. Following along with the Fashion MNIST Autoencoder example , we'll use Scidata to download the MNIST dataset and then preprocess the data. # We&#39;re not going to use the labels so we&#39;ll ignore them { train_images , _train_labels } = Scidata.MNIST . download ( ) { train_images_binary , type , shape } = train_images The shape tells us we have 60,000 images with a single channel of size 28x28. According to the MNIST website : Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black). Let's preprocess and normalize the data accordingly. train_images = train_images_binary |&gt; Nx . from_binary ( type ) # Since pixels are organized row-wise, reshape into rows x columns |&gt; Nx . reshape ( shape , names : [ :images , :channels , :height , :width ] ) # Normalize the pixel values to be between 0 and 1 |&gt; Nx . divide ( 255 ) # Make sure they look like numbers train_images [ [ images : 0 .. 2 ] ] |&gt; Nx . to_heatmap ( ) That looks right! Let's repeat the process for the test set. { test_images , _train_labels } = Scidata.MNIST . download_test ( ) { test_images_binary , type , shape } = test_images test_images = test_images_binary |&gt; Nx . from_binary ( type ) # Since pixels are organized row-wise, reshape into rows x columns |&gt; Nx . reshape ( shape , names : [ :images , :channels , :height , :width ] ) # Normalize the pixel values to be between 0 and 1 |&gt; Nx . divide ( 255 ) test_images [ [ images : 0 .. 2 ] ] |&gt; Nx . to_heatmap ( )","ref":"mnist_autoencoder_using_kino.html#data-loading","title":"MNIST Denoising Autoencoder using Kino for visualization - Data loading","type":"extras"},{"doc":"An autoencoder is a a network that has the same sized input as output, with a &quot;bottleneck&quot; layer in the middle with far fewer parameters than the input. Its goal is to force the output to reconstruct the input. The bottleneck layer forces the network to learn a compressed representation of the input space. A denoising autoencoder is a small tweak on an autoencoder that takes a corrupted input (often corrupted by adding noise or zeroing out pixels) and reconstructs the original input, removing the noise in the process. The part of the autoencoder that takes the input and compresses it into the bottleneck layer is called the encoder and the part that takes the compressed representation and reconstructs the input is called the decoder . Usually the decoder mirrors the encoder. MNIST is a pretty easy dataset, so we're going to try a fairly small autoencoder. The input image has size 784 (28 rows 28 cols 1 pixel). We'll set up the encoder to turn that into 256 features, then 128, 64, and then 10 features for the bottleneck layer. The decoder will do the reverse, take the 10 features and go to 64, 128, 256 and 784. I'll use fully-connected (dense) layers. The model model = Axon . input ( &quot;image&quot; , shape : { nil , 1 , 28 , 28 } ) # This is now 28*28*1 = 784 |&gt; Axon . flatten ( ) # The encoder |&gt; Axon . dense ( 256 , activation : :relu ) |&gt; Axon . dense ( 128 , activation : :relu ) |&gt; Axon . dense ( 64 , activation : :relu ) # Bottleneck layer |&gt; Axon . dense ( 10 , activation : :relu ) # The decoder |&gt; Axon . dense ( 64 , activation : :relu ) |&gt; Axon . dense ( 128 , activation : :relu ) |&gt; Axon . dense ( 256 , activation : :relu ) |&gt; Axon . dense ( 784 , activation : :sigmoid ) # Turn it back into a 28x28 single channel image |&gt; Axon . reshape ( { :auto , 1 , 28 , 28 } ) # We can use Axon.Display to show us what each of the layers would look like # assuming we send in a batch of 4 images Axon.Display . as_table ( model , Nx . template ( { 4 , 1 , 28 , 28 } , :f32 ) ) |&gt; IO . puts ( ) Checking our understanding, since the layers are all dense layers, the number of parameters should be input_features * output_features parameters for the weights + output_features parameters for the biases for each layer. This should match the Total Parameters output from Axon.Display (486298 parameters) # encoder encoder_parameters = 784 * 256 + 256 + ( 256 * 128 + 128 ) + ( 128 * 64 + 64 ) + ( 64 * 10 + 10 ) decoder_parameters = 10 * 64 + 64 + ( 64 * 128 + 128 ) + ( 128 * 256 + 256 ) + ( 256 * 784 + 784 ) total_parameters = encoder_parameters + decoder_parameters Training With the model set up, we can now try to train the model. We'll use MSE loss to compare our reconstruction with the original We'll create the training input by turning our image list into batches of size 128 and then using the same image as both the input and the target. However, the input image will have noise added to it that the autoencoder will have to remove. For validation data, we'll use the test set and look at how the autoencoder does at reconstructing the test set to make sure we're not overfitting The function below adds some noise to the image by adding the image with gaussian noise scaled by a noise factor. We then have to make sure the pixel values are still within the 0..1.0 range. We have to define this function using defn so that Nx can optimize it. If we don't do this, adding noise will take a really long time, making our training loop very slow. See Nx.defn for more details. defn can only be used in a module so we'll define a little module to contain it. defmodule Noiser do import Nx.Defn @noise_factor 0.4 defn add_noise ( images ) do @noise_factor |&gt; Nx . multiply ( Nx . random_normal ( images ) ) |&gt; Nx . add ( images ) |&gt; Nx . clip ( 0.0 , 1.0 ) end end add_noise = Nx.Defn . jit ( &amp; Noiser . add_noise / 1 , compiler : EXLA ) batch_size = 128 # The original image which is the target the network will trying to match batched_train_images = train_images |&gt; Nx . to_batched ( batch_size ) batched_noisy_train_images = train_images |&gt; Nx . to_batched ( batch_size ) # goes after to_batched so the noise is different every time |&gt; Stream . map ( add_noise ) # The noisy image is the input to the network # and the original image is the target it&#39;s trying to match train_data = Stream . zip ( batched_noisy_train_images , batched_train_images ) batched_test_images = test_images |&gt; Nx . to_batched ( batch_size ) batched_noisy_test_images = test_images |&gt; Nx . to_batched ( batch_size ) |&gt; Stream . map ( add_noise ) test_data = Stream . zip ( batched_noisy_test_images , batched_test_images ) Let's see what an element of the input and target look like { input_batch , target_batch } = Enum . at ( train_data , 0 ) { Nx . to_heatmap ( input_batch [ images : 0 ] ) , Nx . to_heatmap ( target_batch [ images : 0 ] ) } Looks right (and tricky). Let's see how the model does. params = model |&gt; Axon.Loop . trainer ( :mean_squared_error , Axon.Optimizers . adamw ( 0.001 ) ) |&gt; Axon.Loop . validate ( model , test_data ) |&gt; Axon.Loop . run ( train_data , %{ } , epochs : 20 , compiler : EXLA ) :ok Now that we have a model that theoretically has learned something , we'll see what it's learned by running it on some images from the test set. We'll use Kino to allow us to select the image from the test set to run the model against. To avoid losing the params that took a while to train, we'll create another branch so we can experiment with the params and stop execution when needed without having to retrain.","ref":"mnist_autoencoder_using_kino.html#building-the-model","title":"MNIST Denoising Autoencoder using Kino for visualization - Building the model","type":"extras"},{"doc":"A note on branching By default, everything in Livebook runs sequentially in a single process. Stopping a running cell aborts that process and consequently all its state is lost. A branching section copies everything from its parent and runs in a separate process. Thanks to this isolation , when we stop a cell in a branching section, only the state within that section is gone. Since we just spent a bunch of time training the model and don't want to lose that memory state as we continue to experiment, we create a branching section. This does add some memory overhead, but it's worth it so we can experiment without fear! To use Kino to give us an interactive tool to evaluate the model, we'll create a Kino.Frame that we can dynamically update. We'll also create a form using Kino.Control to allow the user to select which image from the test set they'd like to evaluate the model on. Finally Kino.Control.stream enables us to respond to changes in the user's selection when the user clicks the &quot;Render&quot; button. We can use Nx.concatenate to stack the images side by side for a prettier output. form = Kino.Control . form ( [ test_image_index : Kino.Input . number ( &quot;Test Image Index&quot; , default : 0 ) ] , submit : &quot;Render&quot; ) Kino . render ( form ) form |&gt; Kino.Control . stream ( ) |&gt; Kino . animate ( fn %{ data : %{ test_image_index : image_index } } -&gt; test_image = test_images [ [ images : image_index ] ] |&gt; add_noise . ( ) reconstructed_image = model |&gt; Axon . predict ( params , test_image ) # Get rid of the batch dimension |&gt; Nx . squeeze ( axes : [ 0 ] ) combined_image = Nx . concatenate ( [ test_image , reconstructed_image ] , axis : :width ) Nx . to_heatmap ( combined_image ) end ) That looks pretty good! Note we used Kino.animate/2 which runs asynchronously so we don't block execution of the rest of the notebook.","ref":"mnist_autoencoder_using_kino.html#evaluation","title":"MNIST Denoising Autoencoder using Kino for visualization - Evaluation","type":"extras"},{"doc":"Note that we branch from the &quot;Building a model&quot; section since we only need the model definition for this section and not the previously trained model. It'd be nice to see how the model improves as it trains. In this section (also a branch since I plan to experiment and don't want to lose the execution state) we'll improve the training loop to use Kino to show us how it's doing. Axon.Loop.handle gives us a hook into various points of the training loop. We'll can use it with the :iteration_completed event to get a copy of the state of the params after some number of completed iterations of the training loop. By using those params to render an image in the test set, we can get a live view of the autoencoder learning to reconstruct its inputs. # A helper function to display the input and output side by side combined_input_output = fn params , image_index -&gt; test_image = test_images [ [ images : image_index ] ] |&gt; add_noise . ( ) reconstructed_image = Axon . predict ( model , params , test_image ) |&gt; Nx . squeeze ( axes : [ 0 ] ) Nx . concatenate ( [ test_image , reconstructed_image ] , axis : :width ) end Nx . to_heatmap ( combined_input_output . ( params , 0 ) ) It'd also be nice to have a prettier version of the output. Let's convert the heatmap to a png to make that happen. image_to_kino = fn image -&gt; image |&gt; Nx . multiply ( 255 ) |&gt; Nx . as_type ( :u8 ) |&gt; Nx . transpose ( axes : [ :height , :width , :channels ] ) |&gt; StbImage . from_nx ( ) |&gt; StbImage . resize ( 200 , 400 ) |&gt; StbImage . to_binary ( :png ) |&gt; Kino.Image . new ( :png ) end image_to_kino . ( combined_input_output . ( params , 0 ) ) Much nicer! Once again we'll use Kino.Frame for dynamically updating output: frame = Kino.Frame . new ( ) |&gt; Kino . render ( ) render_example_handler = fn state -&gt; Kino.Frame . append ( frame , &quot;Epoch: \#{ state . epoch } , Iteration: \#{ state . iteration } &quot; ) # state.step_state[:model_state] contains the model params when this event is fired params = state . step_state [ :model_state ] image_index = Enum . random ( 0 .. ( Nx . axis_size ( test_images , :images ) - 1 ) ) image = combined_input_output . ( params , image_index ) |&gt; image_to_kino . ( ) Kino.Frame . append ( frame , image ) { :continue , state } end params = model |&gt; Axon.Loop . trainer ( :mean_squared_error , Axon.Optimizers . adamw ( 0.001 ) ) |&gt; Axon.Loop . handle ( :iteration_completed , render_example_handler , every : 450 ) |&gt; Axon.Loop . validate ( model , test_data ) |&gt; Axon.Loop . run ( train_data , %{ } , epochs : 20 , compiler : EXLA ) :ok Awesome! We have a working denoising autoencoder that we can visualize getting better in 20 epochs!","ref":"mnist_autoencoder_using_kino.html#a-better-training-loop","title":"MNIST Denoising Autoencoder using Kino for visualization - A better training loop","type":"extras"},{"doc":"Mix . install ( [ { :axon , &quot;~&gt; 0.3.0&quot; } , { :nx , &quot;~&gt; 0.4.0&quot; , override : true } , { :exla , &quot;~&gt; 0.4.0&quot; } , { :scidata , &quot;~&gt; 0.1.9&quot; } ] ) Nx.Defn . default_options ( compiler : EXLA )","ref":"fashionmnist_autoencoder.html","title":"Training an Autoencoder on Fashion MNIST","type":"extras"},{"doc":"An autoencoder is a deep learning model which consists of two parts: encoder and decoder. The encoder compresses high dimensional data into a low dimensional representation and feeds it to the decoder. The decoder tries to recreate the original data from the low dimensional representation. Autoencoders can be used in the following problems: Dimensionality reduction Noise reduction Generative models Data augmentation Let's walk through a basic autoencoder implementation in Axon to get a better understanding of how they work in practice.","ref":"fashionmnist_autoencoder.html#introduction","title":"Training an Autoencoder on Fashion MNIST - Introduction","type":"extras"},{"doc":"To train and test how our model works, we use one of the most popular data sets: Fashion MNIST . It consists of small black and white images of clothes. Loading this data set is very simple with the help of Scidata . { image_data , _label_data } = Scidata.FashionMNIST . download ( ) { bin , type , shape } = image_data We get the data in a raw format, but this is exactly the information we need to build an Nx tensor. train_images = bin |&gt; Nx . from_binary ( type ) |&gt; Nx . reshape ( shape ) |&gt; Nx . divide ( 255.0 ) We also normalize pixel values into the range $[0, 1]$. We can visualize one of the images by looking at the tensor heatmap: Nx . to_heatmap ( train_images [ 1 ] )","ref":"fashionmnist_autoencoder.html#downloading-the-data","title":"Training an Autoencoder on Fashion MNIST - Downloading the data","type":"extras"},{"doc":"First we need to define the encoder and decoder. Both are one-layer neural networks. In the encoder, we start by flattening the input using Axon.flatten because initially, the input shape is {batch_size, 1, 28, 28} and we want to pass the input into a dense layer with Axon.dense . Our dense layer has only latent_dim number of neurons. The latent_dim or latent space is a compressed representation of data. Remember, we want our encoder to compress the input data into a lower-dimensional representation, so we choose a latent_dim which is less than the dimensionality of the input. Next, we pass the output of the encoder to the decoder and try to reconstruct the compressed data into its original form. Since our original input had a dimensionality of 784, we use an Axon.dense layer with 784 neurons. Because our original data was normalized to have pixel values between 0 and 1, we use a :sigmoid activation in our dense layer to squeeze output values between 0 and 1. Our original input shape was 28x28, so we use Axon.reshape to convert the flattened representation of the outputs into an image with correct the width and height. If we just bind the encoder and decoder sequentially, we'll get the desired model. This was pretty smooth, wasn't it? encoder = fn x , latent_dim -&gt; x |&gt; Axon . flatten ( ) |&gt; Axon . dense ( latent_dim , activation : :relu ) end decoder = fn x -&gt; x |&gt; Axon . dense ( 784 , activation : :sigmoid ) |&gt; Axon . reshape ( { 1 , 28 , 28 } ) end model = Axon . input ( &quot;input&quot; , shape : { nil , 1 , 28 , 28 } ) |&gt; encoder . ( 64 ) |&gt; decoder . ( ) First we need to define the encoder and decoder. Both are one-layer neural networks. In the encoder, we start by flattening the input, so we get from shape {batch_size, 1, 28, 28} to {batch_size, 784} and we pass the input into a dense layer. Our dense layer has only latent_dim number of neurons. The latent_dim (or the latent space) is a compressed representation of data. Remember, we want our encoder to compress the input data into a lower-dimensional representation, so we choose a latent_dim which is less than the dimensionality of the input. encoder = fn x , latent_dim -&gt; x |&gt; Axon . flatten ( ) |&gt; Axon . dense ( latent_dim , activation : :relu ) end Next, we pass the output of the encoder to the decoder and try to reconstruct the compressed data into its original form. Since our original input had a dimensionality of 784, we use a dense layer with 784 neurons. Because our original data was normalized to have pixel values between 0 and 1, we use a :sigmoid activation in our dense layer to squeeze output values between 0 and 1. Our original input shape was 28x28, so we use Axon.reshape to convert the flattened representation of the outputs into an image with correct the width and height. decoder = fn x -&gt; x |&gt; Axon . dense ( 784 , activation : :sigmoid ) |&gt; Axon . reshape ( { :batch , 1 , 28 , 28 } ) end If we just bind the encoder and decoder sequentially, we'll get the desired model. This was pretty smooth, wasn't it? model = Axon . input ( &quot;input&quot; , shape : { nil , 1 , 28 , 28 } ) |&gt; encoder . ( 64 ) |&gt; decoder . ( )","ref":"fashionmnist_autoencoder.html#encoder-and-decoder","title":"Training an Autoencoder on Fashion MNIST - Encoder and decoder","type":"extras"},{"doc":"Finally, we can train the model. We'll use the :adam and :mean_squared_error loss with Axon.Loop.trainer . Our loss function will measure the aggregate error between pixels of original images and the model's reconstructed images. We'll also :mean_absolute_error using Axon.Loop.metric . Axon.Loop.run trains the model with the given training data. batch_size = 32 epochs = 5 batched_images = Nx . to_batched ( train_images , batch_size ) train_batches = Stream . zip ( batched_images , batched_images ) params = model |&gt; Axon.Loop . trainer ( :mean_squared_error , :adam ) |&gt; Axon.Loop . metric ( :mean_absolute_error , &quot;Error&quot; ) |&gt; Axon.Loop . run ( train_batches , %{ } , epochs : epochs , compiler : EXLA )","ref":"fashionmnist_autoencoder.html#training-the-model","title":"Training an Autoencoder on Fashion MNIST - Training the model","type":"extras"},{"doc":"To better understand what is mean absolute error (MAE) and mean square error (MSE) let's go through an example. # Error definitions for a single sample mean_square_error = fn y_pred , y -&gt; y_pred |&gt; Nx . subtract ( y ) |&gt; Nx . power ( 2 ) |&gt; Nx . mean ( ) end mean_absolute_erorr = fn y_pred , y -&gt; y_pred |&gt; Nx . subtract ( y ) |&gt; Nx . abs ( ) |&gt; Nx . mean ( ) end We will work with a sample image of a shoe, a slightly noised version of that image, and also an entirely different image from the dataset. shoe_image = train_images [ 0 ] noised_shoe_image = Nx . add ( shoe_image , Nx . random_normal ( shoe_image , 0.0 , 0.05 ) ) other_image = train_images [ 1 ] :ok For the same image both errors should be 0, because when we have two exact copies, there is no pixel difference. { mean_square_error . ( shoe_image , shoe_image ) , mean_absolute_erorr . ( shoe_image , shoe_image ) } Now the noised image: { mean_square_error . ( shoe_image , noised_shoe_image ) , mean_absolute_erorr . ( shoe_image , noised_shoe_image ) } And a different image: { mean_square_error . ( shoe_image , other_image ) , mean_absolute_erorr . ( shoe_image , other_image ) } As we can see, the noised image has a non-zero MSE and MAE but is much smaller than the error of two completely different pictures. In other words, both of these error types measure the level of similarity between images. A small error implies decent prediction values. On the other hand, a large error value suggests poor quality of predictions. If you look at our implementation of MAE and MSE, you will notice that they are very similar. MAE and MSE can also be called the $L_1$ and $L_2$ loss respectively for the $L_1$ and $L_2$ norm. The $L_2$ loss (MSE) is typically preferred because it's a smoother function whereas $L_1$ is often difficult to optimize with stochastic gradient descent (SGD).","ref":"fashionmnist_autoencoder.html#extra-losses","title":"Training an Autoencoder on Fashion MNIST - Extra: losses","type":"extras"},{"doc":"Now, let's see how our model is doing! We will compare a sample image before and after compression. sample_image = train_images [ 0 .. 0 // 1 ] compressed_image = Axon . predict ( model , params , sample_image , compiler : EXLA ) sample_image |&gt; Nx . to_heatmap ( ) |&gt; IO . inspect ( label : &quot;Original&quot; ) compressed_image |&gt; Nx . to_heatmap ( ) |&gt; IO . inspect ( label : &quot;Compressed&quot; ) :ok As we can see, the generated image is similar to the input image. The only difference between them is the absence of a sign in the middle of the second shoe. The model treated the sign as noise and bled this into the plain shoe.","ref":"fashionmnist_autoencoder.html#inference","title":"Training an Autoencoder on Fashion MNIST - Inference","type":"extras"},{"doc":"Mix . install ( [ { :exla , &quot;~&gt; 0.4.0&quot; } , { :nx , &quot;~&gt; 0.4.0&quot; , override : true } , { :axon , &quot;~&gt; 0.3.0&quot; } , { :req , &quot;~&gt; 0.3.1&quot; } , { :kino , &quot;~&gt; 0.7.0&quot; } , { :scidata , &quot;~&gt; 0.1.9&quot; } , { :stb_image , &quot;~&gt; 0.5.2&quot; } , { :kino_vega_lite , &quot;~&gt; 0.1.6&quot; } , { :vega_lite , &quot;~&gt; 0.1.6&quot; } , { :table_rex , &quot;~&gt; 3.1.1&quot; } ] ) alias VegaLite , as : Vl # This speeds up all our `Nx` operations without having to use `defn` Nx . global_default_backend ( EXLA.Backend ) :ok","ref":"fashionmnist_vae.html","title":"A Variational Autoencoder for MNIST","type":"extras"},{"doc":"In this notebook, we'll be building a variational autoencoder (VAE). This will help demonstrate splitting up models, defining custom layers and loss functions, using multiple outputs, and a few additional Kino tricks for training models. This notebook builds on the denoising autoencoder example and turns the simple autoencoder into a variational one for the same dataset.","ref":"fashionmnist_vae.html#introduction","title":"A Variational Autoencoder for MNIST - Introduction","type":"extras"},{"doc":"This section will proceed without much explanation as most of it is extracted from denoising autoencoder example . If anything here doesn't make sense, take a look at that notebook for an explanation. defmodule Data do @moduledoc &quot;&quot;&quot; A module to hold useful data processing utilities, mostly extracted from the previous notebook &quot;&quot;&quot; @doc &quot;&quot;&quot; Converts the given image into a `Kino.Image`. `image` must be a single channel `Nx` tensor with pixel values between 0 and 1. `height` and `width` are the output size in pixels &quot;&quot;&quot; def image_to_kino ( image , height \\\\ 200 , width \\\\ 200 ) do image |&gt; Nx . multiply ( 255 ) |&gt; Nx . as_type ( :u8 ) |&gt; Nx . transpose ( axes : [ :height , :width , :channels ] ) |&gt; StbImage . from_nx ( ) |&gt; StbImage . resize ( height , width ) |&gt; StbImage . to_binary ( :png ) |&gt; Kino.Image . new ( :png ) end @doc &quot;&quot;&quot; Converts image data from `Scidata.MNIST` into an `Nx` tensor and normalizes it. &quot;&quot;&quot; def preprocess_data ( data ) do { image_data , _labels } = data { images_binary , type , shape } = image_data images_binary |&gt; Nx . from_binary ( type ) # Since pixels are organized row-wise, reshape into rows x columns |&gt; Nx . reshape ( shape , names : [ :images , :channels , :height , :width ] ) # Normalize the pixel values to be between 0 and 1 |&gt; Nx . divide ( 255 ) end @doc &quot;&quot;&quot; Converts a tensor of images into random batches of paired images for model training &quot;&quot;&quot; def prepare_training_data ( images , batch_size ) do Stream . flat_map ( [ nil ] , fn nil -&gt; images |&gt; Nx . shuffle ( axis : :images ) |&gt; Nx . to_batched ( batch_size ) end ) |&gt; Stream . map ( fn batch -&gt; { batch , batch } end ) end end train_images = Data . preprocess_data ( Scidata.FashionMNIST . download ( ) ) test_images = Data . preprocess_data ( Scidata.FashionMNIST . download_test ( ) ) Kino . render ( train_images [ [ images : 0 ] ] |&gt; Data . image_to_kino ( ) ) Kino . render ( test_images [ [ images : 0 ] ] |&gt; Data . image_to_kino ( ) ) :ok Now for our simple autoencoder model. We won't be using a denoising autoencoder here. Note that we're giving each of the layers a name - the reason for this will be apparent later. I'm also using a small custom layer to shift and scale the output of the sigmoid layer slightly so it can hit the 0 and 1 targets. I noticed the gradients tend to explode without this. defmodule CustomLayer do import Nx.Defn def scaling_layer ( % Axon { } = input , _opts \\\\ [ ] ) do Axon . layer ( &amp; scaling_layer_impl / 2 , [ input ] ) end defnp scaling_layer_impl ( x , _opts \\\\ [ ] ) do x |&gt; Nx . subtract ( 0.05 ) |&gt; Nx . multiply ( 1.2 ) end end model = Axon . input ( &quot;image&quot; , shape : { nil , 1 , 28 , 28 } ) # This is now 28*28*1 = 784 |&gt; Axon . flatten ( ) # The encoder |&gt; Axon . dense ( 256 , activation : :relu , name : &quot;encoder_layer_1&quot; ) |&gt; Axon . dense ( 128 , activation : :relu , name : &quot;encoder_layer_2&quot; ) |&gt; Axon . dense ( 64 , activation : :relu , name : &quot;encoder_layer_3&quot; ) # Bottleneck layer |&gt; Axon . dense ( 10 , activation : :relu , name : &quot;bottleneck_layer&quot; ) # The decoder |&gt; Axon . dense ( 64 , activation : :relu , name : &quot;decoder_layer_1&quot; ) |&gt; Axon . dense ( 128 , activation : :relu , name : &quot;decoder_layer_2&quot; ) |&gt; Axon . dense ( 256 , activation : :relu , name : &quot;decoder_layer_3&quot; ) |&gt; Axon . dense ( 784 , activation : :sigmoid , name : &quot;decoder_layer_4&quot; ) |&gt; CustomLayer . scaling_layer ( ) # Turn it back into a 28x28 single channel image |&gt; Axon . reshape ( { :auto , 1 , 28 , 28 } ) # We can use Axon.Display to show us what each of the layers would look like # assuming we send in a batch of 4 images Axon.Display . as_table ( model , Nx . template ( { 4 , 1 , 28 , 28 } , :f32 ) ) |&gt; IO . puts ( ) batch_size = 128 train_data = Data . prepare_training_data ( train_images , 128 ) test_data = Data . prepare_training_data ( test_images , 128 ) { input_batch , target_batch } = Enum . at ( train_data , 0 ) Kino . render ( input_batch [ [ images : 0 ] ] |&gt; Data . image_to_kino ( ) ) Kino . render ( target_batch [ [ images : 0 ] ] |&gt; Data . image_to_kino ( ) ) :ok When training, it can be useful to stop execution early - either when you see it's failing and you don't want to waste time waiting for the remaining epochs to finish, or if it's good enough and you want to start experimenting with it. The kino_early_stop/1 function below is a handy handler to give us a Kino.Control.button that will stop the training loop when clicked. We also have plot_losses/1 function to visualize our train and validation losses using VegaLite . defmodule KinoAxon do @doc &quot;&quot;&quot; Adds handler function which adds a frame with a &quot;stop&quot; button to the cell with the training loop. Clicking &quot;stop&quot; will halt the training loop. &quot;&quot;&quot; def kino_early_stop ( loop ) do frame = Kino.Frame . new ( ) |&gt; Kino . render ( ) stop_button = Kino.Control . button ( &quot;stop&quot; ) Kino.Frame . render ( frame , stop_button ) { :ok , button_agent } = Agent . start_link ( fn -&gt; nil end ) stop_button |&gt; Kino.Control . stream ( ) |&gt; Kino . listen ( fn _event -&gt; Agent . update ( button_agent , fn _ -&gt; :stop end ) end ) handler = fn state -&gt; stop_state = Agent . get ( button_agent , &amp; &amp;1 ) if stop_state == :stop do Agent . stop ( button_agent ) Kino.Frame . render ( frame , &quot;stopped&quot; ) { :halt_loop , state } else { :continue , state } end end Axon.Loop . handle ( loop , :iteration_completed , handler ) end @doc &quot;&quot;&quot; Plots the training and validation losses using Kino and VegaLite. This *must* come after `Axon.Loop.validate`. &quot;&quot;&quot; def plot_losses ( loop ) do vl_widget = Vl . new ( width : 600 , height : 400 ) |&gt; Vl . mark ( :point , tooltip : true ) |&gt; Vl . encode_field ( :x , &quot;epoch&quot; , type : :ordinal ) |&gt; Vl . encode_field ( :y , &quot;loss&quot; , type : :quantitative ) |&gt; Vl . encode_field ( :color , &quot;dataset&quot; , type : :nominal ) |&gt; Kino.VegaLite . new ( ) |&gt; Kino . render ( ) handler = fn state -&gt; % Axon.Loop.State { metrics : metrics , epoch : epoch } = state loss = metrics [ &quot;loss&quot; ] |&gt; Nx . to_number ( ) val_loss = metrics [ &quot;validation_loss&quot; ] |&gt; Nx . to_number ( ) points = [ %{ epoch : epoch , loss : loss , dataset : &quot;train&quot; } , %{ epoch : epoch , loss : val_loss , dataset : &quot;validation&quot; } ] Kino.VegaLite . push_many ( vl_widget , points ) { :continue , state } end Axon.Loop . handle ( loop , :epoch_completed , handler ) end end # A helper function to display the input and output side by side combined_input_output = fn params , image_index -&gt; test_image = test_images [ [ images : image_index ] ] reconstructed_image = Axon . predict ( model , params , test_image ) |&gt; Nx . squeeze ( axes : [ 0 ] ) Nx . concatenate ( [ test_image , reconstructed_image ] , axis : :width ) end frame = Kino.Frame . new ( ) |&gt; Kino . render ( ) render_example_handler = fn state -&gt; # state.step_state[:model_state] contains the model params when this event is fired params = state . step_state [ :model_state ] image_index = Enum . random ( 0 .. ( Nx . axis_size ( test_images , :images ) - 1 ) ) image = combined_input_output . ( params , image_index ) |&gt; Data . image_to_kino ( 200 , 400 ) Kino.Frame . render ( frame , image ) Kino.Frame . append ( frame , &quot;Epoch: \#{ state . epoch } , Iteration: \#{ state . iteration } &quot; ) { :continue , state } end params = model |&gt; Axon.Loop . trainer ( :mean_squared_error , Axon.Optimizers . adamw ( 0.001 ) ) |&gt; KinoAxon . kino_early_stop ( ) |&gt; Axon.Loop . handle ( :iteration_completed , render_example_handler , every : 450 ) |&gt; Axon.Loop . validate ( model , test_data ) |&gt; KinoAxon . plot_losses ( ) |&gt; Axon.Loop . run ( train_data , %{ } , epochs : 40 , compiler : EXLA ) :ok","ref":"fashionmnist_vae.html#training-a-simple-autoencoder","title":"A Variational Autoencoder for MNIST - Training a simple autoencoder","type":"extras"},{"doc":"Cool! We now have the parameters for a trained, simple autoencoder. Our next step is to split up the model so we can use the encoder and decoder separately. By doing that, we'll be able to take an image and encode it to get the model's compressed image representation (the latent vector). We can then manipulate the latent vector and run the manipulated latent vector through the decoder to get a new image. Let's start by defining the encoder and decoder separately as two different models. encoder = Axon . input ( &quot;image&quot; , shape : { nil , 1 , 28 , 28 } ) # This is now 28*28*1 = 784 |&gt; Axon . flatten ( ) # The encoder |&gt; Axon . dense ( 256 , activation : :relu , name : &quot;encoder_layer_1&quot; ) |&gt; Axon . dense ( 128 , activation : :relu , name : &quot;encoder_layer_2&quot; ) |&gt; Axon . dense ( 64 , activation : :relu , name : &quot;encoder_layer_3&quot; ) # Bottleneck layer |&gt; Axon . dense ( 10 , activation : :relu , name : &quot;bottleneck_layer&quot; ) # The output from the encoder decoder = Axon . input ( &quot;latent&quot; , shape : { nil , 10 } ) # The decoder |&gt; Axon . dense ( 64 , activation : :relu , name : &quot;decoder_layer_1&quot; ) |&gt; Axon . dense ( 128 , activation : :relu , name : &quot;decoder_layer_2&quot; ) |&gt; Axon . dense ( 256 , activation : :relu , name : &quot;decoder_layer_3&quot; ) |&gt; Axon . dense ( 784 , activation : :sigmoid , name : &quot;decoder_layer_4&quot; ) |&gt; CustomLayer . scaling_layer ( ) # Turn it back into a 28x28 single channel image |&gt; Axon . reshape ( { :auto , 1 , 28 , 28 } ) Axon.Display . as_table ( encoder , Nx . template ( { 4 , 1 , 28 , 28 } , :f32 ) ) |&gt; IO . puts ( ) Axon.Display . as_table ( decoder , Nx . template ( { 4 , 10 } , :f32 ) ) |&gt; IO . puts ( ) We have the two models, but the problem is these are untrained models so we don't have the corresponding set of parameters. We'd like to use the parameters from the autoencoder we just trained and apply them to our split up models. Let's first take a look at what params actually are: params Params are just a Map with the layer name as the key identifying which parameters to use. We can easily match up the layer names with the output from the Axon.Display.as_table/2 call for the autoencoder model. So all we need to do is create a new Map that plucks out the right layers from our autoencoder params for each model and use that to run inference on our split up models. Fortunately, since we gave each of the layers names, this requires no work at all - we can use the Map as it is since the layer names match up! Axon will ignore any extra keys so those won't be a problem. Note that naming the layers wasn't required , if the layers didn't have names we would have some renaming to do to get the names to match between the models. But giving them names made it very convenient :) Let's try encoding an image, printing the latent and then decoding the latent using our split up model to make sure it's working. image = test_images [ [ images : 0 ] ] # Encode the image latent = Axon . predict ( encoder , params , image ) IO . inspect ( latent , label : &quot;Latent&quot; ) # Decode the image reconstructed_image = Axon . predict ( decoder , params , latent ) |&gt; Nx . squeeze ( axes : [ 0 ] ) combined_image = Nx . concatenate ( [ image , reconstructed_image ] , axis : :width ) Data . image_to_kino ( combined_image , 200 , 400 ) Perfect! Seems like the split up models are working as expected. Now let's try to generate some new images using our autoencoder. To do this, we'll manipulate the latent so it's slightly different from what the encoder gave us. Specifically, we'll try to interpolate between two images, showing 100 steps from our starting image to our final image. num_steps = 100 # Get our latents, image at index 0 is our starting point # index 1 is where we&#39;ll end latents = Axon . predict ( encoder , params , test_images [ [ images : 0 .. 1 ] ] ) # Latents is a {2, 10} tensor # The step we&#39;ll add to our latent to move it towards image[1] step = Nx . subtract ( latents [ 1 ] , latents [ 0 ] ) |&gt; Nx . divide ( num_steps ) # We can make a batch of all our new latents new_latents = Nx . multiply ( Nx . iota ( { num_steps + 1 , 1 } ) , step ) |&gt; Nx . add ( latents [ 0 ] ) reconstructed_images = Axon . predict ( decoder , params , new_latents ) reconstructed_images = Nx . reshape ( reconstructed_images , Nx . shape ( reconstructed_images ) , names : [ :images , :channels , :height , :width ] ) Stream . interval ( div ( 5000 , num_steps ) ) |&gt; Stream . take ( num_steps + 1 ) |&gt; Kino . animate ( fn i -&gt; Data . image_to_kino ( reconstructed_images [ i ] ) end ) Cool! We have interpolation! But did you notice that some of the intermediate frames don't look fashionable at all? Autoencoders don't generally return good results for random vectors in their latent space. That's where a VAE can help.","ref":"fashionmnist_vae.html#splitting-up-the-model","title":"A Variational Autoencoder for MNIST - Splitting up the model","type":"extras"},{"doc":"In a VAE, instead of outputting a latent vector, our encoder will output a distribution. Essentially this means instead of 10 outputs we'll have 20. 10 of them will represent the mean and 10 will represent the log of the variance of the latent. We'll have to sample from this distribution to get our latent vector. Finally, we'll have to modify our loss function to also compute the KL Divergence between the latent distribution and a standard normal distribution (this acts as a regularizer of the latent space). We'll start by defining our model: defmodule Vae do import Nx.Defn @latent_features 10 defp sampling_layer ( % Axon { } = input , _opts \\\\ [ ] ) do Axon . layer ( &amp; sampling_layer_impl / 2 , [ input ] , name : &quot;sampling_layer&quot; , op_name : :sample ) end defnp sampling_layer_impl ( x , _opts \\\\ [ ] ) do mu = x [ [ 0 .. - 1 // 1 , 0 , 0 .. - 1 // 1 ] ] log_var = x [ [ 0 .. - 1 // 1 , 1 , 0 .. - 1 // 1 ] ] std_dev = Nx . exp ( 0.5 * log_var ) eps = Nx . random_normal ( std_dev ) sample = mu + std_dev * eps Nx . stack ( [ sample , mu , std_dev ] , axis : 1 ) end defp encoder_partial ( ) do Axon . input ( &quot;image&quot; , shape : { nil , 1 , 28 , 28 } ) # This is now 28*28*1 = 784 |&gt; Axon . flatten ( ) # The encoder |&gt; Axon . dense ( 256 , activation : :relu , name : &quot;encoder_layer_1&quot; ) |&gt; Axon . dense ( 128 , activation : :relu , name : &quot;encoder_layer_2&quot; ) |&gt; Axon . dense ( 64 , activation : :relu , name : &quot;encoder_layer_3&quot; ) # Bottleneck layer |&gt; Axon . dense ( @latent_features * 2 , name : &quot;bottleneck_layer&quot; ) # Split up the mu and logvar |&gt; Axon . reshape ( { :auto , 2 , @latent_features } ) |&gt; sampling_layer ( ) end def encoder ( ) do encoder_partial ( ) # Grab only the sample (ie. the sampled latent) |&gt; Axon . nx ( fn x -&gt; x [ [ 0 .. - 1 // 1 , 0 ] ] end ) end def decoder ( input_latent ) do input_latent |&gt; Axon . dense ( 64 , activation : :relu , name : &quot;decoder_layer_1&quot; ) |&gt; Axon . dense ( 128 , activation : :relu , name : &quot;decoder_layer_2&quot; ) |&gt; Axon . dense ( 256 , activation : :relu , name : &quot;decoder_layer_3&quot; ) |&gt; Axon . dense ( 784 , activation : :sigmoid , name : &quot;decoder_layer_4&quot; ) |&gt; CustomLayer . scaling_layer ( ) # Turn it back into a 28x28 single channel image |&gt; Axon . reshape ( { :auto , 1 , 28 , 28 } ) end def autoencoder ( ) do encoder_partial = encoder_partial ( ) encoder = encoder ( ) autoencoder = decoder ( encoder ) Axon . container ( %{ mu_sigma : encoder_partial , reconstruction : autoencoder } ) end end There's a few interesting things going on here. First, since our model has become more complex, we've used a module to keep it organized. We also built a custom layer to do the sampling and output the sampled latent vector as well as the distribution parameters (mu and sigma). Finally, we need the distribution itself so we can calculate the KL Divergence in our loss function. To make the model output the distribution parameters (mu and sigma), we use Axon.container/1 to produce two outputs from our model instead of one. Now, instead of getting a tensor as an output, we'll get a map with the two tensors we need for our loss function. Our loss function also has to be modified so be the sum of the KL divergence and MSE. Here's our custom loss function: defmodule CustomLoss do import Nx.Defn defn loss ( y_true , %{ reconstruction : reconstruction , mu_sigma : mu_sigma } ) do mu = mu_sigma [ [ 0 .. - 1 // 1 , 1 , 0 .. - 1 // 1 ] ] sigma = mu_sigma [ [ 0 .. - 1 // 1 , 2 , 0 .. - 1 // 1 ] ] kld = Nx . sum ( - Nx . log ( sigma ) - 0.5 + Nx . multiply ( sigma , sigma ) + Nx . multiply ( mu , mu ) ) kld * 0.1 + Axon.Losses . mean_squared_error ( y_true , reconstruction , reduction : :sum ) end end With all our pieces ready, we can pretty much use the same training loop as we did earlier. The only modifications needed are to account for the fact that the model outputs a map with two values instead of a single tensor and telling the trainer to use our custom loss. model = Vae . autoencoder ( ) # A helper function to display the input and output side by side combined_input_output = fn params , image_index -&gt; test_image = test_images [ [ images : image_index ] ] %{ reconstruction : reconstructed_image } = Axon . predict ( model , params , test_image ) reconstructed_image = reconstructed_image |&gt; Nx . squeeze ( axes : [ 0 ] ) Nx . concatenate ( [ test_image , reconstructed_image ] , axis : :width ) end frame = Kino.Frame . new ( ) |&gt; Kino . render ( ) render_example_handler = fn state -&gt; # state.step_state[:model_state] contains the model params when this event is fired params = state . step_state [ :model_state ] image_index = Enum . random ( 0 .. ( Nx . axis_size ( test_images , :images ) - 1 ) ) image = combined_input_output . ( params , image_index ) |&gt; Data . image_to_kino ( 200 , 400 ) Kino.Frame . render ( frame , image ) Kino.Frame . append ( frame , &quot;Epoch: \#{ state . epoch } , Iteration: \#{ state . iteration } &quot; ) { :continue , state } end params = model |&gt; Axon.Loop . trainer ( &amp; CustomLoss . loss / 2 , Axon.Optimizers . adam ( 0.001 ) ) |&gt; KinoAxon . kino_early_stop ( ) |&gt; Axon.Loop . handle ( :epoch_completed , render_example_handler ) |&gt; Axon.Loop . validate ( model , test_data ) |&gt; KinoAxon . plot_losses ( ) |&gt; Axon.Loop . run ( train_data , %{ } , epochs : 40 , compiler : EXLA ) :ok Finally, we can try our interpolation again: num_steps = 100 # Get our latents, image at index 0 is our starting point # index 1 is where we&#39;ll end latents = Axon . predict ( Vae . encoder ( ) , params , test_images [ [ images : 0 .. 1 ] ] ) # Latents is a {2, 10} tensor # The step we&#39;ll add to our latent to move it towards image[1] step = Nx . subtract ( latents [ 1 ] , latents [ 0 ] ) |&gt; Nx . divide ( num_steps ) # We can make a batch of all our new latents new_latents = Nx . multiply ( Nx . iota ( { num_steps + 1 , 1 } ) , step ) |&gt; Nx . add ( latents [ 0 ] ) decoder = Axon . input ( &quot;latent&quot; , shape : { nil , 10 } ) |&gt; Vae . decoder ( ) reconstructed_images = Axon . predict ( decoder , params , new_latents ) reconstructed_images = Nx . reshape ( reconstructed_images , Nx . shape ( reconstructed_images ) , names : [ :images , :channels , :height , :width ] ) Stream . interval ( div ( 5000 , num_steps ) ) |&gt; Stream . take ( num_steps + 1 ) |&gt; Kino . animate ( fn i -&gt; Data . image_to_kino ( reconstructed_images [ i ] ) end ) Did you notice the difference? Every step in our interpolation looks similar to items in our dataset! This is the benefit of the VAE: we can generate new items by using random latents. In contrast, in the simple autoencoder, for the most part only latents we got from our encoder were likely to produce sensible outputs.","ref":"fashionmnist_vae.html#making-it-variational","title":"A Variational Autoencoder for MNIST - Making it variational","type":"extras"}]