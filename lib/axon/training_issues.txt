Training wins:

- mixed precision seems to actually have a memory impact! (wooooot!)

Training Issues:

- no keyword validate!
- gradient accumulation steps causes OOMs in much smaller batch sizes, this is likely because it makes the computation larger, I think maybe we can accomplish this with an Nx while loop rather than the current approach which exponentially increases the computation size
- validation loops always recompile
- weird recompilation at second-to-last batch step (likely change in batch size, maybe we can warn?)
- does not respect seeds
- filters dont make much sense as they are global filters not per-event filters