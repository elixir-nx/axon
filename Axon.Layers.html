<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="ExDoc v0.31.1">
    <meta name="project" content="Axon v0.6.1">


    <title>Axon.Layers â€” Axon v0.6.1</title>
    <link rel="stylesheet" href="dist/html-elixir-FM2CSD74.css" />


    <script src="dist/handlebars.runtime-NWIB6V2M.js"></script>
    <script src="dist/handlebars.templates-43PMFBC7.js"></script>
    <script src="dist/sidebar_items-02629AA9.js"></script>

      <script src="docs_config.js"></script>

    <script async src="dist/html-L4O5OK2K.js"></script>


  </head>
  <body data-type="modules" class="page-module">
    <script>

      try {
        var settings = JSON.parse(localStorage.getItem('ex_doc:settings') || '{}');

        if (settings.theme === 'dark' ||
           ((settings.theme === 'system' || settings.theme == null) &&
             window.matchMedia('(prefers-color-scheme: dark)').matches)
           ) {
          document.body.classList.add('dark')
        }
      } catch (error) { }
    </script>

<div class="main">

<button id="sidebar-menu" class="sidebar-button sidebar-toggle" aria-label="toggle sidebar" aria-controls="sidebar">
  <i class="ri-menu-line ri-lg" title="Collapse/expand sidebar"></i>
</button>

<div class="background-layer"></div>

<nav id="sidebar" class="sidebar">

  <div class="sidebar-header">
    <div class="sidebar-projectInfo">

        <a href="Axon.html" class="sidebar-projectImage">
          <img src="assets/logo.png" alt="Axon" />
        </a>

      <div>
        <a href="Axon.html" class="sidebar-projectName" translate="no">
Axon
        </a>
        <div class="sidebar-projectVersion" translate="no">
          v0.6.1
        </div>
      </div>
    </div>
    <ul id="sidebar-listNav" class="sidebar-listNav" role="tablist">
      <li>
        <button id="extras-list-tab-button" role="tab" data-type="extras" aria-controls="extras-tab-panel" aria-selected="true" tabindex="0">
Pages
        </button>
      </li>

        <li>
          <button id="modules-list-tab-button" role="tab" data-type="modules" aria-controls="modules-tab-panel" aria-selected="false" tabindex="-1">
            Modules
          </button>
        </li>


    </ul>
  </div>

  <div id="extras-tab-panel" class="sidebar-tabpanel" role="tabpanel" aria-labelledby="extras-list-tab-button">
    <ul id="extras-full-list" class="full-list"></ul>
  </div>

    <div id="modules-tab-panel" class="sidebar-tabpanel" role="tabpanel" aria-labelledby="modules-list-tab-button" hidden>
      <ul id="modules-full-list" class="full-list"></ul>
    </div>


</nav>

<main class="content">
  <output role="status" id="toast"></output>
  <div class="content-outer">
    <div id="content" class="content-inner">
      <div class="top-search">
        <div class="search-settings">
          <form class="search-bar" action="search.html">
            <label class="search-label">
              <span class="sr-only">Search documentation of Axon</span>
              <input name="q" type="text" class="search-input" placeholder="Search Documentation (press /)" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" />
            </label>
            <button type="submit" class="search-button" aria-label="Submit Search">
              <i class="ri-search-2-line ri-lg" aria-hidden="true" title="Submit search"></i>
            </button>
            <button type="button" tabindex="-1" class="search-close-button" aria-hidden="true">
              <i class="ri-close-line ri-lg" title="Cancel search"></i>
            </button>
          </form>
          <div class="autocomplete">
          </div>
          <button class="icon-settings display-settings">
            <i class="ri-settings-3-line"></i>
            <span class="sr-only">Settings</span>
          </button>
        </div>

      </div>

<h1>

    <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1" title="View Source" class="icon-action" rel="help">
      <i class="ri-code-s-slash-line" aria-hidden="true"></i>
      <span class="sr-only">View Source</span>
    </a>

  <span translate="no">Axon.Layers</span> 
  <small class="app-vsn" translate="no">(Axon v0.6.1)</small>

</h1>


  <section id="moduledoc">
<p>Functional implementations of common neural network layer
operations.</p><p>Layers are the building blocks of neural networks. These
functional implementations can be used to express higher-level
constructs using fundamental building blocks. Neural network
layers are stateful with respect to their parameters.
These implementations do not assume the responsibility of
managing state - instead opting to delegate this responsibility
to the caller.</p><p>Basic neural networks can be seen as a composition of functions:</p><pre><code class="makeup elixir" translate="no"><span class="n">input</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="n">dense</span><span class="p" data-group-id="0420595481-1">(</span><span class="n">w1</span><span class="p">,</span><span class="w"> </span><span class="n">b1</span><span class="p" data-group-id="0420595481-1">)</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="n">relu</span><span class="p" data-group-id="0420595481-2">(</span><span class="p" data-group-id="0420595481-2">)</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="n">dense</span><span class="p" data-group-id="0420595481-3">(</span><span class="n">w2</span><span class="p">,</span><span class="w"> </span><span class="n">b2</span><span class="p" data-group-id="0420595481-3">)</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="n">softmax</span><span class="p" data-group-id="0420595481-4">(</span><span class="p" data-group-id="0420595481-4">)</span></code></pre><p>These kinds of models are often referred to as deep feedforward networks
or multilayer perceptrons (MLPs) because information flows forward
through the network with no feedback connections. Mathematically,
a feedforward network can be represented as:</p><p>  $$
f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))
$$</p><p>You can see a similar pattern emerge if we condense the call stack
in the previous example:</p><pre><code class="makeup elixir" translate="no"><span class="n">softmax</span><span class="p" data-group-id="9116928075-1">(</span><span class="n">dense</span><span class="p" data-group-id="9116928075-2">(</span><span class="n">relu</span><span class="p" data-group-id="9116928075-3">(</span><span class="n">dense</span><span class="p" data-group-id="9116928075-4">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">w1</span><span class="p">,</span><span class="w"> </span><span class="n">b1</span><span class="p" data-group-id="9116928075-4">)</span><span class="p" data-group-id="9116928075-3">)</span><span class="p">,</span><span class="w"> </span><span class="n">w2</span><span class="p">,</span><span class="w"> </span><span class="n">b2</span><span class="p" data-group-id="9116928075-2">)</span><span class="p" data-group-id="9116928075-1">)</span></code></pre><p>The chain structure shown here is the most common structure used
in neural networks. You can consider each function $f^{(n)}$ as a
<em>layer</em> in the neural network - for example $f^{(2)} is the 2nd
layer in the network. The number of function calls in the
structure is the <em>depth</em> of the network. This is where the term
<em>deep learning</em> comes from.</p><p>Neural networks are often written as the mapping:</p><p>  $$
y = f(x; \theta)
$$</p><p>Where $x$ is the input to the neural network and $\theta$ are the
set of learned parameters. In Elixir, you would write this:</p><pre><code class="makeup elixir" translate="no"><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p" data-group-id="2636840203-1">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">params</span><span class="p" data-group-id="2636840203-1">)</span></code></pre><p>From the previous example, <code class="inline">params</code> would represent the collection:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="9994663263-1">{</span><span class="n">w1</span><span class="p">,</span><span class="w"> </span><span class="n">b1</span><span class="p">,</span><span class="w"> </span><span class="n">w2</span><span class="p">,</span><span class="w"> </span><span class="n">b2</span><span class="p" data-group-id="9994663263-1">}</span></code></pre><p>where <code class="inline">w1</code> and <code class="inline">w2</code> are layer <em>kernels</em>, and <code class="inline">b1</code> and <code class="inline">b2</code> are layer
<em>biases</em>.</p>
  </section>


  <section id="summary" class="details-list">
    <h1 class="section-heading">
      <a class="hover-link" href="#summary">
        <i class="ri-link-m" aria-hidden="true"></i>
      </a>
      <span class="text">Summary</span>
    </h1>
<div class="summary-layers-linear summary">
  <h2>
    <a href="#layers-linear">Layers: Linear</a>
  </h2>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#bilinear/5" translate="no">bilinear(input1, input2, kernel, bias \\ 0, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a bilinear layer.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#dense/4" translate="no">dense(input, kernel, bias \\ 0, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a dense layer.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#embedding/3" translate="no">embedding(input, kernel, arg3 \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Computes embedding by treating kernel matrix as a lookup table
for discrete tokens.</p></div>

    </div>

</div>
<div class="summary-layers-dropout summary">
  <h2>
    <a href="#layers-dropout">Layers: Dropout</a>
  </h2>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#alpha_dropout/3" translate="no">alpha_dropout(input, key, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of an alpha dropout layer.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#dropout/3" translate="no">dropout(input, key, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a dropout layer.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#feature_alpha_dropout/3" translate="no">feature_alpha_dropout(input, key, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a feature alpha dropout layer.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#spatial_dropout/3" translate="no">spatial_dropout(input, key, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of an n-dimensional spatial
dropout layer.</p></div>

    </div>

</div>
<div class="summary-layers-pooling summary">
  <h2>
    <a href="#layers-pooling">Layers: Pooling</a>
  </h2>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#adaptive_avg_pool/2" translate="no">adaptive_avg_pool(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of general dimensional adaptive average
pooling.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#adaptive_lp_pool/2" translate="no">adaptive_lp_pool(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of general dimensional adaptive power
average pooling.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#adaptive_max_pool/2" translate="no">adaptive_max_pool(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of general dimensional adaptive max
pooling.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#avg_pool/2" translate="no">avg_pool(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>A general dimensional functional average pooling layer.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#blur_pool/2" translate="no">blur_pool(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a 2-dimensional blur pooling layer.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#global_avg_pool/2" translate="no">global_avg_pool(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of global average pooling which averages across
the spatial dimensions of the input such that the only remaining dimensions
are the batch and feature dimensions.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#global_lp_pool/2" translate="no">global_lp_pool(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of global LP pooling which computes the following
function across spatial dimensions of the input</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#global_max_pool/2" translate="no">global_max_pool(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of global max pooling which computes maximums across
the spatial dimensions of the input such that the only remaining dimensions are
the batch and feature dimensions.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#lp_pool/2" translate="no">lp_pool(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a general dimensional power average
pooling layer.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#max_pool/2" translate="no">max_pool(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a general dimensional max pooling layer.</p></div>

    </div>

</div>
<div class="summary-layers-normalization summary">
  <h2>
    <a href="#layers-normalization">Layers: Normalization</a>
  </h2>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#batch_norm/6" translate="no">batch_norm(input, gamma, beta, ra_mean, ra_var, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of batch normalization.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#group_norm/4" translate="no">group_norm(input, gamma, beta, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of group normalization.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#instance_norm/6" translate="no">instance_norm(input, gamma, beta, ra_mean, ra_var, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of instance normalization.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#layer_norm/4" translate="no">layer_norm(input, gamma, beta, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of layer normalization.</p></div>

    </div>

</div>
<div class="summary-layers-shape summary">
  <h2>
    <a href="#layers-shape">Layers: Shape</a>
  </h2>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#flatten/2" translate="no">flatten(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Flattens input to shape of <code class="inline">{batch, units}</code> by folding outer
dimensions.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#resize/2" translate="no">resize(input, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Resizes a batch of tensors to the given shape using one of a
number of sampling methods.</p></div>

    </div>

</div>
<div class="summary-functions-convolutional summary">
  <h2>
    <a href="#functions-convolutional">Functions: Convolutional</a>
  </h2>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#conv/4" translate="no">conv(input, kernel, bias \\ 0, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a general dimensional convolutional
layer.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#conv_transpose/4" translate="no">conv_transpose(input, kernel, bias \\ 0, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a general dimensional transposed
convolutional layer.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#depthwise_conv/4" translate="no">depthwise_conv(inputs, kernel, bias \\ 0, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a general dimensional depthwise
convolution.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#separable_conv2d/6" translate="no">separable_conv2d(input, k1, b1, k2, b2, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a 2-dimensional separable depthwise
convolution.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#separable_conv3d/8" translate="no">separable_conv3d(input, k1, b1, k2, b2, k3, b3, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>Functional implementation of a 3-dimensional separable depthwise
convolution.</p></div>

    </div>

</div>
<div class="summary-functions summary">
  <h2>
    <a href="#functions">Functions</a>
  </h2>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#celu/2" translate="no">celu(input, opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#conv_lstm/7" translate="no">conv_lstm(input, hidden_state, mask, input_kernel, hidden_kernel, bias \\ [], opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#conv_lstm_cell/7" translate="no">conv_lstm_cell(input, carry, arg3, ih, hh, bi, opts \\ [])</a>

      </div>

        <div class="summary-synopsis"><p>ConvLSTM Cell.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#dynamic_unroll/7" translate="no">dynamic_unroll(cell_fn, input_sequence, carry, mask, input_kernel, recurrent_kernel, bias)</a>

      </div>

        <div class="summary-synopsis"><p>Dynamically unrolls an RNN.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#elu/2" translate="no">elu(input, opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#gru/7" translate="no">gru(input, hidden_state, mask, input_kernel, hidden_kernel, bias \\ [], opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#gru_cell/8" translate="no">gru_cell(input, carry, mask, arg4, arg5, arg6, gate_fn \\ &amp;Axon.Activations.sigmoid/1, activation_fn \\ &amp;Axon.Activations.tanh/1)</a>

      </div>

        <div class="summary-synopsis"><p>GRU Cell.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#hard_sigmoid/2" translate="no">hard_sigmoid(input, opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#hard_silu/2" translate="no">hard_silu(input, opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#leaky_relu/2" translate="no">leaky_relu(input, opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#log_softmax/2" translate="no">log_softmax(input, opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#log_sumexp/2" translate="no">log_sumexp(input, opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#lstm/7" translate="no">lstm(input, hidden_state, mask, input_kernel, hidden_kernel, bias \\ [], opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#lstm_cell/8" translate="no">lstm_cell(input, carry, mask, arg4, arg5, arg6, gate_fn \\ &amp;Axon.Activations.sigmoid/1, activation_fn \\ &amp;Axon.Activations.tanh/1)</a>

      </div>

        <div class="summary-synopsis"><p>LSTM Cell.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#multiply/2" translate="no">multiply(inputs, opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#padding_config_transform/2" translate="no">padding_config_transform(config, channels)</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#selu/2" translate="no">selu(input, opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#softmax/2" translate="no">softmax(input, opts \\ [])</a>

      </div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#static_unroll/7" translate="no">static_unroll(cell_fn, input_sequence, carry, mask, input_kernel, recurrent_kernel, bias)</a>

      </div>

        <div class="summary-synopsis"><p>Statically unrolls an RNN.</p></div>

    </div>

    <div class="summary-row">
      <div class="summary-signature">
        <a href="#subtract/2" translate="no">subtract(inputs, opts \\ [])</a>

      </div>

    </div>

</div>

  </section>


  <section id="layers-linear" class="details-list">
    <h1 class="section-heading">
      <a class="hover-link" href="#layers-linear">
        <i class="ri-link-m" aria-hidden="true"></i>
      </a>
      <span class="text">Layers: Linear</span>
    </h1>
    <div class="layers-linear-list">
<section class="detail" id="bilinear/5">

    <span id="bilinear/3"></span>

    <span id="bilinear/4"></span>

  <div class="detail-header">
    <a href="#bilinear/5" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">bilinear(input1, input2, kernel, bias \\ 0, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L166" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a bilinear layer.</p><p>Bilinear transformation of the input such that:</p><p>$$
y = x_1^{T}Ax_2 + b
$$</p><h2 id="bilinear/5-parameter-shapes" class="section-heading">
  <a href="#bilinear/5-parameter-shapes" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Parameter Shapes</span>
</h2>
<ul><li><code class="inline">input1</code> - <code class="inline">{batch_size, ..., input1_features}</code></li><li><code class="inline">input2</code> - <code class="inline">{batch_size, ..., input2_features}</code></li><li><code class="inline">kernel</code> - <code class="inline">{out_features, input1_features, input2_features}</code></li></ul><h2 id="bilinear/5-output-shape" class="section-heading">
  <a href="#bilinear/5-output-shape" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Output Shape</span>
</h2>
<p>  <code class="inline">{batch_size, ..., output_features}</code></p><h2 id="bilinear/5-examples" class="section-heading">
  <a href="#bilinear/5-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="n">inp1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="3987318030-1">(</span><span class="p" data-group-id="3987318030-2">{</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p" data-group-id="3987318030-2">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="3987318030-3">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="3987318030-3">}</span><span class="p" data-group-id="3987318030-1">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">inp2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="3987318030-4">(</span><span class="p" data-group-id="3987318030-5">{</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p" data-group-id="3987318030-5">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="3987318030-6">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="3987318030-6">}</span><span class="p" data-group-id="3987318030-4">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="3987318030-7">(</span><span class="p" data-group-id="3987318030-8">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p" data-group-id="3987318030-8">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="3987318030-9">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="3987318030-9">}</span><span class="p" data-group-id="3987318030-7">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="3987318030-10">(</span><span class="mf">1.0</span><span class="p" data-group-id="3987318030-10">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">bilinear</span><span class="p" data-group-id="3987318030-11">(</span><span class="n">inp1</span><span class="p">,</span><span class="w"> </span><span class="n">inp2</span><span class="p">,</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p" data-group-id="3987318030-11">)</span><span class="w">
</span><span class="p" data-group-id="3987318030-12">#</span><span class="nc" data-group-id="3987318030-12">Nx.Tensor</span><span class="p" data-group-id="3987318030-12">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="3987318030-13">[</span><span class="mi">3</span><span class="p" data-group-id="3987318030-13">]</span><span class="p" data-group-id="3987318030-14">[</span><span class="mi">1</span><span class="p" data-group-id="3987318030-14">]</span><span class="w">
  </span><span class="p" data-group-id="3987318030-15">[</span><span class="w">
    </span><span class="p" data-group-id="3987318030-16">[</span><span class="mf">39.0</span><span class="p" data-group-id="3987318030-16">]</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="3987318030-17">[</span><span class="mf">455.0</span><span class="p" data-group-id="3987318030-17">]</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="3987318030-18">[</span><span class="mf">1319.0</span><span class="p" data-group-id="3987318030-18">]</span><span class="w">
  </span><span class="p" data-group-id="3987318030-15">]</span><span class="w">
</span><span class="p" data-group-id="3987318030-12">&gt;</span></code></pre>
  </section>
</section>
<section class="detail" id="dense/4">

    <span id="dense/2"></span>

    <span id="dense/3"></span>

  <div class="detail-header">
    <a href="#dense/4" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">dense(input, kernel, bias \\ 0, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L105" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a dense layer.</p><p>Linear transformation of the input such that:</p><p>$$
y = xW^T + b
$$</p><p>A dense layer or fully connected layer transforms
the input using the given kernel matrix and bias
to compute:</p><pre><code class="makeup elixir" translate="no"><span class="nc">Nx</span><span class="o">.</span><span class="n">dot</span><span class="p" data-group-id="3284789772-1">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">kernel</span><span class="p" data-group-id="3284789772-1">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">bias</span></code></pre><p>Typically, both <code class="inline">kernel</code> and <code class="inline">bias</code> are learnable
parameters trained using gradient-based optimization.</p><h2 id="dense/4-parameter-shapes" class="section-heading">
  <a href="#dense/4-parameter-shapes" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Parameter Shapes</span>
</h2>
<ul><li><code class="inline">input</code> - <code class="inline">{batch_size, * input_features}</code></li><li><code class="inline">kernel</code> - <code class="inline">{input_features, output_features}</code></li><li><code class="inline">bias</code> - <code class="inline">{}</code> or <code class="inline">{output_features}</code></li></ul><h2 id="dense/4-output-shape" class="section-heading">
  <a href="#dense/4-output-shape" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Output Shape</span>
</h2>
<p>  <code class="inline">{batch_size, *, output_features}</code></p><h2 id="dense/4-examples" class="section-heading">
  <a href="#dense/4-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="1914216956-1">(</span><span class="p" data-group-id="1914216956-2">[</span><span class="p" data-group-id="1914216956-3">[</span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5</span><span class="p" data-group-id="1914216956-3">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="1914216956-4">[</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p" data-group-id="1914216956-4">]</span><span class="p" data-group-id="1914216956-2">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="1914216956-5">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="1914216956-5">}</span><span class="p" data-group-id="1914216956-1">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="1914216956-6">(</span><span class="p" data-group-id="1914216956-7">[</span><span class="p" data-group-id="1914216956-8">[</span><span class="mf">0.2</span><span class="p" data-group-id="1914216956-8">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="1914216956-9">[</span><span class="mf">0.3</span><span class="p" data-group-id="1914216956-9">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="1914216956-10">[</span><span class="mf">0.5</span><span class="p" data-group-id="1914216956-10">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="1914216956-11">[</span><span class="mf">0.8</span><span class="p" data-group-id="1914216956-11">]</span><span class="p" data-group-id="1914216956-7">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="1914216956-12">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="1914216956-12">}</span><span class="p" data-group-id="1914216956-6">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="1914216956-13">(</span><span class="p" data-group-id="1914216956-14">[</span><span class="mf">1.0</span><span class="p" data-group-id="1914216956-14">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="1914216956-15">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="1914216956-15">}</span><span class="p" data-group-id="1914216956-13">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">dense</span><span class="p" data-group-id="1914216956-16">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p" data-group-id="1914216956-16">)</span><span class="w">
</span><span class="p" data-group-id="1914216956-17">#</span><span class="nc" data-group-id="1914216956-17">Nx.Tensor</span><span class="p" data-group-id="1914216956-17">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="1914216956-18">[</span><span class="mi">2</span><span class="p" data-group-id="1914216956-18">]</span><span class="p" data-group-id="1914216956-19">[</span><span class="mi">1</span><span class="p" data-group-id="1914216956-19">]</span><span class="w">
  </span><span class="p" data-group-id="1914216956-20">[</span><span class="w">
    </span><span class="p" data-group-id="1914216956-21">[</span><span class="mf">2.25</span><span class="p" data-group-id="1914216956-21">]</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="1914216956-22">[</span><span class="mf">1.0</span><span class="p" data-group-id="1914216956-22">]</span><span class="w">
  </span><span class="p" data-group-id="1914216956-20">]</span><span class="w">
</span><span class="p" data-group-id="1914216956-17">&gt;</span></code></pre>
  </section>
</section>
<section class="detail" id="embedding/3">

    <span id="embedding/2"></span>

  <div class="detail-header">
    <a href="#embedding/3" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">embedding(input, kernel, arg3 \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1796" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Computes embedding by treating kernel matrix as a lookup table
for discrete tokens.</p><p><code class="inline">input</code> is a vector of discrete values, typically representing tokens
(e.g. words, characters, etc.) from a vocabulary. <code class="inline">kernel</code> is a kernel
matrix of shape <code class="inline">{vocab_size, embedding_size}</code> from which the dense
embeddings will be drawn.</p><h2 id="embedding/3-parameter-shapes" class="section-heading">
  <a href="#embedding/3-parameter-shapes" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Parameter Shapes</span>
</h2>
<ul><li><code class="inline">input</code> - <code class="inline">{batch_size, ..., seq_len}</code></li><li><code class="inline">kernel</code> - <code class="inline">{vocab_size, embedding_size}</code></li></ul><h2 id="embedding/3-examples" class="section-heading">
  <a href="#embedding/3-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="8030697786-1">(</span><span class="p" data-group-id="8030697786-2">[</span><span class="p" data-group-id="8030697786-3">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p" data-group-id="8030697786-3">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="8030697786-4">[</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">9</span><span class="p" data-group-id="8030697786-4">]</span><span class="p" data-group-id="8030697786-2">]</span><span class="p" data-group-id="8030697786-1">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">kernels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="8030697786-5">(</span><span class="p" data-group-id="8030697786-6">[</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="8030697786-7">[</span><span class="mf">0.46299999952316284</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5562999844551086</span><span class="p">,</span><span class="w"> </span><span class="mf">0.18170000612735748</span><span class="p" data-group-id="8030697786-7">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="8030697786-8">[</span><span class="mf">0.9801999926567078</span><span class="p">,</span><span class="w"> </span><span class="mf">0.09780000150203705</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5333999991416931</span><span class="p" data-group-id="8030697786-8">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="8030697786-9">[</span><span class="mf">0.6980000138282776</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9240999817848206</span><span class="p">,</span><span class="w"> </span><span class="mf">0.23479999601840973</span><span class="p" data-group-id="8030697786-9">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="8030697786-10">[</span><span class="mf">0.31929999589920044</span><span class="p">,</span><span class="w"> </span><span class="mf">0.42250001430511475</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7865999937057495</span><span class="p" data-group-id="8030697786-10">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="8030697786-11">[</span><span class="mf">0.5519000291824341</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5662999749183655</span><span class="p">,</span><span class="w"> </span><span class="mf">0.20559999346733093</span><span class="p" data-group-id="8030697786-11">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="8030697786-12">[</span><span class="mf">0.1898999959230423</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9311000108718872</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8356000185012817</span><span class="p" data-group-id="8030697786-12">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="8030697786-13">[</span><span class="mf">0.6383000016212463</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8794000148773193</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5282999873161316</span><span class="p" data-group-id="8030697786-13">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="8030697786-14">[</span><span class="mf">0.9523000121116638</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7597000002861023</span><span class="p">,</span><span class="w"> </span><span class="mf">0.08250000327825546</span><span class="p" data-group-id="8030697786-14">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="8030697786-15">[</span><span class="mf">0.6622999906539917</span><span class="p">,</span><span class="w"> </span><span class="mf">0.02329999953508377</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8205999732017517</span><span class="p" data-group-id="8030697786-15">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="8030697786-16">[</span><span class="mf">0.9855999946594238</span><span class="p">,</span><span class="w"> </span><span class="mf">0.36419999599456787</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5372999906539917</span><span class="p" data-group-id="8030697786-16">]</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="p" data-group-id="8030697786-6">]</span><span class="p" data-group-id="8030697786-5">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">embedding</span><span class="p" data-group-id="8030697786-17">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">kernels</span><span class="p" data-group-id="8030697786-17">)</span><span class="w">
</span><span class="p" data-group-id="8030697786-18">#</span><span class="nc" data-group-id="8030697786-18">Nx.Tensor</span><span class="p" data-group-id="8030697786-18">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="8030697786-19">[</span><span class="mi">2</span><span class="p" data-group-id="8030697786-19">]</span><span class="p" data-group-id="8030697786-20">[</span><span class="mi">4</span><span class="p" data-group-id="8030697786-20">]</span><span class="p" data-group-id="8030697786-21">[</span><span class="mi">3</span><span class="p" data-group-id="8030697786-21">]</span><span class="w">
  </span><span class="p" data-group-id="8030697786-22">[</span><span class="w">
    </span><span class="p" data-group-id="8030697786-23">[</span><span class="w">
      </span><span class="p" data-group-id="8030697786-24">[</span><span class="mf">0.9801999926567078</span><span class="p">,</span><span class="w"> </span><span class="mf">0.09780000150203705</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5333999991416931</span><span class="p" data-group-id="8030697786-24">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="8030697786-25">[</span><span class="mf">0.6980000138282776</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9240999817848206</span><span class="p">,</span><span class="w"> </span><span class="mf">0.23479999601840973</span><span class="p" data-group-id="8030697786-25">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="8030697786-26">[</span><span class="mf">0.5519000291824341</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5662999749183655</span><span class="p">,</span><span class="w"> </span><span class="mf">0.20559999346733093</span><span class="p" data-group-id="8030697786-26">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="8030697786-27">[</span><span class="mf">0.1898999959230423</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9311000108718872</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8356000185012817</span><span class="p" data-group-id="8030697786-27">]</span><span class="w">
    </span><span class="p" data-group-id="8030697786-23">]</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="8030697786-28">[</span><span class="w">
      </span><span class="p" data-group-id="8030697786-29">[</span><span class="mf">0.5519000291824341</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5662999749183655</span><span class="p">,</span><span class="w"> </span><span class="mf">0.20559999346733093</span><span class="p" data-group-id="8030697786-29">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="8030697786-30">[</span><span class="mf">0.31929999589920044</span><span class="p">,</span><span class="w"> </span><span class="mf">0.42250001430511475</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7865999937057495</span><span class="p" data-group-id="8030697786-30">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="8030697786-31">[</span><span class="mf">0.6980000138282776</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9240999817848206</span><span class="p">,</span><span class="w"> </span><span class="mf">0.23479999601840973</span><span class="p" data-group-id="8030697786-31">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="8030697786-32">[</span><span class="mf">0.9855999946594238</span><span class="p">,</span><span class="w"> </span><span class="mf">0.36419999599456787</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5372999906539917</span><span class="p" data-group-id="8030697786-32">]</span><span class="w">
    </span><span class="p" data-group-id="8030697786-28">]</span><span class="w">
  </span><span class="p" data-group-id="8030697786-22">]</span><span class="w">
</span><span class="p" data-group-id="8030697786-18">&gt;</span></code></pre>
  </section>
</section>

    </div>
  </section>

  <section id="layers-dropout" class="details-list">
    <h1 class="section-heading">
      <a class="hover-link" href="#layers-dropout">
        <i class="ri-link-m" aria-hidden="true"></i>
      </a>
      <span class="text">Layers: Dropout</span>
    </h1>
    <div class="layers-dropout-list">
<section class="detail" id="alpha_dropout/3">

    <span id="alpha_dropout/2"></span>

  <div class="detail-header">
    <a href="#alpha_dropout/3" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">alpha_dropout(input, key, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1504" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of an alpha dropout layer.</p><p>Alpha dropout is a type of dropout that forces the input
to have zero mean and unit standard deviation. Randomly
masks some elements and scales to enforce self-normalization.</p><h2 id="alpha_dropout/3-options" class="section-heading">
  <a href="#alpha_dropout/3-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:rate</code> - dropout rate. Used to determine probability a connection
will be dropped. Required.</p></li><li><p><code class="inline">:noise_shape</code> - input noise shape. Shape of <code class="inline">mask</code> which can be useful
for broadcasting <code class="inline">mask</code> across feature channels or other dimensions.
Defaults to shape of input tensor.</p></li></ul><h2 id="alpha_dropout/3-references" class="section-heading">
  <a href="#alpha_dropout/3-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a></li></ul>
  </section>
</section>
<section class="detail" id="dropout/3">

    <span id="dropout/2"></span>

  <div class="detail-header">
    <a href="#dropout/3" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">dropout(input, key, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1421" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a dropout layer.</p><p>Applies a mask to some elements of the input tensor with probability
<code class="inline">rate</code> and scales the input tensor by a factor of $\frac{1}{1 - rate}$.</p><p>Dropout is a form of regularization that helps prevent overfitting
by preventing models from becoming too reliant on certain connections.
Dropout can somewhat be thought of as learning an ensemble of models
with random connections masked.</p><h2 id="dropout/3-options" class="section-heading">
  <a href="#dropout/3-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:rate</code> - dropout rate. Used to determine probability a connection
will be dropped. Required.</p></li><li><p><code class="inline">:noise_shape</code> - input noise shape. Shape of <code class="inline">mask</code> which can be useful
for broadcasting <code class="inline">mask</code> across feature channels or other dimensions.
Defaults to shape of input tensor.</p></li></ul><h2 id="dropout/3-references" class="section-heading">
  <a href="#dropout/3-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="https://jmlr.org/papers/v15/srivastava14a.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li></ul>
  </section>
</section>
<section class="detail" id="feature_alpha_dropout/3">

    <span id="feature_alpha_dropout/2"></span>

  <div class="detail-header">
    <a href="#feature_alpha_dropout/3" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">feature_alpha_dropout(input, key, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1544" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a feature alpha dropout layer.</p><p>Feature alpha dropout applies dropout in the same manner as
spatial dropout; however, it also enforces self-normalization
by masking inputs with the SELU activation function and scaling
unmasked inputs.</p><h2 id="feature_alpha_dropout/3-options" class="section-heading">
  <a href="#feature_alpha_dropout/3-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:rate</code> - dropout rate. Used to determine probability a connection
will be dropped. Required.</p></li><li><p><code class="inline">:noise_shape</code> - input noise shape. Shape of <code class="inline">mask</code> which can be useful
for broadcasting <code class="inline">mask</code> across feature channels or other dimensions.
Defaults to shape of input tensor.</p></li></ul>
  </section>
</section>
<section class="detail" id="spatial_dropout/3">

    <span id="spatial_dropout/2"></span>

  <div class="detail-header">
    <a href="#spatial_dropout/3" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">spatial_dropout(input, key, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1469" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of an n-dimensional spatial
dropout layer.</p><p>Applies a mask to entire feature maps instead of individual
elements. This is done by calculating a mask shape equal to
the spatial dimensions of the input tensor with 1 channel,
and then broadcasting the mask across the feature dimension
of the input tensor.</p><h2 id="spatial_dropout/3-options" class="section-heading">
  <a href="#spatial_dropout/3-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:rate</code> - dropout rate. Used to determine probability a connection
will be dropped. Required.</p></li><li><p><code class="inline">:noise_shape</code> - input noise shape. Shape of <code class="inline">mask</code> which can be useful
for broadcasting <code class="inline">mask</code> across feature channels or other dimensions.
Defaults to shape of input tensor.</p></li></ul><h2 id="spatial_dropout/3-references" class="section-heading">
  <a href="#spatial_dropout/3-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="https://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a></li></ul>
  </section>
</section>

    </div>
  </section>

  <section id="layers-pooling" class="details-list">
    <h1 class="section-heading">
      <a class="hover-link" href="#layers-pooling">
        <i class="ri-link-m" aria-hidden="true"></i>
      </a>
      <span class="text">Layers: Pooling</span>
    </h1>
    <div class="layers-pooling-list">
<section class="detail" id="adaptive_avg_pool/2">

    <span id="adaptive_avg_pool/1"></span>

  <div class="detail-header">
    <a href="#adaptive_avg_pool/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">adaptive_avg_pool(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1060" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of general dimensional adaptive average
pooling.</p><p>Adaptive pooling allows you to specify the desired output size
of the transformed input. This will automatically adapt the
window size and strides to obtain the desired output size. It
will then perform average pooling using the calculated window
size and strides.</p><p>Adaptive pooling can be useful when working on multiple inputs with
different spatial input shapes. You can guarantee the output of
an adaptive pooling operation is always the same size regardless
of input shape.</p><h2 id="adaptive_avg_pool/2-options" class="section-heading">
  <a href="#adaptive_avg_pool/2-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:output_size</code> - spatial output size. Must be a tuple with
size equal to the spatial dimensions in the input tensor.
Required.</p></li><li><p><code class="inline">:channels</code> - channel configuration. One of <code class="inline">:first</code> or <code class="inline">:last</code>.
Defaults to <code class="inline">:last</code>.</p></li></ul>
  </section>
</section>
<section class="detail" id="adaptive_lp_pool/2">

    <span id="adaptive_lp_pool/1"></span>

  <div class="detail-header">
    <a href="#adaptive_lp_pool/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">adaptive_lp_pool(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1138" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of general dimensional adaptive power
average pooling.</p><p>Computes:</p><p>  $$
f(X) =  qrt[p]{ um_{x in X} x^{p}}
$$</p><p>Adaptive pooling allows you to specify the desired output size
of the transformed input. This will automatically adapt the
window size and strides to obtain the desired output size. It
will then perform max pooling using the calculated window
size and strides.</p><p>Adaptive pooling can be useful when working on multiple inputs with
different spatial input shapes. You can guarantee the output of
an adaptive pooling operation is always the same size regardless
of input shape.</p><h2 id="adaptive_lp_pool/2-options" class="section-heading">
  <a href="#adaptive_lp_pool/2-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:norm</code> - $p$ from above equation. Defaults to 2.</p></li><li><p><code class="inline">:output_size</code> - spatial output size. Must be a tuple with
size equal to the spatial dimensions in the input tensor.
Required.</p></li></ul>
  </section>
</section>
<section class="detail" id="adaptive_max_pool/2">

    <span id="adaptive_max_pool/1"></span>

  <div class="detail-header">
    <a href="#adaptive_max_pool/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">adaptive_max_pool(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1096" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of general dimensional adaptive max
pooling.</p><p>Adaptive pooling allows you to specify the desired output size
of the transformed input. This will automatically adapt the
window size and strides to obtain the desired output size. It
will then perform max pooling using the calculated window
size and strides.</p><p>Adaptive pooling can be useful when working on multiple inputs with
different spatial input shapes. You can guarantee the output of
an adaptive pooling operation is always the same size regardless
of input shape.</p><h2 id="adaptive_max_pool/2-options" class="section-heading">
  <a href="#adaptive_max_pool/2-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><code class="inline">:output_size</code> - spatial output size. Must be a tuple with
size equal to the spatial dimensions in the input tensor.
Required.</li></ul>
  </section>
</section>
<section class="detail" id="avg_pool/2">

    <span id="avg_pool/1"></span>

  <div class="detail-header">
    <a href="#avg_pool/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">avg_pool(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L840" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>A general dimensional functional average pooling layer.</p><p>Pooling is applied to the spatial dimension of the input tensor.
Average pooling returns the average of all elements in valid
windows in the input tensor. It is often used after convolutional
layers to downsample the input even further.</p><h2 id="avg_pool/2-options" class="section-heading">
  <a href="#avg_pool/2-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">kernel_size</code> - window size. Rank must match spatial dimension
of the input tensor. Required.</p></li><li><p><code class="inline">:strides</code> - kernel strides. Can be a scalar or a list
who's length matches the number of spatial dimensions in
the input tensor. Defaults to 1.</p></li><li><p><code class="inline">:padding</code> - zero padding on the input. Can be one of
<code class="inline">:valid</code>, <code class="inline">:same</code> or a general padding configuration
without interior padding for each spatial dimension
of the input.</p></li><li><p><code class="inline">:window_dilations</code> - kernel dilation factor. Equivalent
to applying interior padding on the kernel. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Can be scalar or list who's length matches the number of
spatial dimensions in the input tensor. Defaults to <code class="inline">1</code> or no
dilation.</p></li><li><p><code class="inline">:channels</code> - channel configuration. One of <code class="inline">:first</code> or <code class="inline">:last</code>.
Defaults to <code class="inline">:last</code>.</p></li></ul>
  </section>
</section>
<section class="detail" id="blur_pool/2">

    <span id="blur_pool/1"></span>

  <div class="detail-header">
    <a href="#blur_pool/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">blur_pool(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L974" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a 2-dimensional blur pooling layer.</p><p>Blur pooling applies a spatial low-pass filter to the input. It is
often applied before pooling and convolutional layers as a way to
increase model accuracy without much additional computation cost.</p><p>The blur pooling implementation follows from <a href="https://github.com/mosaicml/composer/blob/dev/composer/algorithms/blurpool/blurpool_layers.py">MosaicML</a>.</p>
  </section>
</section>
<section class="detail" id="global_avg_pool/2">

    <span id="global_avg_pool/1"></span>

  <div class="detail-header">
    <a href="#global_avg_pool/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">global_avg_pool(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1611" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of global average pooling which averages across
the spatial dimensions of the input such that the only remaining dimensions
are the batch and feature dimensions.</p><p>Assumes data is configured in a channels-first like format.</p><h2 id="global_avg_pool/2-parameter-shapes" class="section-heading">
  <a href="#global_avg_pool/2-parameter-shapes" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Parameter Shapes</span>
</h2>
<ul><li><code class="inline">input</code> - {batch_size, features, s1, ..., sN}</li></ul><h2 id="global_avg_pool/2-options" class="section-heading">
  <a href="#global_avg_pool/2-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><code class="inline">:keep_axes</code> - option to keep reduced axes with size 1 for each reduced
dimensions. Defaults to <code class="inline">false</code></li></ul><h2 id="global_avg_pool/2-examples" class="section-heading">
  <a href="#global_avg_pool/2-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">global_avg_pool</span><span class="p" data-group-id="1759944032-1">(</span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="1759944032-2">(</span><span class="p" data-group-id="1759944032-3">{</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p" data-group-id="1759944032-3">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="1759944032-4">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="1759944032-4">}</span><span class="p" data-group-id="1759944032-2">)</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="1759944032-1">)</span><span class="w">
</span><span class="p" data-group-id="1759944032-5">#</span><span class="nc" data-group-id="1759944032-5">Nx.Tensor</span><span class="p" data-group-id="1759944032-5">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="1759944032-6">[</span><span class="mi">3</span><span class="p" data-group-id="1759944032-6">]</span><span class="p" data-group-id="1759944032-7">[</span><span class="mi">2</span><span class="p" data-group-id="1759944032-7">]</span><span class="w">
  </span><span class="p" data-group-id="1759944032-8">[</span><span class="w">
    </span><span class="p" data-group-id="1759944032-9">[</span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="mf">4.0</span><span class="p" data-group-id="1759944032-9">]</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="1759944032-10">[</span><span class="mf">7.0</span><span class="p">,</span><span class="w"> </span><span class="mf">10.0</span><span class="p" data-group-id="1759944032-10">]</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="1759944032-11">[</span><span class="mf">13.0</span><span class="p">,</span><span class="w"> </span><span class="mf">16.0</span><span class="p" data-group-id="1759944032-11">]</span><span class="w">
  </span><span class="p" data-group-id="1759944032-8">]</span><span class="w">
</span><span class="p" data-group-id="1759944032-5">&gt;</span><span class="w">

</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">global_avg_pool</span><span class="p" data-group-id="1759944032-12">(</span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="1759944032-13">(</span><span class="p" data-group-id="1759944032-14">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p" data-group-id="1759944032-14">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="1759944032-15">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="1759944032-15">}</span><span class="p" data-group-id="1759944032-13">)</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p">,</span><span class="w"> </span><span class="ss">keep_axes</span><span class="p">:</span><span class="w"> </span><span class="no">true</span><span class="p" data-group-id="1759944032-12">)</span><span class="w">
</span><span class="p" data-group-id="1759944032-16">#</span><span class="nc" data-group-id="1759944032-16">Nx.Tensor</span><span class="p" data-group-id="1759944032-16">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="1759944032-17">[</span><span class="mi">1</span><span class="p" data-group-id="1759944032-17">]</span><span class="p" data-group-id="1759944032-18">[</span><span class="mi">3</span><span class="p" data-group-id="1759944032-18">]</span><span class="p" data-group-id="1759944032-19">[</span><span class="mi">1</span><span class="p" data-group-id="1759944032-19">]</span><span class="p" data-group-id="1759944032-20">[</span><span class="mi">1</span><span class="p" data-group-id="1759944032-20">]</span><span class="w">
  </span><span class="p" data-group-id="1759944032-21">[</span><span class="w">
    </span><span class="p" data-group-id="1759944032-22">[</span><span class="w">
      </span><span class="p" data-group-id="1759944032-23">[</span><span class="w">
        </span><span class="p" data-group-id="1759944032-24">[</span><span class="mf">1.5</span><span class="p" data-group-id="1759944032-24">]</span><span class="w">
      </span><span class="p" data-group-id="1759944032-23">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="1759944032-25">[</span><span class="w">
        </span><span class="p" data-group-id="1759944032-26">[</span><span class="mf">5.5</span><span class="p" data-group-id="1759944032-26">]</span><span class="w">
      </span><span class="p" data-group-id="1759944032-25">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="1759944032-27">[</span><span class="w">
        </span><span class="p" data-group-id="1759944032-28">[</span><span class="mf">9.5</span><span class="p" data-group-id="1759944032-28">]</span><span class="w">
      </span><span class="p" data-group-id="1759944032-27">]</span><span class="w">
    </span><span class="p" data-group-id="1759944032-22">]</span><span class="w">
  </span><span class="p" data-group-id="1759944032-21">]</span><span class="w">
</span><span class="p" data-group-id="1759944032-16">&gt;</span></code></pre>
  </section>
</section>
<section class="detail" id="global_lp_pool/2">

    <span id="global_lp_pool/1"></span>

  <div class="detail-header">
    <a href="#global_lp_pool/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">global_lp_pool(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1730" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of global LP pooling which computes the following
function across spatial dimensions of the input:</p><p>  $$
f(X) =  qrt[p]{ um_{x in X} x^{p}}
$$</p><p>Where $p$ is given by the keyword argument <code class="inline">:norm</code>. As $p$ approaches
infinity, it becomes equivalent to max pooling.</p><p>Assumes data is configured in a channels-first like format.</p><h2 id="global_lp_pool/2-parameter-shapes" class="section-heading">
  <a href="#global_lp_pool/2-parameter-shapes" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Parameter Shapes</span>
</h2>
<ul><li><code class="inline">input</code> - {batch_size, s1, ..., sN, features}</li></ul><h2 id="global_lp_pool/2-options" class="section-heading">
  <a href="#global_lp_pool/2-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><code class="inline">:keep_axes</code> - option to keep reduced axes with size 1 for each reduced
dimensions. Defaults to <code class="inline">false</code></li><li><code class="inline">:norm</code> - $p$ in above function. Defaults to 2</li></ul><h2 id="global_lp_pool/2-examples" class="section-heading">
  <a href="#global_lp_pool/2-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">global_lp_pool</span><span class="p" data-group-id="5648607303-1">(</span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="5648607303-2">(</span><span class="p" data-group-id="5648607303-3">{</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p" data-group-id="5648607303-3">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="5648607303-4">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="5648607303-4">}</span><span class="p" data-group-id="5648607303-2">)</span><span class="p">,</span><span class="w"> </span><span class="ss">norm</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="5648607303-1">)</span><span class="w">
</span><span class="p" data-group-id="5648607303-5">#</span><span class="nc" data-group-id="5648607303-5">Nx.Tensor</span><span class="p" data-group-id="5648607303-5">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="5648607303-6">[</span><span class="mi">3</span><span class="p" data-group-id="5648607303-6">]</span><span class="p" data-group-id="5648607303-7">[</span><span class="mi">2</span><span class="p" data-group-id="5648607303-7">]</span><span class="w">
  </span><span class="p" data-group-id="5648607303-8">[</span><span class="w">
    </span><span class="p" data-group-id="5648607303-9">[</span><span class="mf">3.0</span><span class="p">,</span><span class="w"> </span><span class="mf">12.0</span><span class="p" data-group-id="5648607303-9">]</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="5648607303-10">[</span><span class="mf">21.0</span><span class="p">,</span><span class="w"> </span><span class="mf">30.0</span><span class="p" data-group-id="5648607303-10">]</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="5648607303-11">[</span><span class="mf">39.0</span><span class="p">,</span><span class="w"> </span><span class="mf">48.0</span><span class="p" data-group-id="5648607303-11">]</span><span class="w">
  </span><span class="p" data-group-id="5648607303-8">]</span><span class="w">
</span><span class="p" data-group-id="5648607303-5">&gt;</span><span class="w">

</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">global_lp_pool</span><span class="p" data-group-id="5648607303-12">(</span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="5648607303-13">(</span><span class="p" data-group-id="5648607303-14">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p" data-group-id="5648607303-14">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="5648607303-15">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p" data-group-id="5648607303-15">}</span><span class="p" data-group-id="5648607303-13">)</span><span class="p">,</span><span class="w"> </span><span class="ss">keep_axes</span><span class="p">:</span><span class="w"> </span><span class="no">true</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="5648607303-12">)</span><span class="w">
</span><span class="p" data-group-id="5648607303-16">#</span><span class="nc" data-group-id="5648607303-16">Nx.Tensor</span><span class="p" data-group-id="5648607303-16">&lt;</span><span class="w">
  </span><span class="n">f16</span><span class="p" data-group-id="5648607303-17">[</span><span class="mi">1</span><span class="p" data-group-id="5648607303-17">]</span><span class="p" data-group-id="5648607303-18">[</span><span class="mi">3</span><span class="p" data-group-id="5648607303-18">]</span><span class="p" data-group-id="5648607303-19">[</span><span class="mi">1</span><span class="p" data-group-id="5648607303-19">]</span><span class="p" data-group-id="5648607303-20">[</span><span class="mi">1</span><span class="p" data-group-id="5648607303-20">]</span><span class="w">
  </span><span class="p" data-group-id="5648607303-21">[</span><span class="w">
    </span><span class="p" data-group-id="5648607303-22">[</span><span class="w">
      </span><span class="p" data-group-id="5648607303-23">[</span><span class="w">
        </span><span class="p" data-group-id="5648607303-24">[</span><span class="mf">3.7421875</span><span class="p" data-group-id="5648607303-24">]</span><span class="w">
      </span><span class="p" data-group-id="5648607303-23">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="5648607303-25">[</span><span class="w">
        </span><span class="p" data-group-id="5648607303-26">[</span><span class="mf">11.2265625</span><span class="p" data-group-id="5648607303-26">]</span><span class="w">
      </span><span class="p" data-group-id="5648607303-25">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="5648607303-27">[</span><span class="w">
        </span><span class="p" data-group-id="5648607303-28">[</span><span class="mf">19.125</span><span class="p" data-group-id="5648607303-28">]</span><span class="w">
      </span><span class="p" data-group-id="5648607303-27">]</span><span class="w">
    </span><span class="p" data-group-id="5648607303-22">]</span><span class="w">
  </span><span class="p" data-group-id="5648607303-21">]</span><span class="w">
</span><span class="p" data-group-id="5648607303-16">&gt;</span></code></pre>
  </section>
</section>
<section class="detail" id="global_max_pool/2">

    <span id="global_max_pool/1"></span>

  <div class="detail-header">
    <a href="#global_max_pool/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">global_max_pool(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1668" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of global max pooling which computes maximums across
the spatial dimensions of the input such that the only remaining dimensions are
the batch and feature dimensions.</p><p>Assumes data is configured in a channels-first like format.</p><h2 id="global_max_pool/2-parameter-shapes" class="section-heading">
  <a href="#global_max_pool/2-parameter-shapes" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Parameter Shapes</span>
</h2>
<ul><li><code class="inline">input</code> - {batch_size, s1, ..., sN, features}</li></ul><h2 id="global_max_pool/2-options" class="section-heading">
  <a href="#global_max_pool/2-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><code class="inline">:keep_axes</code> - option to keep reduced axes with size 1 for each reduced
dimensions. Defaults to <code class="inline">false</code></li></ul><h2 id="global_max_pool/2-examples" class="section-heading">
  <a href="#global_max_pool/2-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">global_max_pool</span><span class="p" data-group-id="5057921806-1">(</span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="5057921806-2">(</span><span class="p" data-group-id="5057921806-3">{</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p" data-group-id="5057921806-3">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="5057921806-4">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="5057921806-4">}</span><span class="p" data-group-id="5057921806-2">)</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="5057921806-1">)</span><span class="w">
</span><span class="p" data-group-id="5057921806-5">#</span><span class="nc" data-group-id="5057921806-5">Nx.Tensor</span><span class="p" data-group-id="5057921806-5">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="5057921806-6">[</span><span class="mi">3</span><span class="p" data-group-id="5057921806-6">]</span><span class="p" data-group-id="5057921806-7">[</span><span class="mi">2</span><span class="p" data-group-id="5057921806-7">]</span><span class="w">
  </span><span class="p" data-group-id="5057921806-8">[</span><span class="w">
    </span><span class="p" data-group-id="5057921806-9">[</span><span class="mf">2.0</span><span class="p">,</span><span class="w"> </span><span class="mf">5.0</span><span class="p" data-group-id="5057921806-9">]</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="5057921806-10">[</span><span class="mf">8.0</span><span class="p">,</span><span class="w"> </span><span class="mf">11.0</span><span class="p" data-group-id="5057921806-10">]</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="5057921806-11">[</span><span class="mf">14.0</span><span class="p">,</span><span class="w"> </span><span class="mf">17.0</span><span class="p" data-group-id="5057921806-11">]</span><span class="w">
  </span><span class="p" data-group-id="5057921806-8">]</span><span class="w">
</span><span class="p" data-group-id="5057921806-5">&gt;</span><span class="w">

</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">global_max_pool</span><span class="p" data-group-id="5057921806-12">(</span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="5057921806-13">(</span><span class="p" data-group-id="5057921806-14">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p" data-group-id="5057921806-14">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="5057921806-15">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="5057921806-15">}</span><span class="p" data-group-id="5057921806-13">)</span><span class="p">,</span><span class="w"> </span><span class="ss">keep_axes</span><span class="p">:</span><span class="w"> </span><span class="no">true</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="5057921806-12">)</span><span class="w">
</span><span class="p" data-group-id="5057921806-16">#</span><span class="nc" data-group-id="5057921806-16">Nx.Tensor</span><span class="p" data-group-id="5057921806-16">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="5057921806-17">[</span><span class="mi">1</span><span class="p" data-group-id="5057921806-17">]</span><span class="p" data-group-id="5057921806-18">[</span><span class="mi">3</span><span class="p" data-group-id="5057921806-18">]</span><span class="p" data-group-id="5057921806-19">[</span><span class="mi">1</span><span class="p" data-group-id="5057921806-19">]</span><span class="p" data-group-id="5057921806-20">[</span><span class="mi">1</span><span class="p" data-group-id="5057921806-20">]</span><span class="w">
  </span><span class="p" data-group-id="5057921806-21">[</span><span class="w">
    </span><span class="p" data-group-id="5057921806-22">[</span><span class="w">
      </span><span class="p" data-group-id="5057921806-23">[</span><span class="w">
        </span><span class="p" data-group-id="5057921806-24">[</span><span class="mf">3.0</span><span class="p" data-group-id="5057921806-24">]</span><span class="w">
      </span><span class="p" data-group-id="5057921806-23">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="5057921806-25">[</span><span class="w">
        </span><span class="p" data-group-id="5057921806-26">[</span><span class="mf">7.0</span><span class="p" data-group-id="5057921806-26">]</span><span class="w">
      </span><span class="p" data-group-id="5057921806-25">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="5057921806-27">[</span><span class="w">
        </span><span class="p" data-group-id="5057921806-28">[</span><span class="mf">11.0</span><span class="p" data-group-id="5057921806-28">]</span><span class="w">
      </span><span class="p" data-group-id="5057921806-27">]</span><span class="w">
    </span><span class="p" data-group-id="5057921806-22">]</span><span class="w">
  </span><span class="p" data-group-id="5057921806-21">]</span><span class="w">
</span><span class="p" data-group-id="5057921806-16">&gt;</span></code></pre>
  </section>
</section>
<section class="detail" id="lp_pool/2">

    <span id="lp_pool/1"></span>

  <div class="detail-header">
    <a href="#lp_pool/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">lp_pool(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L927" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a general dimensional power average
pooling layer.</p><p>Pooling is applied to the spatial dimension of the input tensor.
Power average pooling computes the following function on each
valid window of the input tensor:</p><p>$$
f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}
$$</p><p>Where $p$ is given by the keyword argument <code class="inline">:norm</code>. As $p$ approaches
infinity, it becomes equivalent to max pooling.</p><h2 id="lp_pool/2-options" class="section-heading">
  <a href="#lp_pool/2-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:norm</code> - $p$ from above equation. Defaults to 2.</p></li><li><p><code class="inline">:kernel_size</code> - window size. Rank must match spatial dimension
of the input tensor. Required.</p></li><li><p><code class="inline">:strides</code> - kernel strides. Can be a scalar or a list
who's length matches the number of spatial dimensions in
the input tensor. Defaults to size of kernel.</p></li><li><p><code class="inline">:padding</code> - zero padding on the input. Can be one of
<code class="inline">:valid</code>, <code class="inline">:same</code> or a general padding configuration
without interior padding for each spatial dimension
of the input.</p></li><li><p><code class="inline">:window_dilations</code> - kernel dilation factor. Equivalent
to applying interior padding on the kernel. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Can be scalar or list who's length matches the number of
spatial dimensions in the input tensor. Defaults to <code class="inline">1</code> or no
dilation.</p></li><li><p><code class="inline">:channels</code> - channel configuration. One of <code class="inline">:first</code> or <code class="inline">:last</code>.
Defaults to <code class="inline">:last</code>.</p></li></ul><h2 id="lp_pool/2-examples" class="section-heading">
  <a href="#lp_pool/2-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="6550792598-1">(</span><span class="p" data-group-id="6550792598-2">[</span><span class="p" data-group-id="6550792598-3">[</span><span class="p" data-group-id="6550792598-4">[</span><span class="mf">0.9450</span><span class="p">,</span><span class="w"> </span><span class="mf">0.4684</span><span class="p">,</span><span class="w"> </span><span class="mf">1.8146</span><span class="p" data-group-id="6550792598-4">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="6550792598-5">[</span><span class="mf">1.2663</span><span class="p">,</span><span class="w"> </span><span class="mf">0.4354</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.0781</span><span class="p" data-group-id="6550792598-5">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="6550792598-6">[</span><span class="o">-</span><span class="mf">0.4759</span><span class="p">,</span><span class="w"> </span><span class="mf">0.3251</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8742</span><span class="p" data-group-id="6550792598-6">]</span><span class="p" data-group-id="6550792598-3">]</span><span class="p" data-group-id="6550792598-2">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="6550792598-7">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="6550792598-7">}</span><span class="p" data-group-id="6550792598-1">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">lp_pool</span><span class="p" data-group-id="6550792598-8">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="ss">kernel_size</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="ss">norm</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="6550792598-8">)</span><span class="w">
</span><span class="p" data-group-id="6550792598-9">#</span><span class="nc" data-group-id="6550792598-9">Nx.Tensor</span><span class="p" data-group-id="6550792598-9">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="6550792598-10">[</span><span class="mi">1</span><span class="p" data-group-id="6550792598-10">]</span><span class="p" data-group-id="6550792598-11">[</span><span class="mi">3</span><span class="p" data-group-id="6550792598-11">]</span><span class="p" data-group-id="6550792598-12">[</span><span class="mi">1</span><span class="p" data-group-id="6550792598-12">]</span><span class="w">
  </span><span class="p" data-group-id="6550792598-13">[</span><span class="w">
    </span><span class="p" data-group-id="6550792598-14">[</span><span class="w">
      </span><span class="p" data-group-id="6550792598-15">[</span><span class="mf">1.0547149181365967</span><span class="p" data-group-id="6550792598-15">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="6550792598-16">[</span><span class="mf">1.3390626907348633</span><span class="p" data-group-id="6550792598-16">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="6550792598-17">[</span><span class="mf">0.5763426423072815</span><span class="p" data-group-id="6550792598-17">]</span><span class="w">
    </span><span class="p" data-group-id="6550792598-14">]</span><span class="w">
  </span><span class="p" data-group-id="6550792598-13">]</span><span class="w">
</span><span class="p" data-group-id="6550792598-9">&gt;</span></code></pre>
  </section>
</section>
<section class="detail" id="max_pool/2">

    <span id="max_pool/1"></span>

  <div class="detail-header">
    <a href="#max_pool/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">max_pool(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L775" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a general dimensional max pooling layer.</p><p>Pooling is applied to the spatial dimension of the input tensor.
Max pooling returns the maximum element in each valid window of
the input tensor. It is often used after convolutional layers
to downsample the input even further.</p><h2 id="max_pool/2-options" class="section-heading">
  <a href="#max_pool/2-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">kernel_size</code> - window size. Rank must match spatial dimension
of the input tensor. Required.</p></li><li><p><code class="inline">:strides</code> - kernel strides. Can be a scalar or a list
whose length matches the number of spatial dimensions in
the input tensor. Defaults to size of kernel.</p></li><li><p><code class="inline">:padding</code> - zero padding on the input. Can be one of
<code class="inline">:valid</code>, <code class="inline">:same</code> or a general padding configuration
without interior padding for each spatial dimension
of the input.</p></li><li><p><code class="inline">:window_dilations</code> - kernel dilation factor. Equivalent
to applying interior padding on the kernel. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Can be scalar or list who's length matches the number of
spatial dimensions in the input tensor. Defaults to <code class="inline">1</code> or no
dilation.</p></li><li><p><code class="inline">:channels</code> - channel configuration. One of <code class="inline">:first</code> or <code class="inline">:last</code>.
Defaults to <code class="inline">:last</code>.</p></li></ul><h2 id="max_pool/2-examples" class="section-heading">
  <a href="#max_pool/2-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="1133530712-1">(</span><span class="p" data-group-id="1133530712-2">[</span><span class="p" data-group-id="1133530712-3">[</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="p" data-group-id="1133530712-4">[</span><span class="mf">0.051500000059604645</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.7042999863624573</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.32899999618530273</span><span class="p" data-group-id="1133530712-4">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="p" data-group-id="1133530712-5">[</span><span class="o">-</span><span class="mf">0.37130001187324524</span><span class="p">,</span><span class="w"> </span><span class="mf">1.6191999912261963</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.11829999834299088</span><span class="p" data-group-id="1133530712-5">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="p" data-group-id="1133530712-6">[</span><span class="mf">0.7099999785423279</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7282999753952026</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.18639999628067017</span><span class="p" data-group-id="1133530712-6">]</span><span class="p" data-group-id="1133530712-3">]</span><span class="p" data-group-id="1133530712-2">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="1133530712-7">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="1133530712-7">}</span><span class="p" data-group-id="1133530712-1">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">max_pool</span><span class="p" data-group-id="1133530712-8">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="ss">kernel_size</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="1133530712-8">)</span><span class="w">
</span><span class="p" data-group-id="1133530712-9">#</span><span class="nc" data-group-id="1133530712-9">Nx.Tensor</span><span class="p" data-group-id="1133530712-9">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="1133530712-10">[</span><span class="mi">1</span><span class="p" data-group-id="1133530712-10">]</span><span class="p" data-group-id="1133530712-11">[</span><span class="mi">3</span><span class="p" data-group-id="1133530712-11">]</span><span class="p" data-group-id="1133530712-12">[</span><span class="mi">1</span><span class="p" data-group-id="1133530712-12">]</span><span class="w">
  </span><span class="p" data-group-id="1133530712-13">[</span><span class="w">
    </span><span class="p" data-group-id="1133530712-14">[</span><span class="w">
      </span><span class="p" data-group-id="1133530712-15">[</span><span class="mf">0.051500000059604645</span><span class="p" data-group-id="1133530712-15">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="1133530712-16">[</span><span class="mf">1.6191999912261963</span><span class="p" data-group-id="1133530712-16">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="1133530712-17">[</span><span class="mf">0.7282999753952026</span><span class="p" data-group-id="1133530712-17">]</span><span class="w">
    </span><span class="p" data-group-id="1133530712-14">]</span><span class="w">
  </span><span class="p" data-group-id="1133530712-13">]</span><span class="w">
</span><span class="p" data-group-id="1133530712-9">&gt;</span></code></pre>
  </section>
</section>

    </div>
  </section>

  <section id="layers-normalization" class="details-list">
    <h1 class="section-heading">
      <a class="hover-link" href="#layers-normalization">
        <i class="ri-link-m" aria-hidden="true"></i>
      </a>
      <span class="text">Layers: Normalization</span>
    </h1>
    <div class="layers-normalization-list">
<section class="detail" id="batch_norm/6">

    <span id="batch_norm/5"></span>

  <div class="detail-header">
    <a href="#batch_norm/6" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">batch_norm(input, gamma, beta, ra_mean, ra_var, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1190" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of batch normalization.</p><p>Normalizes the input by calculating mean and variance of the
input tensor along every dimension but the given <code class="inline">:channel_index</code>,
and then scaling according to:</p><p>$$
y = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}} * \gamma + \beta
$$</p><p><code class="inline">gamma</code> and <code class="inline">beta</code> are often trainable parameters. If <code class="inline">training?</code> is
true, this method will compute a new mean and variance, and return
the updated <code class="inline">ra_mean</code> and <code class="inline">ra_var</code>. Otherwise, it will just compute
batch norm from the given ra_mean and ra_var.</p><h2 id="batch_norm/6-options" class="section-heading">
  <a href="#batch_norm/6-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:epsilon</code> - numerical stability term. $epsilon$ in the above
formulation.</p></li><li><p><code class="inline">:channel_index</code> - channel index used to determine reduction
axes for mean and variance calculation.</p></li><li><p><code class="inline">:momentum</code> - momentum to use for EMA update.</p></li><li><p><code class="inline">:mode</code> - if <code class="inline">:train</code>, uses training mode batch norm. Defaults to <code class="inline">:inference</code>.</p></li></ul><h2 id="batch_norm/6-references" class="section-heading">
  <a href="#batch_norm/6-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li></ul>
  </section>
</section>
<section class="detail" id="group_norm/4">

    <span id="group_norm/3"></span>

  <div class="detail-header">
    <a href="#group_norm/4" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">group_norm(input, gamma, beta, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1278" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of group normalization.</p><p>Normalizes the input by reshaping input into <code class="inline">:num_groups</code>
groups and then calculating the mean and variance along
every dimension but the input batch dimension.</p><p>$$
y = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}} * \gamma + \beta
$$</p><p><code class="inline">gamma</code> and <code class="inline">beta</code> are often trainable parameters. This method does
not maintain an EMA of mean and variance.</p><h2 id="group_norm/4-options" class="section-heading">
  <a href="#group_norm/4-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:num_groups</code> - Number of groups.</p></li><li><p><code class="inline">:epsilon</code> - numerical stability term. $epsilon$ in the above
formulation.</p></li><li><p><code class="inline">:channel_index</code> - channel index used to determine reduction
axes and group shape for mean and variance calculation.</p></li></ul><h2 id="group_norm/4-references" class="section-heading">
  <a href="#group_norm/4-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="https://arxiv.org/abs/1803.08494v3">Group Normalization</a></li></ul>
  </section>
</section>
<section class="detail" id="instance_norm/6">

    <span id="instance_norm/5"></span>

  <div class="detail-header">
    <a href="#instance_norm/6" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">instance_norm(input, gamma, beta, ra_mean, ra_var, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1334" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of instance normalization.</p><p>Normalizes the input by calculating mean and variance of the
input tensor along the spatial dimensions of the input.</p><p>$$
y = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}} * \gamma + \beta
$$</p><p><code class="inline">gamma</code> and <code class="inline">beta</code> are often trainable parameters. If <code class="inline">training?</code> is
true, this method will compute a new mean and variance, and return
the updated <code class="inline">ra_mean</code> and <code class="inline">ra_var</code>. Otherwise, it will just compute
batch norm from the given ra_mean and ra_var.</p><h2 id="instance_norm/6-options" class="section-heading">
  <a href="#instance_norm/6-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:epsilon</code> - numerical stability term. $epsilon$ in the above
formulation.</p></li><li><p><code class="inline">:channel_index</code> - channel index used to determine reduction
axes for mean and variance calculation.</p></li><li><p><code class="inline">:momentum</code> - momentum to use for EMA update.</p></li><li><p><code class="inline">:training?</code> - if true, uses training mode batch norm. Defaults to false.</p></li></ul><h2 id="instance_norm/6-references" class="section-heading">
  <a href="#instance_norm/6-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="https://arxiv.org/abs/1607.08022v3">Instance Normalization: The Missing Ingredient for Fast Stylization</a></li></ul>
  </section>
</section>
<section class="detail" id="layer_norm/4">

    <span id="layer_norm/3"></span>

  <div class="detail-header">
    <a href="#layer_norm/4" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">layer_norm(input, gamma, beta, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1236" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of layer normalization.</p><p>Normalizes the input by calculating mean and variance of the
input tensor along the given feature dimension <code class="inline">:channel_index</code>.</p><p>$$
y = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}} * \gamma + \beta
$$</p><p><code class="inline">gamma</code> and <code class="inline">beta</code> are often trainable parameters. This method does
not maintain an EMA of mean and variance.</p><h2 id="layer_norm/4-options" class="section-heading">
  <a href="#layer_norm/4-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:epsilon</code> - numerical stability term. $epsilon$ in the above
formulation.</p></li><li><p><code class="inline">:channel_index</code> - channel index used to determine reduction
axes for mean and variance calculation.</p></li></ul>
  </section>
</section>

    </div>
  </section>

  <section id="layers-shape" class="details-list">
    <h1 class="section-heading">
      <a class="hover-link" href="#layers-shape">
        <i class="ri-link-m" aria-hidden="true"></i>
      </a>
      <span class="text">Layers: Shape</span>
    </h1>
    <div class="layers-shape-list">
<section class="detail" id="flatten/2">

    <span id="flatten/1"></span>

  <div class="detail-header">
    <a href="#flatten/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">flatten(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1818" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Flattens input to shape of <code class="inline">{batch, units}</code> by folding outer
dimensions.</p><h2 id="flatten/2-examples" class="section-heading">
  <a href="#flatten/2-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">flatten</span><span class="p" data-group-id="4118448741-1">(</span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="4118448741-2">(</span><span class="p" data-group-id="4118448741-3">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p" data-group-id="4118448741-3">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="4118448741-4">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="4118448741-4">}</span><span class="p" data-group-id="4118448741-2">)</span><span class="p" data-group-id="4118448741-1">)</span><span class="w">
</span><span class="p" data-group-id="4118448741-5">#</span><span class="nc" data-group-id="4118448741-5">Nx.Tensor</span><span class="p" data-group-id="4118448741-5">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="4118448741-6">[</span><span class="mi">1</span><span class="p" data-group-id="4118448741-6">]</span><span class="p" data-group-id="4118448741-7">[</span><span class="mi">4</span><span class="p" data-group-id="4118448741-7">]</span><span class="w">
  </span><span class="p" data-group-id="4118448741-8">[</span><span class="w">
    </span><span class="p" data-group-id="4118448741-9">[</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="mf">2.0</span><span class="p">,</span><span class="w"> </span><span class="mf">3.0</span><span class="p" data-group-id="4118448741-9">]</span><span class="w">
  </span><span class="p" data-group-id="4118448741-8">]</span><span class="w">
</span><span class="p" data-group-id="4118448741-5">&gt;</span></code></pre>
  </section>
</section>
<section class="detail" id="resize/2">

    <span id="resize/1"></span>

  <div class="detail-header">
    <a href="#resize/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">resize(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1959" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Resizes a batch of tensors to the given shape using one of a
number of sampling methods.</p><p>Requires input option <code class="inline">:size</code> which should be a tuple specifying
the resized spatial dimensions of the input tensor. Input tensor
must be at least rank 3, with fixed <code class="inline">batch</code> and <code class="inline">channel</code> dimensions.
Resizing will upsample or downsample using the given resize method.</p><h2 id="resize/2-options" class="section-heading">
  <a href="#resize/2-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:size</code> - a tuple specifying the resized spatial dimensions.
Required.</p></li><li><p><code class="inline">:method</code> - the resizing method to use, either of <code class="inline">:nearest</code>,
<code class="inline">:bilinear</code>, <code class="inline">:bicubic</code>, <code class="inline">:lanczos3</code>, <code class="inline">:lanczos5</code>. Defaults to
<code class="inline">:nearest</code>.</p></li><li><p><code class="inline">:antialias</code> - whether an anti-aliasing filter should be used
when downsampling. This has no effect with upsampling. Defaults
to <code class="inline">true</code>.</p></li><li><p><code class="inline">:channels</code> - channels location, either <code class="inline">:first</code> or <code class="inline">:last</code>.
Defaults to <code class="inline">:last</code>.</p></li></ul><h2 id="resize/2-examples" class="section-heading">
  <a href="#resize/2-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="n">img</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="1284347188-1">(</span><span class="p" data-group-id="1284347188-2">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p" data-group-id="1284347188-2">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="1284347188-3">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="1284347188-3">}</span><span class="p" data-group-id="1284347188-1">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">resize</span><span class="p" data-group-id="1284347188-4">(</span><span class="n">img</span><span class="p">,</span><span class="w"> </span><span class="ss">size</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="1284347188-5">{</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p" data-group-id="1284347188-5">}</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="1284347188-4">)</span><span class="w">
</span><span class="p" data-group-id="1284347188-6">#</span><span class="nc" data-group-id="1284347188-6">Nx.Tensor</span><span class="p" data-group-id="1284347188-6">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="1284347188-7">[</span><span class="mi">1</span><span class="p" data-group-id="1284347188-7">]</span><span class="p" data-group-id="1284347188-8">[</span><span class="mi">1</span><span class="p" data-group-id="1284347188-8">]</span><span class="p" data-group-id="1284347188-9">[</span><span class="mi">4</span><span class="p" data-group-id="1284347188-9">]</span><span class="p" data-group-id="1284347188-10">[</span><span class="mi">4</span><span class="p" data-group-id="1284347188-10">]</span><span class="w">
  </span><span class="p" data-group-id="1284347188-11">[</span><span class="w">
    </span><span class="p" data-group-id="1284347188-12">[</span><span class="w">
      </span><span class="p" data-group-id="1284347188-13">[</span><span class="w">
        </span><span class="p" data-group-id="1284347188-14">[</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="mf">2.0</span><span class="p" data-group-id="1284347188-14">]</span><span class="p">,</span><span class="w">
        </span><span class="p" data-group-id="1284347188-15">[</span><span class="mf">3.0</span><span class="p">,</span><span class="w"> </span><span class="mf">4.0</span><span class="p">,</span><span class="w"> </span><span class="mf">4.0</span><span class="p">,</span><span class="w"> </span><span class="mf">5.0</span><span class="p" data-group-id="1284347188-15">]</span><span class="p">,</span><span class="w">
        </span><span class="p" data-group-id="1284347188-16">[</span><span class="mf">3.0</span><span class="p">,</span><span class="w"> </span><span class="mf">4.0</span><span class="p">,</span><span class="w"> </span><span class="mf">4.0</span><span class="p">,</span><span class="w"> </span><span class="mf">5.0</span><span class="p" data-group-id="1284347188-16">]</span><span class="p">,</span><span class="w">
        </span><span class="p" data-group-id="1284347188-17">[</span><span class="mf">6.0</span><span class="p">,</span><span class="w"> </span><span class="mf">7.0</span><span class="p">,</span><span class="w"> </span><span class="mf">7.0</span><span class="p">,</span><span class="w"> </span><span class="mf">8.0</span><span class="p" data-group-id="1284347188-17">]</span><span class="w">
      </span><span class="p" data-group-id="1284347188-13">]</span><span class="w">
    </span><span class="p" data-group-id="1284347188-12">]</span><span class="w">
  </span><span class="p" data-group-id="1284347188-11">]</span><span class="w">
</span><span class="p" data-group-id="1284347188-6">&gt;</span></code></pre><h3 id="resize/2-error-cases" class="section-heading">
  <a href="#resize/2-error-cases" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Error cases</span>
</h3>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="n">img</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="6678532731-1">(</span><span class="p" data-group-id="6678532731-2">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p" data-group-id="6678532731-2">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="6678532731-3">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="6678532731-3">}</span><span class="p" data-group-id="6678532731-1">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">resize</span><span class="p" data-group-id="6678532731-4">(</span><span class="n">img</span><span class="p">,</span><span class="w"> </span><span class="ss">size</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="6678532731-5">{</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p" data-group-id="6678532731-5">}</span><span class="p">,</span><span class="w"> </span><span class="ss">method</span><span class="p">:</span><span class="w"> </span><span class="ss">:foo</span><span class="p" data-group-id="6678532731-4">)</span><span class="w">
</span><span class="gt">** (ArgumentError) expected :method to be either of :nearest, :bilinear, :bicubic, :lanczos3, :lanczos5, got: :foo</span></code></pre>
  </section>
</section>

    </div>
  </section>

  <section id="functions-convolutional" class="details-list">
    <h1 class="section-heading">
      <a class="hover-link" href="#functions-convolutional">
        <i class="ri-link-m" aria-hidden="true"></i>
      </a>
      <span class="text">Functions: Convolutional</span>
    </h1>
    <div class="functions-convolutional-list">
<section class="detail" id="conv/4">

    <span id="conv/2"></span>

    <span id="conv/3"></span>

  <div class="detail-header">
    <a href="#conv/4" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">conv(input, kernel, bias \\ 0, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L325" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a general dimensional convolutional
layer.</p><p>Convolutional layers can be described as applying a convolution
over an input signal composed of several input planes. Intuitively,
the input kernel slides <code class="inline">output_channels</code> number of filters over
the input tensor to extract features from the input tensor.</p><p>Convolutional layers are most commonly used in computer vision,
but can also be useful when working with sequences and other input signals.</p><h2 id="conv/4-parameter-shapes" class="section-heading">
  <a href="#conv/4-parameter-shapes" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Parameter Shapes</span>
</h2>
<ul><li><code class="inline">input</code> - <code class="inline">{batch_size, input_channels, input_spatial0, ..., input_spatialN}</code></li><li><code class="inline">kernel</code> - <code class="inline">{output_channels, input_channels, kernel_spatial0, ..., kernel_spatialN}</code></li><li><code class="inline">bias</code> - <code class="inline">{}</code> or <code class="inline">{output_channels}</code></li></ul><h2 id="conv/4-options" class="section-heading">
  <a href="#conv/4-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:strides</code> - kernel strides. Can be a scalar or a list
whose length matches the number of spatial dimensions in
the input tensor. Defaults to 1.</p></li><li><p><code class="inline">:padding</code> - zero padding on the input. Can be one of
<code class="inline">:valid</code>, <code class="inline">:same</code> or a general padding configuration
without interior padding for each spatial dimension
of the input.</p></li><li><p><code class="inline">:input_dilation</code> - input dilation factor. Equivalent
to applying interior padding on the input. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Defaults to <code class="inline">1</code> or no dilation.</p></li><li><p><code class="inline">:kernel_dilation</code> - kernel dilation factor. Equivalent
to applying interior padding on the kernel. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Defaults to <code class="inline">1</code> or no dilation.</p></li><li><p><code class="inline">:channels</code> - channel configuration. One of <code class="inline">:first</code> or <code class="inline">:last</code>.
Defaults to <code class="inline">:last</code>.</p></li></ul><h2 id="conv/4-examples" class="section-heading">
  <a href="#conv/4-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<h3 id="conv/4-one-dimensional-convolution" class="section-heading">
  <a href="#conv/4-one-dimensional-convolution" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">One-dimensional convolution</span>
</h3>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="2176729903-1">(</span><span class="p" data-group-id="2176729903-2">[</span><span class="p" data-group-id="2176729903-3">[</span><span class="p" data-group-id="2176729903-4">[</span><span class="mf">0.1294</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.6638</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0251</span><span class="p" data-group-id="2176729903-4">]</span><span class="p" data-group-id="2176729903-3">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="2176729903-5">[</span><span class="p" data-group-id="2176729903-6">[</span><span class="w"> </span><span class="mf">0.9182</span><span class="p">,</span><span class="w">  </span><span class="mf">1.1512</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">1.6149</span><span class="p" data-group-id="2176729903-6">]</span><span class="p" data-group-id="2176729903-5">]</span><span class="p" data-group-id="2176729903-2">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="2176729903-7">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="2176729903-7">}</span><span class="p" data-group-id="2176729903-1">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="2176729903-8">(</span><span class="p" data-group-id="2176729903-9">[</span><span class="p" data-group-id="2176729903-10">[</span><span class="p" data-group-id="2176729903-11">[</span><span class="o">-</span><span class="mf">1.5475</span><span class="p">,</span><span class="w"> </span><span class="mf">1.2425</span><span class="p" data-group-id="2176729903-11">]</span><span class="p" data-group-id="2176729903-10">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="2176729903-12">[</span><span class="p" data-group-id="2176729903-13">[</span><span class="mf">0.1871</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5458</span><span class="p" data-group-id="2176729903-13">]</span><span class="p" data-group-id="2176729903-12">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="2176729903-14">[</span><span class="p" data-group-id="2176729903-15">[</span><span class="o">-</span><span class="mf">0.4488</span><span class="p">,</span><span class="w">  </span><span class="mf">0.8879</span><span class="p" data-group-id="2176729903-15">]</span><span class="p" data-group-id="2176729903-14">]</span><span class="p" data-group-id="2176729903-9">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="2176729903-16">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="2176729903-16">}</span><span class="p" data-group-id="2176729903-8">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="2176729903-17">(</span><span class="p" data-group-id="2176729903-18">[</span><span class="mf">0.7791</span><span class="p">,</span><span class="w"> </span><span class="mf">0.1676</span><span class="p">,</span><span class="w"> </span><span class="mf">1.5971</span><span class="p" data-group-id="2176729903-18">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="2176729903-19">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="2176729903-19">}</span><span class="p" data-group-id="2176729903-17">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">conv</span><span class="p" data-group-id="2176729903-20">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="2176729903-20">)</span><span class="w">
</span><span class="p" data-group-id="2176729903-21">#</span><span class="nc" data-group-id="2176729903-21">Nx.Tensor</span><span class="p" data-group-id="2176729903-21">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="2176729903-22">[</span><span class="mi">2</span><span class="p" data-group-id="2176729903-22">]</span><span class="p" data-group-id="2176729903-23">[</span><span class="mi">3</span><span class="p" data-group-id="2176729903-23">]</span><span class="p" data-group-id="2176729903-24">[</span><span class="mi">2</span><span class="p" data-group-id="2176729903-24">]</span><span class="w">
  </span><span class="p" data-group-id="2176729903-25">[</span><span class="w">
    </span><span class="p" data-group-id="2176729903-26">[</span><span class="w">
      </span><span class="p" data-group-id="2176729903-27">[</span><span class="o">-</span><span class="mf">0.24591797590255737</span><span class="p">,</span><span class="w"> </span><span class="mf">3.08001708984375</span><span class="p" data-group-id="2176729903-27">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="2176729903-28">[</span><span class="o">-</span><span class="mf">0.1704912781715393</span><span class="p">,</span><span class="w"> </span><span class="mf">0.6029025316238403</span><span class="p" data-group-id="2176729903-28">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="2176729903-29">[</span><span class="mf">0.9496372938156128</span><span class="p">,</span><span class="w"> </span><span class="mf">2.80519962310791</span><span class="p" data-group-id="2176729903-29">]</span><span class="w">
    </span><span class="p" data-group-id="2176729903-26">]</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="2176729903-30">[</span><span class="w">
      </span><span class="p" data-group-id="2176729903-31">[</span><span class="mf">0.7885514497756958</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">3.0088953971862793</span><span class="p" data-group-id="2176729903-31">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="2176729903-32">[</span><span class="mf">0.9677201509475708</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.4984228312969208</span><span class="p" data-group-id="2176729903-32">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="2176729903-33">[</span><span class="mf">2.207162380218506</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.3534282445907593</span><span class="p" data-group-id="2176729903-33">]</span><span class="w">
    </span><span class="p" data-group-id="2176729903-30">]</span><span class="w">
  </span><span class="p" data-group-id="2176729903-25">]</span><span class="w">
</span><span class="p" data-group-id="2176729903-21">&gt;</span></code></pre><h3 id="conv/4-two-dimensional-convolution" class="section-heading">
  <a href="#conv/4-two-dimensional-convolution" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Two-dimensional convolution</span>
</h3>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="5161122192-1">(</span><span class="p" data-group-id="5161122192-2">[</span><span class="p" data-group-id="5161122192-3">[</span><span class="p" data-group-id="5161122192-4">[</span><span class="p" data-group-id="5161122192-5">[</span><span class="o">-</span><span class="mf">1.0476</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.5041</span><span class="p" data-group-id="5161122192-5">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5161122192-6">[</span><span class="o">-</span><span class="mf">0.9336</span><span class="p">,</span><span class="w"> </span><span class="mf">1.5907</span><span class="p" data-group-id="5161122192-6">]</span><span class="p" data-group-id="5161122192-4">]</span><span class="p" data-group-id="5161122192-3">]</span><span class="p" data-group-id="5161122192-2">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="5161122192-7">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="5161122192-7">}</span><span class="p" data-group-id="5161122192-1">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="5161122192-8">(</span><span class="p" data-group-id="5161122192-9">[</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="5161122192-10">[</span><span class="p" data-group-id="5161122192-11">[</span><span class="p" data-group-id="5161122192-12">[</span><span class="mf">0.7514</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7356</span><span class="p" data-group-id="5161122192-12">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5161122192-13">[</span><span class="mf">1.3909</span><span class="p">,</span><span class="w">  </span><span class="mf">0.6800</span><span class="p" data-group-id="5161122192-13">]</span><span class="p" data-group-id="5161122192-11">]</span><span class="p" data-group-id="5161122192-10">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="5161122192-14">[</span><span class="p" data-group-id="5161122192-15">[</span><span class="p" data-group-id="5161122192-16">[</span><span class="o">-</span><span class="mf">0.3450</span><span class="p">,</span><span class="w">  </span><span class="mf">0.4551</span><span class="p" data-group-id="5161122192-16">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5161122192-17">[</span><span class="o">-</span><span class="mf">0.6275</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.9875</span><span class="p" data-group-id="5161122192-17">]</span><span class="p" data-group-id="5161122192-15">]</span><span class="p" data-group-id="5161122192-14">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="5161122192-18">[</span><span class="p" data-group-id="5161122192-19">[</span><span class="p" data-group-id="5161122192-20">[</span><span class="mf">1.8587</span><span class="p">,</span><span class="w"> </span><span class="mf">0.4722</span><span class="p" data-group-id="5161122192-20">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5161122192-21">[</span><span class="mf">0.6058</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">1.0301</span><span class="p" data-group-id="5161122192-21">]</span><span class="p" data-group-id="5161122192-19">]</span><span class="p" data-group-id="5161122192-18">]</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="p" data-group-id="5161122192-9">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="5161122192-22">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="5161122192-22">}</span><span class="p" data-group-id="5161122192-8">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="5161122192-23">(</span><span class="p" data-group-id="5161122192-24">[</span><span class="mf">1.9564</span><span class="p">,</span><span class="w"> </span><span class="mf">0.2822</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.5385</span><span class="p" data-group-id="5161122192-24">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="5161122192-25">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="5161122192-25">}</span><span class="p" data-group-id="5161122192-23">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">conv</span><span class="p" data-group-id="5161122192-26">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="5161122192-26">)</span><span class="w">
</span><span class="p" data-group-id="5161122192-27">#</span><span class="nc" data-group-id="5161122192-27">Nx.Tensor</span><span class="p" data-group-id="5161122192-27">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="5161122192-28">[</span><span class="mi">1</span><span class="p" data-group-id="5161122192-28">]</span><span class="p" data-group-id="5161122192-29">[</span><span class="mi">3</span><span class="p" data-group-id="5161122192-29">]</span><span class="p" data-group-id="5161122192-30">[</span><span class="mi">1</span><span class="p" data-group-id="5161122192-30">]</span><span class="p" data-group-id="5161122192-31">[</span><span class="mi">1</span><span class="p" data-group-id="5161122192-31">]</span><span class="w">
  </span><span class="p" data-group-id="5161122192-32">[</span><span class="w">
    </span><span class="p" data-group-id="5161122192-33">[</span><span class="w">
      </span><span class="p" data-group-id="5161122192-34">[</span><span class="w">
        </span><span class="p" data-group-id="5161122192-35">[</span><span class="mf">0.5815491676330566</span><span class="p" data-group-id="5161122192-35">]</span><span class="w">
      </span><span class="p" data-group-id="5161122192-34">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="5161122192-36">[</span><span class="w">
        </span><span class="p" data-group-id="5161122192-37">[</span><span class="o">-</span><span class="mf">0.5707762241363525</span><span class="p" data-group-id="5161122192-37">]</span><span class="w">
      </span><span class="p" data-group-id="5161122192-36">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="5161122192-38">[</span><span class="w">
        </span><span class="p" data-group-id="5161122192-39">[</span><span class="o">-</span><span class="mf">4.927865028381348</span><span class="p" data-group-id="5161122192-39">]</span><span class="w">
      </span><span class="p" data-group-id="5161122192-38">]</span><span class="w">
    </span><span class="p" data-group-id="5161122192-33">]</span><span class="w">
  </span><span class="p" data-group-id="5161122192-32">]</span><span class="w">
</span><span class="p" data-group-id="5161122192-27">&gt;</span></code></pre><h3 id="conv/4-three-dimensional-convolution" class="section-heading">
  <a href="#conv/4-three-dimensional-convolution" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Three-dimensional convolution</span>
</h3>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="5456332519-1">(</span><span class="p" data-group-id="5456332519-2">[</span><span class="p" data-group-id="5456332519-3">[</span><span class="p" data-group-id="5456332519-4">[</span><span class="p" data-group-id="5456332519-5">[</span><span class="p" data-group-id="5456332519-6">[</span><span class="o">-</span><span class="mf">0.6497</span><span class="p" data-group-id="5456332519-6">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5456332519-7">[</span><span class="mf">1.0939</span><span class="p" data-group-id="5456332519-7">]</span><span class="p" data-group-id="5456332519-5">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5456332519-8">[</span><span class="p" data-group-id="5456332519-9">[</span><span class="o">-</span><span class="mf">2.5465</span><span class="p" data-group-id="5456332519-9">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5456332519-10">[</span><span class="mf">0.7801</span><span class="p" data-group-id="5456332519-10">]</span><span class="p" data-group-id="5456332519-8">]</span><span class="p" data-group-id="5456332519-4">]</span><span class="p" data-group-id="5456332519-3">]</span><span class="p" data-group-id="5456332519-2">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="5456332519-11">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="5456332519-11">}</span><span class="p" data-group-id="5456332519-1">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="5456332519-12">(</span><span class="p" data-group-id="5456332519-13">[</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="5456332519-14">[</span><span class="p" data-group-id="5456332519-15">[</span><span class="p" data-group-id="5456332519-16">[</span><span class="p" data-group-id="5456332519-17">[</span><span class="w"> </span><span class="mf">0.7390</span><span class="p" data-group-id="5456332519-17">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5456332519-18">[</span><span class="o">-</span><span class="mf">0.0927</span><span class="p" data-group-id="5456332519-18">]</span><span class="p" data-group-id="5456332519-16">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5456332519-19">[</span><span class="p" data-group-id="5456332519-20">[</span><span class="o">-</span><span class="mf">0.8675</span><span class="p" data-group-id="5456332519-20">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5456332519-21">[</span><span class="o">-</span><span class="mf">0.9209</span><span class="p" data-group-id="5456332519-21">]</span><span class="p" data-group-id="5456332519-19">]</span><span class="p" data-group-id="5456332519-15">]</span><span class="p" data-group-id="5456332519-14">]</span><span class="p">,</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="w"> </span><span class="p" data-group-id="5456332519-22">[</span><span class="p" data-group-id="5456332519-23">[</span><span class="p" data-group-id="5456332519-24">[</span><span class="p" data-group-id="5456332519-25">[</span><span class="o">-</span><span class="mf">0.6638</span><span class="p" data-group-id="5456332519-25">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5456332519-26">[</span><span class="mf">0.4341</span><span class="p" data-group-id="5456332519-26">]</span><span class="p" data-group-id="5456332519-24">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5456332519-27">[</span><span class="p" data-group-id="5456332519-28">[</span><span class="mf">0.6368</span><span class="p" data-group-id="5456332519-28">]</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="5456332519-29">[</span><span class="mf">1.1846</span><span class="p" data-group-id="5456332519-29">]</span><span class="p" data-group-id="5456332519-27">]</span><span class="p" data-group-id="5456332519-23">]</span><span class="p" data-group-id="5456332519-22">]</span><span class="w">
</span><span class="gp unselectable">...&gt; </span><span class="p" data-group-id="5456332519-13">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="5456332519-30">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="5456332519-30">}</span><span class="p" data-group-id="5456332519-12">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="5456332519-31">(</span><span class="p" data-group-id="5456332519-32">[</span><span class="o">-</span><span class="mf">0.4101</span><span class="p">,</span><span class="w">  </span><span class="mf">0.1776</span><span class="p" data-group-id="5456332519-32">]</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="5456332519-33">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="5456332519-33">}</span><span class="p" data-group-id="5456332519-31">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">conv</span><span class="p" data-group-id="5456332519-34">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="5456332519-34">)</span><span class="w">
</span><span class="p" data-group-id="5456332519-35">#</span><span class="nc" data-group-id="5456332519-35">Nx.Tensor</span><span class="p" data-group-id="5456332519-35">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="5456332519-36">[</span><span class="mi">1</span><span class="p" data-group-id="5456332519-36">]</span><span class="p" data-group-id="5456332519-37">[</span><span class="mi">2</span><span class="p" data-group-id="5456332519-37">]</span><span class="p" data-group-id="5456332519-38">[</span><span class="mi">1</span><span class="p" data-group-id="5456332519-38">]</span><span class="p" data-group-id="5456332519-39">[</span><span class="mi">1</span><span class="p" data-group-id="5456332519-39">]</span><span class="p" data-group-id="5456332519-40">[</span><span class="mi">1</span><span class="p" data-group-id="5456332519-40">]</span><span class="w">
  </span><span class="p" data-group-id="5456332519-41">[</span><span class="w">
    </span><span class="p" data-group-id="5456332519-42">[</span><span class="w">
      </span><span class="p" data-group-id="5456332519-43">[</span><span class="w">
        </span><span class="p" data-group-id="5456332519-44">[</span><span class="w">
          </span><span class="p" data-group-id="5456332519-45">[</span><span class="mf">0.49906185269355774</span><span class="p" data-group-id="5456332519-45">]</span><span class="w">
        </span><span class="p" data-group-id="5456332519-44">]</span><span class="w">
      </span><span class="p" data-group-id="5456332519-43">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="5456332519-46">[</span><span class="w">
        </span><span class="p" data-group-id="5456332519-47">[</span><span class="w">
          </span><span class="p" data-group-id="5456332519-48">[</span><span class="mf">0.38622811436653137</span><span class="p" data-group-id="5456332519-48">]</span><span class="w">
        </span><span class="p" data-group-id="5456332519-47">]</span><span class="w">
      </span><span class="p" data-group-id="5456332519-46">]</span><span class="w">
    </span><span class="p" data-group-id="5456332519-42">]</span><span class="w">
  </span><span class="p" data-group-id="5456332519-41">]</span><span class="w">
</span><span class="p" data-group-id="5456332519-35">&gt;</span></code></pre>
  </section>
</section>
<section class="detail" id="conv_transpose/4">

    <span id="conv_transpose/2"></span>

    <span id="conv_transpose/3"></span>

  <div class="detail-header">
    <a href="#conv_transpose/4" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">conv_transpose(input, kernel, bias \\ 0, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L443" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a general dimensional transposed
convolutional layer.</p><p><em>Note: This layer is currently implemented as a fractionally strided
convolution by padding the input tensor. Please open an issue if you'd
like this behavior changed.</em></p><p>Transposed convolutions are sometimes (incorrectly) referred to as
deconvolutions because it &quot;reverses&quot; the spatial dimensions
of a normal convolution. Transposed convolutions are a form of upsampling -
they produce larger spatial dimensions than the input tensor. They
can be thought of as a convolution in reverse - and are sometimes
implemented as the backward pass of a normal convolution.</p><h2 id="conv_transpose/4-options" class="section-heading">
  <a href="#conv_transpose/4-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:strides</code> - kernel strides. Can be a scalar or a list
whose length matches the number of spatial dimensions in
the input tensor. Defaults to 1.</p></li><li><p><code class="inline">:padding</code> - zero padding on the input. Can be one of
<code class="inline">:valid</code>, <code class="inline">:same</code> or a general padding configuration
without interior padding for each spatial dimension
of the input.</p></li><li><p><code class="inline">:input_dilation</code> - input dilation factor. Equivalent
to applying interior padding on the input. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Defaults to <code class="inline">1</code> or no dilation.</p></li><li><p><code class="inline">:kernel_dilation</code> - kernel dilation factor. Equivalent
to applying interior padding on the kernel. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Defaults to <code class="inline">1</code> or no dilation.</p></li><li><p><code class="inline">:channels</code> - channel configuration. One of <code class="inline">:first</code> or <code class="inline">:last</code>.
Defaults to <code class="inline">:last</code>.</p></li></ul><h2 id="conv_transpose/4-examples" class="section-heading">
  <a href="#conv_transpose/4-examples" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Examples</span>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="gp unselectable">iex&gt; </span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="0081901490-1">(</span><span class="p" data-group-id="0081901490-2">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p" data-group-id="0081901490-2">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="0081901490-3">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="0081901490-3">}</span><span class="p" data-group-id="0081901490-1">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">iota</span><span class="p" data-group-id="0081901490-4">(</span><span class="p" data-group-id="0081901490-5">{</span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p" data-group-id="0081901490-5">}</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="0081901490-6">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="0081901490-6">}</span><span class="p" data-group-id="0081901490-4">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="n">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">tensor</span><span class="p" data-group-id="0081901490-7">(</span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="ss">type</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="0081901490-8">{</span><span class="ss">:f</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p" data-group-id="0081901490-8">}</span><span class="p" data-group-id="0081901490-7">)</span><span class="w">
</span><span class="gp unselectable">iex&gt; </span><span class="nc">Axon.Layers</span><span class="o">.</span><span class="n">conv_transpose</span><span class="p" data-group-id="0081901490-9">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="ss">channels</span><span class="p">:</span><span class="w"> </span><span class="ss">:first</span><span class="p" data-group-id="0081901490-9">)</span><span class="w">
</span><span class="p" data-group-id="0081901490-10">#</span><span class="nc" data-group-id="0081901490-10">Nx.Tensor</span><span class="p" data-group-id="0081901490-10">&lt;</span><span class="w">
  </span><span class="n">f32</span><span class="p" data-group-id="0081901490-11">[</span><span class="mi">1</span><span class="p" data-group-id="0081901490-11">]</span><span class="p" data-group-id="0081901490-12">[</span><span class="mi">6</span><span class="p" data-group-id="0081901490-12">]</span><span class="p" data-group-id="0081901490-13">[</span><span class="mi">4</span><span class="p" data-group-id="0081901490-13">]</span><span class="w">
  </span><span class="p" data-group-id="0081901490-14">[</span><span class="w">
    </span><span class="p" data-group-id="0081901490-15">[</span><span class="w">
      </span><span class="p" data-group-id="0081901490-16">[</span><span class="mf">40.0</span><span class="p">,</span><span class="w"> </span><span class="mf">79.0</span><span class="p">,</span><span class="w"> </span><span class="mf">94.0</span><span class="p">,</span><span class="w"> </span><span class="mf">43.0</span><span class="p" data-group-id="0081901490-16">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="0081901490-17">[</span><span class="mf">94.0</span><span class="p">,</span><span class="w"> </span><span class="mf">205.0</span><span class="p">,</span><span class="w"> </span><span class="mf">256.0</span><span class="p">,</span><span class="w"> </span><span class="mf">133.0</span><span class="p" data-group-id="0081901490-17">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="0081901490-18">[</span><span class="mf">148.0</span><span class="p">,</span><span class="w"> </span><span class="mf">331.0</span><span class="p">,</span><span class="w"> </span><span class="mf">418.0</span><span class="p">,</span><span class="w"> </span><span class="mf">223.0</span><span class="p" data-group-id="0081901490-18">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="0081901490-19">[</span><span class="mf">202.0</span><span class="p">,</span><span class="w"> </span><span class="mf">457.0</span><span class="p">,</span><span class="w"> </span><span class="mf">580.0</span><span class="p">,</span><span class="w"> </span><span class="mf">313.0</span><span class="p" data-group-id="0081901490-19">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="0081901490-20">[</span><span class="mf">256.0</span><span class="p">,</span><span class="w"> </span><span class="mf">583.0</span><span class="p">,</span><span class="w"> </span><span class="mf">742.0</span><span class="p">,</span><span class="w"> </span><span class="mf">403.0</span><span class="p" data-group-id="0081901490-20">]</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="0081901490-21">[</span><span class="mf">310.0</span><span class="p">,</span><span class="w"> </span><span class="mf">709.0</span><span class="p">,</span><span class="w"> </span><span class="mf">904.0</span><span class="p">,</span><span class="w"> </span><span class="mf">493.0</span><span class="p" data-group-id="0081901490-21">]</span><span class="w">
    </span><span class="p" data-group-id="0081901490-15">]</span><span class="w">
  </span><span class="p" data-group-id="0081901490-14">]</span><span class="w">
</span><span class="p" data-group-id="0081901490-10">&gt;</span></code></pre><h2 id="conv_transpose/4-references" class="section-heading">
  <a href="#conv_transpose/4-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="https://arxiv.org/abs/1603.07285v1">A guide to convolution arithmetic for deep learning</a></li><li><a href="https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf">Deconvolutional Networks</a></li></ul>
  </section>
</section>
<section class="detail" id="depthwise_conv/4">

    <span id="depthwise_conv/2"></span>

    <span id="depthwise_conv/3"></span>

  <div class="detail-header">
    <a href="#depthwise_conv/4" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">depthwise_conv(inputs, kernel, bias \\ 0, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L542" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a general dimensional depthwise
convolution.</p><p>Depthwise convolutions apply a single convolutional filter to
each input channel. This is done by setting <code class="inline">feature_group_size</code>
equal to the number of input channels. This will split the
<code class="inline">output_channels</code> into <code class="inline">input_channels</code> number of groups and
convolve the grouped kernel channels over the corresponding input
channel.</p><h2 id="depthwise_conv/4-parameter-shapes" class="section-heading">
  <a href="#depthwise_conv/4-parameter-shapes" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Parameter Shapes</span>
</h2>
<ul><li><code class="inline">input</code> - <code class="inline">{batch_size, input_channels, input_spatial0, ..., input_spatialN}</code></li><li><code class="inline">kernel</code> - <code class="inline">{output_channels, 1, kernel_spatial0, ..., kernel_spatialN}</code></li><li><code class="inline">bias</code> - <code class="inline">{output_channels}</code> or <code class="inline">{}</code></li></ul><p>  <code class="inline">output_channels</code> must be a multiple of the input channels.</p><h2 id="depthwise_conv/4-options" class="section-heading">
  <a href="#depthwise_conv/4-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:strides</code> - kernel strides. Can be a scalar or a list
whose length matches the number of spatial dimensions in
the input tensor. Defaults to 1.</p></li><li><p><code class="inline">:padding</code> - zero padding on the input. Can be one of
<code class="inline">:valid</code>, <code class="inline">:same</code> or a general padding configuration
without interior padding for each spatial dimension
of the input.</p></li><li><p><code class="inline">:input_dilation</code> - input dilation factor. Equivalent
to applying interior padding on the input. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Defaults to <code class="inline">1</code> or no dilation.</p></li><li><p><code class="inline">:kernel_dilation</code> - kernel dilation factor. Equivalent
to applying interior padding on the kernel. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Defaults to <code class="inline">1</code> or no dilation.</p></li><li><p><code class="inline">:channels</code> - channel configuration. One of <code class="inline">:first</code> or <code class="inline">:last</code>.
Defaults to <code class="inline">:last</code>.</p></li></ul>
  </section>
</section>
<section class="detail" id="separable_conv2d/6">

    <span id="separable_conv2d/5"></span>

  <div class="detail-header">
    <a href="#separable_conv2d/6" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">separable_conv2d(input, k1, b1, k2, b2, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L637" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a 2-dimensional separable depthwise
convolution.</p><p>The 2-d depthwise separable convolution performs 2 depthwise convolutions
each over 1 spatial dimension of the input.</p><h2 id="separable_conv2d/6-parameter-shapes" class="section-heading">
  <a href="#separable_conv2d/6-parameter-shapes" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Parameter Shapes</span>
</h2>
<ul><li><code class="inline">input</code> - <code class="inline">{batch_size, input_channels, input_spatial0, ..., input_spatialN}</code></li><li><code class="inline">k1</code> - <code class="inline">{output_channels, 1, kernel_spatial0, 1}</code></li><li><code class="inline">b1</code> - <code class="inline">{output_channels}</code> or <code class="inline">{}</code></li><li><code class="inline">k2</code> - <code class="inline">{output_channels, 1, 1, kernel_spatial1}</code></li><li><code class="inline">b2</code> - <code class="inline">{output_channels}</code> or <code class="inline">{}</code></li></ul><p>  <code class="inline">output_channels</code> must be a multiple of the input channels.</p><h2 id="separable_conv2d/6-options" class="section-heading">
  <a href="#separable_conv2d/6-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:strides</code> - kernel strides. Can be a scalar or a list
whose length matches the number of spatial dimensions in
the input tensor. Defaults to 1.</p></li><li><p><code class="inline">:padding</code> - zero padding on the input. Can be one of
<code class="inline">:valid</code>, <code class="inline">:same</code> or a general padding configuration
without interior padding for each spatial dimension
of the input.</p></li><li><p><code class="inline">:input_dilation</code> - input dilation factor. Equivalent
to applying interior padding on the input. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Defaults to <code class="inline">1</code> or no dilation.</p></li><li><p><code class="inline">:kernel_dilation</code> - kernel dilation factor. Equivalent
to applying interior padding on the kernel. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Defaults to <code class="inline">1</code> or no dilation.</p></li><li><p><code class="inline">:channels</code> - channel configuration. One of <code class="inline">:first</code> or <code class="inline">:last</code>.
Defaults to <code class="inline">:last</code>.</p></li></ul><h2 id="separable_conv2d/6-references" class="section-heading">
  <a href="#separable_conv2d/6-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="https://arxiv.org/abs/1610.02357">Xception: Deep Learning with Depthwise Separable Convolutions</a></li></ul>
  </section>
</section>
<section class="detail" id="separable_conv3d/8">

    <span id="separable_conv3d/7"></span>

  <div class="detail-header">
    <a href="#separable_conv3d/8" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">separable_conv3d(input, k1, b1, k2, b2, k3, b3, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L704" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Functional implementation of a 3-dimensional separable depthwise
convolution.</p><p>The 3-d depthwise separable convolution performs 3 depthwise convolutions
each over 1 spatial dimension of the input.</p><h2 id="separable_conv3d/8-parameter-shapes" class="section-heading">
  <a href="#separable_conv3d/8-parameter-shapes" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Parameter Shapes</span>
</h2>
<ul><li><code class="inline">input</code> - <code class="inline">{batch_size, input_channels, input_spatial0, input_spatial1, input_spatial2}</code></li><li><code class="inline">k1</code> - <code class="inline">{output_channels, 1, kernel_spatial0, 1, 1}</code></li><li><code class="inline">b1</code> - <code class="inline">{output_channels}</code> or <code class="inline">{}</code></li><li><code class="inline">k2</code> - <code class="inline">{output_channels, 1, 1, kernel_spatial1, 1}</code></li><li><code class="inline">b2</code> - <code class="inline">{output_channels}</code> or <code class="inline">{}</code></li><li><code class="inline">k3</code> - <code class="inline">{output_channels, 1, 1, 1, 1, kernel_spatial2}</code></li><li><code class="inline">b3</code> - <code class="inline">{output_channels}</code> or <code class="inline">{}</code></li></ul><p>  <code class="inline">output_channels</code> must be a multiple of the input channels.</p><h2 id="separable_conv3d/8-options" class="section-heading">
  <a href="#separable_conv3d/8-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:strides</code> - kernel strides. Can be a scalar or a list
whose length matches the number of spatial dimensions in
the input tensor. Defaults to 1.</p></li><li><p><code class="inline">:padding</code> - zero padding on the input. Can be one of
<code class="inline">:valid</code>, <code class="inline">:same</code> or a general padding configuration
without interior padding for each spatial dimension
of the input.</p></li><li><p><code class="inline">:input_dilation</code> - input dilation factor. Equivalent
to applying interior padding on the input. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Defaults to <code class="inline">1</code> or no dilation.</p></li><li><p><code class="inline">:kernel_dilation</code> - kernel dilation factor. Equivalent
to applying interior padding on the kernel. The amount
of interior padding applied is given by <code class="inline">kernel_dilation - 1</code>.
Defaults to <code class="inline">1</code> or no dilation.</p></li><li><p><code class="inline">:channels</code> - channel configuration. One of <code class="inline">:first</code> or <code class="inline">:last</code>.
Defaults to <code class="inline">:last</code>.</p></li></ul><h2 id="separable_conv3d/8-references" class="section-heading">
  <a href="#separable_conv3d/8-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="https://arxiv.org/abs/1610.02357">Xception: Deep Learning with Depthwise Separable Convolutions</a></li></ul>
  </section>
</section>

    </div>
  </section>

  <section id="functions" class="details-list">
    <h1 class="section-heading">
      <a class="hover-link" href="#functions">
        <i class="ri-link-m" aria-hidden="true"></i>
      </a>
      <span class="text">Functions</span>
    </h1>
    <div class="functions-list">
<section class="detail" id="celu/2">

    <span id="celu/1"></span>

  <div class="detail-header">
    <a href="#celu/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">celu(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2163" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="conv_lstm/7">

    <span id="conv_lstm/5"></span>

    <span id="conv_lstm/6"></span>

  <div class="detail-header">
    <a href="#conv_lstm/7" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">conv_lstm(input, hidden_state, mask, input_kernel, hidden_kernel, bias \\ [], opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2473" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="conv_lstm_cell/7">

    <span id="conv_lstm_cell/6"></span>

  <div class="detail-header">
    <a href="#conv_lstm_cell/7" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">conv_lstm_cell(input, carry, arg3, ih, hh, bi, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2281" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>ConvLSTM Cell.</p><p>When combined with <code class="inline">Axon.Layers.*_unroll</code>, implements a
ConvLSTM-based RNN. More memory efficient than traditional LSTM.</p><h2 id="conv_lstm_cell/7-options" class="section-heading">
  <a href="#conv_lstm_cell/7-options" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">Options</span>
</h2>
<ul><li><p><code class="inline">:strides</code> - convolution strides. Defaults to <code class="inline">1</code>.</p></li><li><p><code class="inline">:padding</code> - convolution padding. Defaults to <code class="inline">:same</code>.</p></li></ul><h2 id="conv_lstm_cell/7-references" class="section-heading">
  <a href="#conv_lstm_cell/7-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="https://arxiv.org/abs/1506.04214">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</a></li></ul>
  </section>
</section>
<section class="detail" id="dynamic_unroll/7">

  <div class="detail-header">
    <a href="#dynamic_unroll/7" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">dynamic_unroll(cell_fn, input_sequence, carry, mask, input_kernel, recurrent_kernel, bias)</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2348" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Dynamically unrolls an RNN.</p><p>Unrolls implement a <code class="inline">scan</code> operation which applies a
transformation on the leading axis of <code class="inline">input_sequence</code> carrying
some state. In this instance <code class="inline">cell_fn</code> is an RNN cell function
such as <code class="inline">lstm_cell</code> or <code class="inline">gru_cell</code>.</p><p>This function will make use of an <code class="inline">defn</code> while-loop such and thus
may be more efficient for long sequences.</p>
  </section>
</section>
<section class="detail" id="elu/2">

    <span id="elu/1"></span>

  <div class="detail-header">
    <a href="#elu/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">elu(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2163" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="gru/7">

    <span id="gru/5"></span>

    <span id="gru/6"></span>

  <div class="detail-header">
    <a href="#gru/7" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">gru(input, hidden_state, mask, input_kernel, hidden_kernel, bias \\ [], opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2473" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="gru_cell/8">

    <span id="gru_cell/6"></span>

    <span id="gru_cell/7"></span>

  <div class="detail-header">
    <a href="#gru_cell/8" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">gru_cell(input, carry, mask, arg4, arg5, arg6, gate_fn \\ &amp;Axon.Activations.sigmoid/1, activation_fn \\ &amp;Axon.Activations.tanh/1)</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2204" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>GRU Cell.</p><p>When combined with <code class="inline">Axon.Layers.*_unroll</code>, implements a
GRU-based RNN. More memory efficient than traditional LSTM.</p><h2 id="gru_cell/8-references" class="section-heading">
  <a href="#gru_cell/8-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="https://arxiv.org/pdf/1412.3555v1.pdf">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></li></ul>
  </section>
</section>
<section class="detail" id="hard_sigmoid/2">

    <span id="hard_sigmoid/1"></span>

  <div class="detail-header">
    <a href="#hard_sigmoid/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">hard_sigmoid(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2163" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="hard_silu/2">

    <span id="hard_silu/1"></span>

  <div class="detail-header">
    <a href="#hard_silu/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">hard_silu(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2163" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="leaky_relu/2">

    <span id="leaky_relu/1"></span>

  <div class="detail-header">
    <a href="#leaky_relu/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">leaky_relu(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2163" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="log_softmax/2">

    <span id="log_softmax/1"></span>

  <div class="detail-header">
    <a href="#log_softmax/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">log_softmax(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2163" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="log_sumexp/2">

    <span id="log_sumexp/1"></span>

  <div class="detail-header">
    <a href="#log_sumexp/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">log_sumexp(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2163" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="lstm/7">

    <span id="lstm/5"></span>

    <span id="lstm/6"></span>

  <div class="detail-header">
    <a href="#lstm/7" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">lstm(input, hidden_state, mask, input_kernel, hidden_kernel, bias \\ [], opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2473" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="lstm_cell/8">

    <span id="lstm_cell/6"></span>

    <span id="lstm_cell/7"></span>

  <div class="detail-header">
    <a href="#lstm_cell/8" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">lstm_cell(input, carry, mask, arg4, arg5, arg6, gate_fn \\ &amp;Axon.Activations.sigmoid/1, activation_fn \\ &amp;Axon.Activations.tanh/1)</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2238" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>LSTM Cell.</p><p>When combined with <code class="inline">Axon.Layers.*_unroll</code>, implements a
LSTM-based RNN. More memory efficient than traditional LSTM.</p><h2 id="lstm_cell/8-references" class="section-heading">
  <a href="#lstm_cell/8-references" class="hover-link">
    <i class="ri-link-m" aria-hidden="true"></i>
  </a>
  <span class="text">References</span>
</h2>
<ul><li><a href="http://www.bioinf.jku.at/publications/older/2604.pdf">Long Short-Term Memory</a></li></ul>
  </section>
</section>
<section class="detail" id="multiply/2">

    <span id="multiply/1"></span>

  <div class="detail-header">
    <a href="#multiply/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">multiply(inputs, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2174" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="padding_config_transform/2">

  <div class="detail-header">
    <a href="#padding_config_transform/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">padding_config_transform(config, channels)</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L1853" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="selu/2">

    <span id="selu/1"></span>

  <div class="detail-header">
    <a href="#selu/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">selu(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2163" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="softmax/2">

    <span id="softmax/1"></span>

  <div class="detail-header">
    <a href="#softmax/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">softmax(input, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2163" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>
<section class="detail" id="static_unroll/7">

  <div class="detail-header">
    <a href="#static_unroll/7" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">static_unroll(cell_fn, input_sequence, carry, mask, input_kernel, recurrent_kernel, bias)</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2423" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">

<p>Statically unrolls an RNN.</p><p>Unrolls implement a <code class="inline">scan</code> operation which applies a
transformation on the leading axis of <code class="inline">input_sequence</code> carrying
some state. In this instance <code class="inline">cell_fn</code> is an RNN cell function
such as <code class="inline">lstm_cell</code> or <code class="inline">gru_cell</code>.</p><p>This function inlines the unrolling of the sequence such that
the entire operation appears as a part of the compilation graph.
This makes it suitable for shorter sequences.</p>
  </section>
</section>
<section class="detail" id="subtract/2">

    <span id="subtract/1"></span>

  <div class="detail-header">
    <a href="#subtract/2" class="detail-link" title="Link to this function">
      <i class="ri-link-m" aria-hidden="true"></i>
      <span class="sr-only">Link to this function</span>
    </a>
    <h1 class="signature" translate="no">subtract(inputs, opts \\ [])</h1>

      <a href="https://github.com/elixir-nx/axon/blob/v0.6.1/lib/axon/layers.ex#L2174" class="icon-action" rel="help" title="View Source">
       <i class="ri-code-s-slash-line" aria-hidden="true"></i>
       <span class="sr-only">View Source</span>
     </a>


  </div>

  <section class="docstring">


  </section>
</section>

    </div>
  </section>

      <footer class="footer">
        <p>

            <span class="line">
              <a href="https://hex.pm/packages/axon/0.6.1" class="footer-hex-package">Hex Package</a>

              <a href="https://preview.hex.pm/preview/axon/0.6.1">Hex Preview</a>

            </span>

          <span class="line">
            <button class="a-main footer-button display-quick-switch" title="Search HexDocs packages">
              Search HexDocs
            </button>

              <a href="Axon.epub" title="ePub version">
                Download ePub version
              </a>

          </span>
        </p>

        <p class="built-using">
          Built using
          <a href="https://github.com/elixir-lang/ex_doc" title="ExDoc" target="_blank" rel="help noopener" translate="no">ExDoc</a> (v0.31.1) for the

            <a href="https://elixir-lang.org" title="Elixir" target="_blank" translate="no">Elixir programming language</a>

        </p>
      </footer>
    </div>
  </div>
</main>
</div>

<!-- Render math with KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js" integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB+w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
      ]
    });
  });
</script>

<!-- Render diagrams with Mermaid -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@8.13.3/dist/mermaid.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    mermaid.initialize({ startOnLoad: false });
    let id = 0;
    for (const codeEl of document.querySelectorAll("pre code.mermaid")) {
      const preEl = codeEl.parentElement;
      const graphDefinition = codeEl.textContent;
      const graphEl = document.createElement("div");
      const graphId = "mermaid-graph-" + id++;
      mermaid.render(graphId, graphDefinition, function (svgSource, bindListeners) {
        graphEl.innerHTML = svgSource;
        bindListeners && bindListeners(graphEl);
        preEl.insertAdjacentElement("afterend", graphEl);
        preEl.remove();
      });
    }
  });
</script>

  </body>
</html>
