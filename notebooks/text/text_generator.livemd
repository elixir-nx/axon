# Generating text with an LSTM

```elixir
Mix.install([
  {:axon, github: "elixir-nx/axon"},
  {:nx, "~> 0.2.1"},
  {:exla, "~> 0.2.2"},
  {:req, "~> 0.3.0"}
])

EXLA.set_as_nx_default([:tpu, :cuda, :rocm, :host])
```

<!-- livebook:{"output":true} -->

```
:cuda
```

## Recurrent Neural Networks as generative models

Recurrent neural networks can also be used as generative models.

This means that in addition to being used for predictive models (making predictions) they can learn the sequences of a problem and then generate entirely new plausible sequences for the problem domain.

Generative models like this are useful not only to study how well a model has learned a problem, but to learn more about the problem domain itself.

In this example, we will discover how to create a generative model for text, character-by-character using LSTM recurrent neural networks in Elixir with Axon.

## Preparation

Using [Project Gutenburg](https://www.gutenberg.org/) we can download a text book which is no longer protected under copywrite so we can experiment with them.

The one that we will use for this experiment is [Alice's Adventures in Wonderland by Lewis Carroll](https://www.gutenberg.org/ebooks/11). You can choose any other text or book that you like for this experiment.

```elixir
# change download_url if you like to experiment with other text books.
download_url = "https://www.gutenberg.org/files/11/11-0.txt"

book_text = Req.get!(download_url).body
```

<!-- livebook:{"output":true} -->

```
"\uFEFFThe Project Gutenberg eBook of Alice’s Adventures in Wonderland, by Lewis Carroll\r\n\r\nThis eBook is for the use of anyone anywhere in the United States and\r\nmost other parts of the world at no cost and with almost no restrictions\r\nwhatsoever. You may copy it, give it away or re-use it under the terms\r\nof the Project Gutenberg License included with this eBook or online at\r\nwww.gutenberg.org. If you are not located in the United States, you\r\nwill have to check the laws of the country where you are located before\r\nusing this eBook.\r\n\r\nTitle: Alice’s Adventures in Wonderland\r\n\r\nAuthor: Lewis Carroll\r\n\r\nRelease Date: January, 1991 [eBook #11]\r\n[Most recently updated: October 12, 2020]\r\n\r\nLanguage: English\r\n\r\nCharacter set encoding: UTF-8\r\n\r\nProduced by: Arthur DiBianca and David Widger\r\n\r\n*** START OF THE PROJECT GUTENBERG EBOOK ALICE’S ADVENTURES IN WONDERLAND ***\r\n\r\n[Illustration]\r\n\r\n\r\n\r\n\r\nAlice’s Adventures in Wonderland\r\n\r\nby Lewis Carroll\r\n\r\nTHE MILLENNIUM FULCRUM EDITION 3.0\r\n\r\nContents\r\n\r\n CHAPTER I.     Down the Rabbit-Hole\r\n CHAPTER II.    The Pool of Tears\r\n CHAPTER III.   A Caucus-Race and a Long Tale\r\n CHAPTER IV.    The Rabbit Sends in a Little Bill\r\n CHAPTER V.     Advice from a Caterpillar\r\n CHAPTER VI.    Pig and Pepper\r\n CHAPTER VII.   A Mad Tea-Party\r\n CHAPTER VIII.  The Queen’s Croquet-Ground\r\n CHAPTER IX.    The Mock Turtle’s Story\r\n CHAPTER X.     The Lobster Quadrille\r\n CHAPTER XI.    Who Stole the Tarts?\r\n CHAPTER XII.   Alice’s Evidence\r\n\r\n\r\n\r\n\r\nCHAPTER I.\r\nDown the Rabbit-Hole\r\n\r\n\r\nAlice was beginning to get very tired of sitting by her sister on the\r\nbank, and of having nothing to do: once or twice she had peeped into\r\nthe book her sister was reading, but it had no pictures or\r\nconversations in it, “and what is the use of a book,” thought Alice\r\n“without pictures or conversations?”\r\n\r\nSo she was considering in her own mind (as well as she could, for the\r\nhot day made her feel very sleepy and stupid), whether the pleasure of\r\nmaking a daisy-chain would be worth the trouble of getting up and\r\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\r\nclose by her.\r\n\r\nThere was nothing so _very_ remarkable in that; nor did Alice think it\r\nso _very_ much out of the way to hear the Rabbit say to itself, “Oh\r\ndear! Oh dear! I shall be late!” (when she thought it over afterwards,\r\nit occurred to her that she ought to have wondered at this, but at the\r\ntime it all seemed quite natural); but when the Rabbit actually _took a\r\nwatch out of its waistcoat-pocket_, and looked at it, and then hurried\r\non, Alice started to her feet, for it flashed across her mind that she\r\nhad never before seen a rabbit with either a waistcoat-pocket, or a\r\nwatch to take out of it, and burning with curiosity, she ran across the\r\nfield after it, and fortunately was just in time to see it pop down a\r\nlarge rabbit-hole under the hedge.\r\n\r\nIn another moment down went Alice after it, never once considering how\r\nin the world she was to get out again.\r\n\r\nThe rabbit-hole went straight on like a tunnel for some way, and then\r\ndipped suddenly down, so suddenly that Alice had not a moment to think\r\nabout stopping herself before she found herself falling down a very\r\ndeep well.\r\n\r\nEither the well was very deep, or she fell very slowly, for she had\r\nplenty of time as she went down to look about her and to wonder what\r\nwas going to happen next. First, she tried to look down and make out\r\nwhat she was coming to, but it was too dark to see anything; then she\r\nlooked at the sides of the well, and noticed that they were filled with\r\ncupboards and book-shelves; here and there she saw maps and pictures\r\nhung upon pegs. She took down a jar from one of the shelves as she\r\npassed; it was labelled “ORANGE MARMALADE”, but to her great\r\ndisappointment it was empty: she did not like to drop the jar for fear\r\nof killing somebody underneath, so managed to put it into one of the\r\ncupboards as she fell past it.\r\n\r\n“Well!” thought Alice to herself, “after such a fall as this, I shall\r\nthink nothing of tumbling down stairs! How brave they’ll all think me" <> ...
```

First of all, we need to normalize the content of the book. We are only interested in the sequence of English characters, periods and new lines. Also currently we don't care about the capitalization and things like apostrophe so we can remove all other unknown characters and downcase everything. We can use a regular expression for that.

We can also convert the string into a list of characters so we can handle them easier. You will understand exactly why a bit further.

```elixir
normalized_book_text =
  book_text
  |> String.downcase()
  |> String.replace(~r/[^a-z \.\n]/, "")
  |> String.to_charlist()
```

<!-- livebook:{"output":true} -->

```
'the project gutenberg ebook of alices adventures in wonderland by lewis carroll\n\nthis ebook is for the use of anyone anywhere in the united states and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. you may copy it give it away or reuse it under the terms\nof the project gutenberg license included with this ebook or online at\nwww.gutenberg.org. if you are not located in the united states you\nwill have to check the laws of the country where you are located before\nusing this ebook.\n\ntitle alices adventures in wonderland\n\nauthor lewis carroll\n\nrelease date january  ebook \nmost recently updated october  \n\nlanguage english\n\ncharacter set encoding utf\n\nproduced by arthur dibianca and david widger\n\n start of the project gutenberg ebook alices adventures in wonderland \n\nillustration\n\n\n\n\nalices adventures in wonderland\n\nby lewis carroll\n\nthe millennium fulcrum edition .\n\ncontents\n\n chapter i.     down the rabbithole\n chapter ii.    the pool of tears\n chapter iii.   a caucusrace and a long tale\n chapter iv.    the rabbit sends in a little bill\n chapter v.     advice from a caterpillar\n chapter vi.    pig and pepper\n chapter vii.   a mad teaparty\n chapter viii.  the queens croquetground\n chapter ix.    the mock turtles story\n chapter x.     the lobster quadrille\n chapter xi.    who stole the tarts\n chapter xii.   alices evidence\n\n\n\n\nchapter i.\ndown the rabbithole\n\n\nalice was beginning to get very tired of sitting by her sister on the\nbank and of having nothing to do once or twice she had peeped into\nthe book her sister was reading but it had no pictures or\nconversations in it and what is the use of a book thought alice\nwithout pictures or conversations\n\nso she was considering in her own mind as well as she could for the\nhot day made her feel very sleepy and stupid whether the pleasure of\nmaking a daisychain would be worth the trouble of getting up and\npicking the daisies when suddenly a white rabbit with pink eyes ran\nclose by her.\n\nthere was nothing so very remarkable in that nor did alice think it\nso very much out of the way to hear the rabbit say to itself oh\ndear oh dear i shall be late when she thought it over afterwards\nit occurred to her that she ought to have wondered at this but at the\ntime it all seemed quite natural but when the rabbit actually took a\nwatch out of its waistcoatpocket and looked at it and then hurried\non alice started to her feet for it flashed across her mind that she\nhad never before seen a rabbit with either a waistcoatpocket or a\nwatch to take out of it and burning with curiosity she ran across the\nfield after it and fortunately was just in time to see it pop down a\nlarge rabbithole under the hedge.\n\nin another moment down went alice after it never once considering how\nin the world she was to get out again.\n\nthe rabbithole went straight on like a tunnel for some way and then\ndipped suddenly down so suddenly that alice had not a moment to think\nabout stopping herself before she found herself falling down a very\ndeep well.\n\neither the well was very deep or she fell very slowly for she had\nplenty of time as she went down to look about her and to wonder what\nwas going to happen next. first she tried to look down and make out\nwhat she was coming to but it was too dark to see anything then she\nlooked at the sides of the well and noticed that they were filled with\ncupboards and bookshelves here and there she saw maps and pictures\nhung upon pegs. she took down a jar from one of the shelves as she\npassed it was labelled orange marmalade but to her great\ndisappointment it was empty she did not like to drop the jar for fear\nof killing somebody underneath so managed to put it into one of the\ncupboards as she fell past it.\n\nwell thought alice to herself after such a fall as this i shall\nthink nothing of tumbling down stairs how brave theyll all think me\nat home why i wouldnt say anything about it even if i fell off the\ntop of the house which was very likely true.\n\ndown down down. would the fall never come to an end i wonder how\nmany miles ive fallen by this time she said aloud. ' ++ ...
```

We converted the text to a list of characters, where each character is a number (specifically, a Unicode code point). Lowercase English characters are represented with numbers between `97 = a` and `122 = z`, a space is `32 = [ ]`, a new line is `10 = \n` and the period is `46 = .`.

So we should have 26 + 3 (= 29) characters in total. Let's see if that's true.

```elixir
Enum.uniq(normalized_book_text) |> Enum.count()
# Yay!
```

<!-- livebook:{"output":true} -->

```
29
```

Since we want to use this 29 characters as possible values for each input in our neural network, We can re-map them to new values between 0 and 28. So each specific neuron will indicate a specific character.

```elixir
# Extract all then unique characters we have. Optionally we can sort them.
characters = normalized_book_text |> Enum.uniq() |> Enum.sort()
characters_count = Enum.count(characters)

# Create a mapping for every character
char_to_idx = Enum.with_index(characters) |> Enum.into(%{})
# And a reverse mapping to convert back to characters
idx_to_char = Enum.with_index(characters, &{&2, &1}) |> Enum.into(%{})

IO.puts("Total Book Characters: #{Enum.count(normalized_book_text)}")
IO.puts("Total Unique Characters: #{characters_count}")
```

<!-- livebook:{"output":true} -->

```
Total Book Characters: 156020
Total Unique Characters: 29
```

<!-- livebook:{"output":true} -->

```
:ok
```

Now we need to create our training and testing data sets. But how?

Our goal is to teach the machine what comes after a sequence of characters (usually). For example given the following sequence **"Hello, My name i"** the computer should be able to guess that the next character is probably **"s"**.

<!-- livebook:{"break_markdown":true} -->

<!-- Learn more at https://mermaid-js.github.io/mermaid -->

```mermaid
graph LR;
  A[Input: Hello my name i]-->NN[Neural Network]-->B[Output: s];
```

<!-- livebook:{"break_markdown":true} -->

Let's choose an arbitrary sequence length and create a data set from the book text. All we need to do is read X amount of characters from the book as the input and then read 1 more as the designated output.

After doing all that, we also want to convert every character to it's index using the `char_to_idx` mapping that we have created before.

Neural networks work best if you scale your inputs and outputs. In this case we are going to scale everything between 0 and 1 by dividing them by the number of unique characters that we have.

And for the final step we will reshape it so we can use the data in our LSTM model.

```elixir
sequence_length = 100

train_data =
  normalized_book_text
  |> Enum.map(&Map.fetch!(char_to_idx, &1))
  |> Enum.chunk_every(sequence_length, 1, :discard)
  # We don't want the last chunk since we don't have a prediction for it.
  |> Enum.drop(-1)
  |> Nx.tensor()
  |> Nx.divide(characters_count)
  |> Nx.reshape({:auto, sequence_length, 1})
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[155920][100][1]
  EXLA.Backend<cuda:0, 0.1092860687.3230269458.76811>
  [
    [
      [0.7586206793785095],
      [0.3448275923728943],
      [0.24137930572032928],
      [0.03448275849223137],
      [0.6206896305084229],
      [0.6896551847457886],
      [0.5862069129943848],
      [0.41379308700561523],
      [0.24137930572032928],
      [0.17241379618644714],
      [0.7586206793785095],
      [0.03448275849223137],
      [0.3103448152542114],
      [0.7931034564971924],
      [0.7586206793785095],
      [0.24137930572032928],
      [0.5517241358757019],
      [0.13793103396892548],
      [0.24137930572032928],
      [0.6896551847457886],
      [0.3103448152542114],
      [0.03448275849223137],
      [0.24137930572032928],
      [0.13793103396892548],
      [0.5862069129943848],
      [0.5862069129943848],
      [0.4482758641242981],
      [0.03448275849223137],
      [0.5862069129943848],
      [0.27586206793785095],
      [0.03448275849223137],
      [0.10344827175140381],
      [0.48275861144065857],
      [0.37931033968925476],
      [0.17241379618644714],
      [0.24137930572032928],
      [0.7241379022598267],
      [0.03448275849223137],
      [0.10344827175140381],
      [0.20689654350280762],
      [0.8275861740112305],
      [0.24137930572032928],
      [0.5517241358757019],
      [0.7586206793785095],
      [0.7931034564971924],
      [0.6896551847457886],
      [0.24137930572032928],
      [0.7241379022598267],
      [0.03448275849223137],
      [0.37931033968925476],
      ...
    ],
    ...
  ]
>
```

For our train results, We will do the same. Drop the first `sequence_length` characters and then convert them to the mapping. This time we need to do something called **one-hot encoding**.

The reason we want to use one hot encoding is that in our model we don't want to only return a character as the output. We want it to return the possibility of each character for the output. This way we can decide if certain possibility is good or not or even we can decide between multiple possible outputs or even discard everything if the network is not confident enough.

In Nx, you can achieve this encoding by using this snippet

```elixir
Nx.tensor([
  [0],
  [1],
  [2]
])
|> Nx.equal(Nx.iota({3}))
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  u8[3][3]
  EXLA.Backend<cuda:0, 0.1092860687.3230269458.76814>
  [
    [1, 0, 0],
    [0, 1, 0],
    [0, 0, 1]
  ]
>
```

To sum it up, Here is how we generate the train results.

```elixir
train_results =
  normalized_book_text
  |> Enum.drop(sequence_length)
  |> Enum.map(&Map.fetch!(char_to_idx, &1))
  |> Nx.tensor()
  |> Nx.reshape({:auto, 1})
  |> Nx.equal(Nx.iota({characters_count}))
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  u8[155920][29]
  EXLA.Backend<cuda:0, 0.1092860687.3230269458.76818>
  [
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...],
    ...
  ]
>
```

## Defining the Model

```elixir
# As the input, We expect the sequence_length characters. Each one is a number.
model =
  Axon.input("input_chars", shape: {nil, sequence_length, 1})
  # The Long Short Term Memory layer of our network
  |> Axon.lstm(256)
  # Selecting only the output from the LSTM Layer
  |> then(fn {_, out} -> out end)
  # Since we only want the last sequence in LSTM we will slice it and select the last one
  |> Axon.nx(fn t -> t[[0..-1//1, -1]] end)
  # 20% dropout so we will not become too dependent on specific neurons.
  |> Axon.dropout(rate: 0.2)
  # The output layer. One for each character and using softmax as activation
  # so every node represents a probability.
  |> Axon.dense(characters_count, activation: :softmax)
```

<!-- livebook:{"output":true} -->

```
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                                           Model
============================================================================================================================================================================================
 Layer                                                                               Shape                                               Policy              Parameters   Parameters Memory
============================================================================================================================================================================================
 input_chars ( input )                                                               {nil, 100, 1}                                       p=f32 c=f32 o=f32   0            0 bytes
 lstm__c_hidden_state ( recurrent_state["input_chars"] )                             {nil, 1, 256}                                       p=f32 c=f32 o=f32   0            0 bytes
 lstm__h_hidden_state ( recurrent_state["input_chars"] )                             {nil, 1, 256}                                       p=f32 c=f32 o=f32   0            0 bytes
 lstm__hidden_state ( container {"lstm__c_hidden_state", "lstm__h_hidden_state"} )   {{nil, 1, 256}, {nil, 1, 256}}                      p=f32 c=f32 o=f32   0            0 bytes
 lstm_0 ( lstm["input_chars", "lstm__hidden_state"] )                                {{{nil, 1, 256}, {nil, 1, 256}}, {nil, 100, 256}}   p=f32 c=f32 o=f32   264192       1056768 bytes
 lstm_1_output_sequence ( elem["lstm_0"] )                                           {nil, 100, 256}                                     p=f32 c=f32 o=f32   0            0 bytes
 nx_0 ( nx["lstm_1_output_sequence"] )                                               {nil, 256}                                          p=f32 c=f32 o=f32   0            0 bytes
 dropout_0 ( dropout["nx_0"] )                                                       {nil, 256}                                          p=f32 c=f32 o=f32   0            0 bytes
 dense_0 ( dense["dropout_0"] )                                                      {nil, 29}                                           p=f32 c=f32 o=f32   7453         29812 bytes
 softmax_0 ( softmax["dense_0"] )                                                    {nil, 29}                                           p=f32 c=f32 o=f32   0            0 bytes
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total Parameters: 271645
Total Parameters Memory: 1086580 bytes
Inputs: %{"input_chars" => {nil, 100, 1}}

```

## Training the network

To train the network, We will use Axon's Loop API. It is pretty straightforward.

For the loss function we can use `categorical_cross_entropy` since we are dealing as categories (each character) in our output and for the optimizer we can use `adam`.

We will train our network for 20 epochs and it will take some time depending on your machine. It is faster to run this with a GPU.

```elixir
batch_size = 128
train_batches = Nx.to_batched_list(train_data, batch_size)
result_batches = Nx.to_batched_list(train_results, batch_size)

IO.puts("Total batches: #{Enum.count(train_batches)}")

params =
  model
  |> Axon.Loop.trainer(:categorical_cross_entropy, Axon.Optimizers.adam(0.001))
  |> Axon.Loop.run(Stream.zip(train_batches, result_batches), %{}, epochs: 20, compiler: EXLA)

:ok
```

<!-- livebook:{"output":true} -->

```
Total batches: 1219
Epoch: 0, Batch: 1200, loss: 2.8182118
Epoch: 1, Batch: 1200, loss: 2.7643051
Epoch: 2, Batch: 1200, loss: 2.7256687
Epoch: 3, Batch: 1200, loss: 2.6924984
Epoch: 4, Batch: 1200, loss: 2.6610374
Epoch: 5, Batch: 1200, loss: 2.6315992
Epoch: 6, Batch: 1200, loss: 2.6038766
Epoch: 7, Batch: 1200, loss: 2.5778201
Epoch: 8, Batch: 1200, loss: 2.5536559
Epoch: 9, Batch: 1200, loss: 2.5307744
Epoch: 10, Batch: 1200, loss: 2.5094700
Epoch: 11, Batch: 1200, loss: 2.4894619
Epoch: 12, Batch: 1200, loss: 2.4707501
Epoch: 13, Batch: 1200, loss: 2.4531305
Epoch: 14, Batch: 1200, loss: 2.4365997
Epoch: 15, Batch: 1200, loss: 2.4208741
Epoch: 16, Batch: 1200, loss: 2.4061375
Epoch: 17, Batch: 1200, loss: 2.3922462
Epoch: 18, Batch: 1200, loss: 2.3791831
Epoch: 19, Batch: 1200, loss: 2.3668413
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Generating text

After last step we will have our trained network parameters so we can start generating text with it. We simply need to give the initial sequence as the input to the network and select the most confident output. `Axon.predict/3` will give us the output layer and then using `Nx.argmax/1` we get the most confident neuron index then simply convert that index back to it's Unicode representation.

```elixir
require Axon

generate_fn = fn model, params, init_seq ->
  # The initial sequence that we want the network to complete for us.
  init_seq =
    init_seq
    |> String.trim()
    |> String.downcase()
    |> String.to_charlist()
    |> Enum.map(&Map.fetch!(char_to_idx, &1))

  Enum.reduce(1..100, init_seq, fn _, seq ->
    init_seq =
      seq
      |> Enum.take(-sequence_length)
      |> Nx.tensor()
      |> Nx.divide(characters_count)
      |> Nx.reshape({1, sequence_length, 1})

    char =
      Axon.predict(model, params, init_seq)
      |> Nx.argmax()
      |> Nx.to_number()

    seq ++ [char]
  end)
  |> Enum.map(&Map.fetch!(idx_to_char, &1))
end

# The initial sequence that we want the network to complete for us.
init_seq = """
not like to drop the jar for fear
of killing somebody underneath so managed to put it into one of the
cupboards as she fell past it.
"""

generate_fn.(model, params, init_seq) |> IO.puts()
```

<!-- livebook:{"output":true} -->

```
not like to drop the jar for fear
of killing somebody underneath so managed to put it into one of the
cupboards as she fell past it.aty arh a coec to tee it and the project gutenbergtm electronic worn and the project gutenbergtm ele
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Multi LSTM layers

We can improve our network by stacking multiple LSTM layers together. We just need to change our model and re-train our network.

```elixir
new_model =
  Axon.input("input_chars", shape: {nil, sequence_length, 1})
  |> Axon.lstm(256)
  |> then(fn {_, out} -> out end)
  |> Axon.dropout(rate: 0.2)
  # This time we will pass all of the `out` to the next lstm layer.
  # We just need to slice the last one.
  |> Axon.lstm(256)
  |> then(fn {_, out} -> out end)
  |> Axon.nx(fn x -> x[[0..-1//1, -1]] end)
  |> Axon.dropout(rate: 0.2)
  |> Axon.dense(characters_count, activation: :softmax)
```

<!-- livebook:{"output":true} -->

```
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                                             Model
===============================================================================================================================================================================================
 Layer                                                                                  Shape                                               Policy              Parameters   Parameters Memory
===============================================================================================================================================================================================
 input_chars ( input )                                                                  {nil, 100, 1}                                       p=f32 c=f32 o=f32   0            0 bytes
 lstm__c_hidden_state ( recurrent_state["input_chars"] )                                {nil, 1, 256}                                       p=f32 c=f32 o=f32   0            0 bytes
 lstm__h_hidden_state ( recurrent_state["input_chars"] )                                {nil, 1, 256}                                       p=f32 c=f32 o=f32   0            0 bytes
 lstm__hidden_state ( container {"lstm__c_hidden_state", "lstm__h_hidden_state"} )      {{nil, 1, 256}, {nil, 1, 256}}                      p=f32 c=f32 o=f32   0            0 bytes
 lstm_0 ( lstm["input_chars", "lstm__hidden_state"] )                                   {{{nil, 1, 256}, {nil, 1, 256}}, {nil, 100, 256}}   p=f32 c=f32 o=f32   264192       1056768 bytes
 lstm_1_output_sequence ( elem["lstm_0"] )                                              {nil, 100, 256}                                     p=f32 c=f32 o=f32   0            0 bytes
 dropout_0 ( dropout["lstm_1_output_sequence"] )                                        {nil, 100, 256}                                     p=f32 c=f32 o=f32   0            0 bytes
 lstm_1_c_hidden_state ( recurrent_state["dropout_0"] )                                 {nil, 1, 256}                                       p=f32 c=f32 o=f32   0            0 bytes
 lstm_1_h_hidden_state ( recurrent_state["dropout_0"] )                                 {nil, 1, 256}                                       p=f32 c=f32 o=f32   0            0 bytes
 lstm_1_hidden_state ( container {"lstm_1_c_hidden_state", "lstm_1_h_hidden_state"} )   {{nil, 1, 256}, {nil, 1, 256}}                      p=f32 c=f32 o=f32   0            0 bytes
 lstm_1 ( lstm["dropout_0", "lstm_1_hidden_state"] )                                    {{{nil, 1, 256}, {nil, 1, 256}}, {nil, 100, 256}}   p=f32 c=f32 o=f32   525312       2101248 bytes
 lstm_2_output_sequence ( elem["lstm_1"] )                                              {nil, 100, 256}                                     p=f32 c=f32 o=f32   0            0 bytes
 nx_0 ( nx["lstm_2_output_sequence"] )                                                  {nil, 256}                                          p=f32 c=f32 o=f32   0            0 bytes
 dropout_1 ( dropout["nx_0"] )                                                          {nil, 256}                                          p=f32 c=f32 o=f32   0            0 bytes
 dense_0 ( dense["dropout_1"] )                                                         {nil, 29}                                           p=f32 c=f32 o=f32   7453         29812 bytes
 softmax_0 ( softmax["dense_0"] )                                                       {nil, 29}                                           p=f32 c=f32 o=f32   0            0 bytes
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total Parameters: 796957
Total Parameters Memory: 3187828 bytes
Inputs: %{"input_chars" => {nil, 100, 1}}

```

Then we can train the network using the exact same code as before

```elixir
# Using a smaller batch size in this case will give the network more opportunity to learn
batch_size = 64
train_batches = Nx.to_batched_list(train_data, batch_size)
result_batches = Nx.to_batched_list(train_results, batch_size)

IO.puts("Total batches: #{Enum.count(train_batches)}")

new_params =
  new_model
  |> Axon.Loop.trainer(:categorical_cross_entropy, Axon.Optimizers.adam(0.001))
  |> Axon.Loop.run(Stream.zip(train_batches, result_batches), %{}, epochs: 50, compiler: EXLA)

:ok
```

<!-- livebook:{"output":true} -->

```
Total batches: 2437
Epoch: 0, Batch: 2400, loss: 2.6059778
Epoch: 1, Batch: 2400, loss: 2.4498167
Epoch: 2, Batch: 2400, loss: 2.3533349
Epoch: 3, Batch: 2400, loss: 2.2811732
Epoch: 4, Batch: 2400, loss: 2.2224922
Epoch: 5, Batch: 2400, loss: 2.1729276
Epoch: 6, Batch: 2400, loss: 2.1301043
Epoch: 7, Batch: 2400, loss: 2.0929463
Epoch: 8, Batch: 2400, loss: 2.0600429
Epoch: 9, Batch: 2400, loss: 2.0308213
Epoch: 10, Batch: 2400, loss: 2.0048594
Epoch: 11, Batch: 2400, loss: 1.9810826
Epoch: 12, Batch: 2400, loss: 1.9595370
Epoch: 13, Batch: 2400, loss: 1.9399362
Epoch: 14, Batch: 2400, loss: 1.9219711
Epoch: 15, Batch: 2400, loss: 1.9055365
Epoch: 16, Batch: 2400, loss: 1.8901759
Epoch: 17, Batch: 2400, loss: 1.8759004
Epoch: 18, Batch: 2400, loss: 1.8626444
Epoch: 19, Batch: 2400, loss: 1.8501121
Epoch: 20, Batch: 2400, loss: 1.8386080
Epoch: 21, Batch: 2400, loss: 1.8277082
Epoch: 22, Batch: 2400, loss: 1.8173727
Epoch: 23, Batch: 2400, loss: 1.8076453
Epoch: 24, Batch: 2400, loss: 1.7987509
Epoch: 25, Batch: 2400, loss: 1.7901217
Epoch: 26, Batch: 2400, loss: 1.7818677
Epoch: 27, Batch: 2400, loss: 1.7738990
Epoch: 28, Batch: 2400, loss: 1.7662609
Epoch: 29, Batch: 2400, loss: 1.7591102
Epoch: 30, Batch: 2400, loss: 1.7523022
Epoch: 31, Batch: 2400, loss: 1.7456987
Epoch: 32, Batch: 2400, loss: 1.7394242
Epoch: 33, Batch: 2400, loss: 1.7333591
Epoch: 34, Batch: 2400, loss: 1.7274810
Epoch: 35, Batch: 2400, loss: 1.7218159
Epoch: 36, Batch: 2400, loss: 1.7164260
Epoch: 37, Batch: 2400, loss: 1.7111731
Epoch: 38, Batch: 2400, loss: 1.7061847
Epoch: 39, Batch: 2400, loss: 1.7012498
Epoch: 40, Batch: 2400, loss: 1.6966366
Epoch: 41, Batch: 2400, loss: 1.6921319
Epoch: 42, Batch: 2400, loss: 1.6876395
Epoch: 43, Batch: 2400, loss: 1.6834700
Epoch: 44, Batch: 2400, loss: 1.6793612
Epoch: 45, Batch: 2400, loss: 1.6752253
Epoch: 46, Batch: 2400, loss: 1.6712373
Epoch: 47, Batch: 2400, loss: 1.6673778
Epoch: 48, Batch: 2400, loss: 1.6635668
Epoch: 49, Batch: 2400, loss: 1.6598836
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Generate text with the new network

```elixir
generate_fn.(new_model, new_params, init_seq) |> IO.puts()
```

<!-- livebook:{"output":true} -->

```
not like to drop the jar for fear
of killing somebody underneath so managed to put it into one of the
cupboards as she fell past it. the project gutenbergtm electronic works and any part of the project gutenbergtm electronic works

```

<!-- livebook:{"output":true} -->

```
:ok
```

As you may see, It improved a lot with this new model and the extensive training. This time it knows about rules like adding a space after period.

## References

The above example was written heavily inspired by [this article](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/) by Jason Brownlee.
