# Humans or horses

## General overview

In this notebook, we want to predict wheater an image present a human or a horse. To do this efficiently, we will build a convolutional neural net and compare the learning process with and without gradient centralization.

## Imports

```elixir
Mix.install([
  {:stb_image, "~> 0.5.2"},
  {:axon, "~> 0.1.0"},
  {:exla, "~> 0.2.2"},
  {:nx, "~> 0.2.1"}
])

EXLA.set_as_nx_default(
  [:tpu, :cuda, :rocm, :host],
  run_options: [keep_on_device: true]
)
```

## Data processing

Firstly, we need to preprocess the data to our CNN. At the beginning of the process, chunk images into batches. We use the ```parse_png``` function to load images and label them accurately. In the end, we "augment" the input, which means that we normalize data and flip the images along one of the axes. The last procedure helps NN to make predictions regardless of the orientation of the image.

```elixir
@directories "examples/vision/{horses,humans}/*"
def data() do
  Path.wildcard(@directories)
  |> Stream.chunk_every(32, 32, :discard)
  |> Task.async_stream(fn batch ->
    {inp, labels} = batch |> Enum.map(&parse_png/1) |> Enum.unzip()
    {Nx.stack(inp), Nx.stack(labels)}
  end)
  |> Stream.map(fn {:ok, {inp, labels}} -> {augment(inp), labels} end)
  |> Stream.cycle()
end

defnp augment(inp) do
  # Normalize
  inp = inp / 255.0

  # For now just a random flip
  if Nx.random_uniform({}) > 0.5 do
    Nx.reverse(inp, axes: [0])
  else
    Nx.reverse(inp, axes: [1])
  end
end

defp parse_png(filename) do
  class =
    if String.contains?(filename, "horses"),
      do: Nx.tensor([1, 0], type: {:u, 8}),
      else: Nx.tensor([0, 1], type: {:u, 8})

  {:ok, binary, shape, :u8, :rgba} = StbImage.from_file(filename)

  {StbImage.to_nx(img), class}
end
```

## Building Model

The next step is creating a model architecture. In the notebook, we choose classic Convolutional Neural Net architecture. There are a few Axon functions that I will try to explain briefly.

<!-- livebook:{"break_markdown":true} -->

```Axon.conv``` creates a convolutional layer. The convolutional layer is a core of CNNs. For a given *kernel_size*, it applies a filter function through the image to separate features which differ images from each other.

<!-- livebook:{"break_markdown":true} -->

| ![](https://miroslawmamczur.pl/wp-content/uploads/2021/03/06.gif)         |
| :-----------------------------------------------------------------------: |
| Figure 1: An example of convolutional layer works for *kernel_size*={3,3} |

<!-- livebook:{"break_markdown":true} -->

```Axon.max_pool``` is a mathematical operation which gives maximum value from a subtensor of a size equal to *kernel_size*.

<!-- livebook:{"break_markdown":true} -->

| ![](https://production-media.paperswithcode.com/methods/MaxpoolSample2.png) |
| :-------------------------------------------------------------------------: |
| Figure 2: Max pooling operation for *kernel_size*={2,2}                     |

<!-- livebook:{"break_markdown":true} -->

Axon.dropout and Axon.spatial_dropout creates dropout layers which prevent NN from overfitting. Standard dropout drops a given rate of randomly chosen neurons during the training process. On the other hand, spatial dropout gets rid of whole feature maps. The graphical difference between dropout and spatial dropout is presented in a picture below.

<!-- livebook:{"break_markdown":true} -->

| ![](https://miro.medium.com/max/1400/1*KkqxjvXTIV_b365B41ltfg.png)    |
| :-------------------------------------------------------------------: |
| Figure 3: The difference between standard dropout and spatial dropout |

<!-- livebook:{"break_markdown":true} -->

And eventually, we get the model build. At the end of the CNN, we use one dense layer with 512 neurons and an output layer with two neurons and a softmax function.

```elixir
defp build_model(input_shape, transpose_shape) do
  Axon.input(input_shape, "input")
  |> Axon.transpose(transpose_shape)
  |> Axon.conv(16, kernel_size: {3, 3}, activation: :relu)
  |> Axon.max_pool(kernel_size: {2, 2})
  |> Axon.conv(32, kernel_size: {3, 3}, activation: :relu)
  |> Axon.spatial_dropout(rate: 0.5)
  |> Axon.max_pool(kernel_size: {2, 2})
  |> Axon.conv(64, kernel_size: {3, 3}, activation: :relu)
  |> Axon.spatial_dropout(rate: 0.5)
  |> Axon.max_pool(kernel_size: {2, 2})
  |> Axon.conv(64, kernel_size: {3, 3}, activation: :relu)
  |> Axon.max_pool(kernel_size: {2, 2})
  |> Axon.conv(64, kernel_size: {3, 3}, activation: :relu)
  |> Axon.max_pool(kernel_size: {2, 2})
  |> Axon.flatten()
  |> Axon.dropout(rate: 0.5)
  |> Axon.dense(512, activation: :relu)
  |> Axon.dense(2, activation: :softmax)
end
```

## Training

Creating a training loop starts with setting a loss function to binary cross-entropy and optimizer of your choice. We also set the log option to 1 to keep track of loss and metrics during training. Next, we choose accuracy as a metric. Finally, we pass the number of epochs and set the number of iterations per epoch to 100.

```elixir
defp train_model(model, data, optimizer, epochs) do
  model
  |> Axon.Loop.trainer(:binary_cross_entropy, optimizer, log: 1)
  |> Axon.Loop.metric(:accuracy)
  |> Axon.Loop.run(data, %{}, epochs: epochs, iterations: 100)
end
```

## Model evaluation

Now we can build and train our model. We start with passing input shape and indices to data transposition. Next, we set the optimizer to adam with a learning rate of 1.0e-4. In the next step, we define an optimizer but with gradient centralization. It is technic with a similar purpose to batch normalization. For each loss gradient, we subtract a mean value to have a gradient with mean equal to zero. This process prevents us from exploding gradients.

```elixir
def run() do
  model = build_model({nil, 300, 300, 4}, [2, 0, 1]) |> IO.inspect()
  optimizer = Axon.Optimizers.adam(1.0e-4)
  centralized_optimizer = Axon.Updates.compose(Axon.Updates.centralize(), optimizer)

  data = data()

  IO.write("\n\nTraining model without gradient centralization\n\n")
  train_model(model, data, optimizer, 10)

  IO.write("\n\nTraining model with gradient centralization\n\n")
  train_model(model, data, centralized_optimizer, 10)
end
```
