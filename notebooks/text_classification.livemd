# Named Entity Recognition via LSTM with Words in Axon

```elixir
Mix.install([
  {:axon, github: "elixir-nx/axon"},
  {:nx, "~> 0.2.1"},
  {:exla, "~> 0.2.2"}
])

EXLA.set_as_nx_default([:tpu, :cuda, :rocm, :host])
@sequence_length 75
@batch_size 128
@lstm 256
@def_split " "
@start "-DOCSTART- -X- -X- O\n"
@ending "-------------------------------------------------------------"
```

## Introduction

In this example, we will discover how to create a generative model for text, character-by-character using LSTM recurrent neural networks in Elixir with Axon. The follwing information is based on some really helpful guides: [NER Recognition in Keras](https://valueml.com/named-entity-recognition-using-lstm-in-keras/) & [NER RNN TensorFlow](http://alexminnaar.com/2019/08/22/ner-rnns-tensorflow.html).

## Preparation

Using [Example Datasets](https://github.com/bhuvanakundumani/NER_tensorflow2.2.0/blob/master/data/) we can download three example datasets from the CONL2003 library, these will be the Train, Test and Validate datasets.

```elixir
test_src = "https://github.com/bhuvanakundumani/NER_tensorflow2.2.0/blob/master/data/test.txt"
train_src = "https://github.com/bhuvanakundumani/NER_tensorflow2.2.0/blob/master/data/train.txt"

validation_src =
  "https://github.com/bhuvanakundumani/NER_tensorflow2.2.0/blob/master/data/valid.txt"
```

## Using the datasets

For each dataset, we'll need to stream the contents. This is due to the size of datasets, and the nature in which the RNN operates. If you look closely at the references you can see that Keras & TensorFlow use a generator function in order to process data through the RNN. We'll be using File.stream/1 & some helper functions.

### from_file/2

is responsible for Streaming the contents of the file

### pre_process/1

Takes the streamed data & sanitizes the incoming data. We'll cover this in the section below.

```elixir
defp from_file(filename, opts \\ []) do
  data_stream =
    filename
    |> File.stream!()
end

defp pre_process(streamed_data) do
  char_label_data =
    streamed_data
    |> Stream.map(fn line ->
      [token | entities] = String.split(line, @def_split)

      if(
        !is_nil(token) &&
          token !== "" &&
          line !== "\n" &&
          token !== @start &&
          token !== @ending
      ) do
        [r | rest] = entities |> List.last() |> String.split("\n")

        {token, r}
      else
        nil
      end
    end)
    |> Stream.reject(fn x -> is_nil(x) end)

  unique_words = char_label_data |> Enum.map(&elem(&1, 0)) |> Enum.uniq()

  unique_labels = char_label_data |> Enum.map(&elem(&1, 1)) |> Enum.uniq()

  {char_label_data, unique_words, unique_labels}
end
```

## Pre-Processing the Data

As you can see above, we need to process the data after we have opened the stream. We will first look at the structure of the dataset.

```
[["TOKEN_WORD_WERE_LOOKING_FOR", "LABEL_1", "LABEL_2", ...], ...]
```

As seen above, it's comprised of Rows of a word (token) plus it's corresponding labels.

We first use the ```@def_split``` in order to split the raw line into the above sequences.

We're only worried about the last label for this example, if you we're to use more than the one label you would have a one to many relationship, where only looking at a one to one relationship of ```Tokens:Labels```

## Optimizng the dataset

RNN's & LSTM's are memory intensive programs that can cause performance issues & unintended side-effects in your results iof the datasets haven't been optimized. This can result in large pre-processing times, large epoch intervals (slow processing).

By the nature of LSTM based RNN newtworks, the program will look across a window consisting of timesteps.

```[0,1,2,3, [4,5,6], 7,8,9] (timesteps == 3)```

Each character or token the network has to learn, increases the output of the model size. This can be insiginificant with smaller datasets, however, if you're using words & characters that don't effect the final result, youre spending resources for no reason.

```!is_nil(token) && token !== "" && line !== "\n" && token !== @start && token !== @ending```

Above you can see we're removing any unwanted word

## Putting the Datasets Together

Now that we have covered some basics, we'll finally stream the dataset & pre-process ready for the next stage.

```elixir
{word_label_data, unique_words, unique_labels} =
  from_file(train_src)
  |> pre_process

{test_word_label_data, unique_test_words, unique_test_labels} =
  from_file(test_src)
  |> pre_process
```

## Encoding & Decoding the Tokens & Labels

RNN's don't operate with strings as values very efficiently. This is due to the operation of the RNN working via matrice's and transformations. For this reason, we'll tokenize the words & labels, we'll call this encoding. Once we've tokenzied the words & Labels we also need a method to convert the result from the RNN back into a word or label, we'll call this decoding.

This will be performed using the following helper functions ```to_idx/1``` and ```idx_to/1```

idx is a nice way of saying ID's, thus:
```to_idx``` ~> converts words/labels to tokens (id's) and
```idx_to``` ~> converts tokens to words/labels

```encode/2``` & ```decode/2``` are responsbile for the actual encoding/decoding from the token. This is somewhat needed to solve the issue of unknown tokens & labels being introduced into the dataset via an input. This can be made clearer if you thjink about training the RNN on a certain vocabulary and then it encounters a word (token in this case) that it hasn't seen before.

i.e: ```%{"CAT" => 0, "DOG" => 1, "BIRD" => 3}```, now if you try and encode the word ```"Donkey"``` it wouldn't have a token for it.

```elixir
defp to_idx(str_array) do
  str_array
  |> Stream.uniq()
  |> Stream.with_index()
  |> Enum.into(%{})
end

defp idx_to(str_array) do
  str_array
  |> Stream.uniq()
  |> Enum.with_index(&{&2, &1})
  |> Enum.into(%{})
end

defp encode(dictionary, word) do
  with {:ok, id} <- Map.fetch(dictionary, word) do
    id
  else
    _ ->
      0
  end
end

defp decode(dictionary, word) do
  with {:ok, id} <- Map.fetch(dictionary, word) do
    word
  else
    _ -> "Unknown"
  end
end

word_to_idx = to_idx(unique_words)
label_to_idx = to_idx(unique_labels)

idx_to_word = idx_to(unique_words)
idx_to_label = idx_to(unique_labels)
```

## LSTM Input Preparation

For the RNN network to operate, it uses input sequences of tokens & labels. this can be visualized as:

``` [1,3,4,3,1,2,0, ...] ```

These input sequences are generated from the tokenized (encoded) words that we processed in the steps above. This means that our sequence from the file we stream would be an array of:

```[token, n++]```, where n is the sum of all the tokens in the file, this could mean you have 1200 words in the file, or 12000 words in the file.

The same sequence pattern must be followed by the labels, as we defined above we have a one to one relationship for our datasets:

```[token, n++] = [label, n++]```

As you can expect, for every token there must be a respective label. This is essential as we feed our network in inputs, with each input corresponding to ```{token, label}```, thus one of the major goals in this step is to ensure consistency across the labels & tokens.

LSTM networks are fed in batches of inputs, usually designated by ```batch_size```, this variable allows us to split our Single Dimension input sequence into groups of input sequences (i.e batches). We have defined the batch size in the start of the modukle (```@batch_size```)

RNN inputs must be of equal size matrice, this means that you cannot have an input sequence(row) with 8 elements, and then another with 6 and so forth. The process of transforming the input sequences into equal size input sequences (rows) is known as either Padding or Truncation (depending on the method used).

Padding must be used if you know you fixed length input, i.e if we expect we are only going to need a sequence length of 200 characters, we can pad the input sequence to achieve the max length:

```
Input Sequence 1 ~> [1,3,4,4]
Input Sequence 2 ~> [1,3,4]
```

After padding:

```
Input Sequence 1 ~> [1,3,4,4]
Input Sequence 2 ~> [1,3,4,0]
```

Truncation is chosen if we have an infinite length for the sequence, i.e we are processing paragraphs of text, that can have any legth, opposed to the former situation where you might be taking an input from an api of fixed character length.

```
Input Sequence 1 ~> [1,3,4,4]
Input Sequence 2 ~> [1,3,4]
```

After Truncation:

```
Input Sequence 1 ~> [1,3,4]
Input Sequence 2 ~> [1,3,4]
```

The important thing to note here, if we truncate or pad the input sequence, we need to do the same process to the label input sequence; this ensures the training data (tokens/words) matches the labelling data (labels).

For this example, since we don't know the sequence length that we would require, we'll use truncation. Within elixir this is ```Enum.chunk_every/2```

We have defined a sequence length (```@sequence_length```) at the start of the module, this is aklso known as the ```time steps```. This value isn't arbritrary and you can find the optimal sequence length by analyzing your input words and seeing the frequency of the word count. i.e you may find that sentences with 64 words were used mostly, and anything below that were only used a small fraction. This is important as it's one fo the variables you can use to adjust your models performance.

```elixir
def transform_words(word_labels, word_to_idx, label_to_idx, wcount, lcount) do
  train_data =
    word_labels
    |> Enum.map(fn {word, label} ->
      encode(word_to_idx, word)
    end)
    |> Enum.chunk_every(@sequence_length, 1, :discard)
    |> Nx.tensor()
    |> Nx.divide(wcount)
    |> Nx.reshape({:auto, @sequence_length, 1})
    |> Nx.to_batched_list(@batch_size)

  train_labels =
    word_labels
    |> Enum.map(fn {word, label} ->
      encode(label_to_idx, label)
    end)
    |> Enum.chunk_every(@sequence_length, 1, :discard)
    |> Nx.tensor()
    |> Nx.reshape({:auto, @sequence_length, 1})
    |> Nx.to_batched_list(@batch_size)

  {train_data, train_labels}
end
```

In the above code block, we can see the input sequence is first encoded using the ```encode/1``` (```to_idx```) function discussed earlier, this is ran through the encode/1 function to catch errors in the input sequence (unknown characters etc)

we use Enum.chunk_every/3 to split the long input sequence into our designated sequence length, i.e:

```
sequence_length = 100 (time steps)
total_input_sequence_length = 1000
Total_Input_Sequences = (total_sequence_length/10) => 100 input sequences of 100 tokens
```

The ```:discard``` option tells Elixir to discard any remainder token that can't be chunked.

<!-- livebook:{"break_markdown":true} -->

The next transformation for the input sequence is to be converted into a Tensor so that it can be fed into the RNN network & utilized by the layers. ```Nx.tensor``` will create a tensor from an array. The shape of the tensor is proportional to the shape of the array it was created from:

```Tensor[time_steps][sequence_length] = [[input_sequence_of_length, ...], ...]```

We will then reshape that tensor so that we have an undetermined amount of rows of sequence_length timesteps of sequence length of 1.

```Tensor[time_steps][sequence_length][1]```

The last part of this transformation process is to thenm create batches from the tensor created above. ```Nx.to_batch_list``` will return batches of your input_sequence with correposnding time steps.

This means that you will be feeding the model batches of input_sequences which contain X time steps:

```Tensor[batch_size][time_steps][sequence_length]```

This is the correct input shape for an RNN LSTM model

## Transforming the Words & Labels

Before we transform the words and lables using ```transform_words/5``` we need to count the amount of unique words & labels that we have. This is so we can further optimize the input layers as RNN nertworks don't like to use values such as 0,1,2,3, but instead floating point values that are relational betwen each other i.e 0.4321, 0.521 etc.

We use the total amount of words to normalize the input layer, this is done by ```Nx.divide```.  We use the total amount of labels to give us our resulting model dimensions, this will be covered at a later stage.

```elixir
word_count = unique_words |> Enum.count()
label_count = unique_labels |> Enum.count()
```

Now we can transform our words using the respective functions:

```elixir
{train_data, train_labels} =
  transform_words(word_label_data, word_to_idx, label_to_idx, word_count, label_count)

{test_train_data, test_train_labels} =
  transform_words(test_word_label_data, word_to_idx, label_to_idx, word_count, label_count)
```

## Building the model

```elixir
def build_model(word_count, label_count) do
  Axon.input({nil, @sequence_length, 1}, "inputs")
  |> Axon.embedding(word_count, @sequence_length)
  |> Axon.nx(fn t ->
    t[[0..-1//1, -1]]
  end)
  |> Axon.spatial_dropout(rate: 0.1)
  |> Axon.lstm(@lstm)
  |> then(fn {{new_cell, new_hidden}, out} ->
    out
  end)
  |> Axon.dropout(rate: 0.2)
  |> Axon.dense(label_count, activation: :softmax)
end
```

Now it's time to build our model. our model consists of a base input layer of shape
```{batch_size, time_steps_sequence_length, 1}```
the embedding layer is a vectorization layer for the model, this allows us to compute the corresponding vectors of ```{token, label}``` to determine how accurate the result was.

We use ```Axon.nx``` to then take the tensor returned from the above layer and using ```t[[0..-1//1, -1]]``` we slice the arity 1 dimension from the layer, returning the needed vectorization (embed matrice) along with the input sequence. You can also use ```Nx.squeeze``` here as long as the squeeze column is of value 1.

```@lstm``` is our lstm memory size, this is the memory 'window' of the LSTM model. The LSTM memory size will greatly affect performance and is related to your standard sentence length, i.e if you were trying to classify sentences above 100 words long, a memory window of 50 would split them into two seperate passes.

```Axon.dense``` is our final layer which uses the total label count to define it's input_length. If we only have 5 labels to classify a word with, it wouldn't make sense to have an ionput_length of above 5 as it would be wasted memory & space. the result shape would look like ```{batch_size, time_step, label_count}``` The dense layer will compute the result along the tensor.

An important thing to note
The activation function of ```:softmax``` has a bearing on the loss function you'll need to use to calculate the difference between the results. Some loss functions require the tensor values to be between 0 & 1 etc, this is where the activation function comes into play.

## Model Training Setup

```elixir
def train(model, data) do
  model
  |> Axon.Loop.trainer(&kl_divergence/2, Axon.Optimizers.adam(0.001))
  |> Axon.Loop.metric(:accuracy, "Accuracy")
  |> Axon.Loop.run(data, %{}, epochs: 15, iterations: 5)
end

defp kl_divergence(y_true, y_pred) do
  Axon.Losses.kl_divergence(y_true, y_pred, reduction: :mean)
end
```

In Axon we trainh the model using ```Axon.Loop.trainer``` this creates a supervised training loop for the model to use.

```kl_divergence/2``` is the loss function we'll for the trainer, kl_divergence is one of many loss functions. For classifcation problems You can use Categorical Cross Entropy, or Binary Cross Entropy; however the latter is used for one hot encoding (either 0 or 1), where the former is a computation by it's respective reduction, i.e ```:mean```, ```:sum```

You can see loss functions generally take two arguments, the true value and the predicted value. The function will operate on thos eteo arguments and return a value. It's important to not get hung up on the actual loss value, as this is proportional to your input scale (we have normalized via ```Nx.divide```)

If we hadn't normalized the loss value maybe: 12300, where as if we normalize it may be: 2.350. The result you want is the loss to decrease at the next epoch, not the actual value.

## Training Execution

Before we can train our model we need to transform our training_data & labels into a format the model will accept at the input layer. As discussed earlier this is of the form ```{input, output}```, where input is the word token and output is the label token.

To do this we'll use ```Stream.zip/2```, this function takes two arrays and combines each respective child into the tuple ```{input, output}```.

Before Zip:

```
[word_token, n++]
[label_token, n++]
```

After Zip: ```[{word_token, label_token}, n++]```

```elixir
data = Stream.zip(train_data, train_labels)
```

To execute our training Loop using ```train/2```, this first argument for the train function is a model, lets use ```build_model/2``` to generate the model we'll need.

```elixir
model = build_model(word_count, label_count)
```

We can now execute the training loop using ```train/2``` and the output of the training loop will be training paramteres which are needed to predict word classifications from the model.

```elixir
training_params = train(model, data)
```

## Prediction Setup

```elixir
def predict(init_sequence, params, model, idx_to_char, idx_to_label, count, real) do
  Enum.reduce(1..10, [], fn _, seq ->
    p = Axon.predict(model, params, init_sequence)

    predicted =
      p
      |> Nx.argmax(axis: -1)
      |> Nx.to_flat_list()
      |> Enum.map(fn predict_sequence ->
        decode(idx_to_label, predict_sequence)
      end)

    given =
      init_sequence
      |> Nx.to_flat_list()
      |> Enum.map(fn inpseq ->
        {actual_result, _} =
          (inpseq * count)
          |> Float.floor()
          |> Float.to_string()
          |> Integer.parse()

        decode(idx_to_char, actual_result)
      end)

    rl =
      real
      |> Nx.to_flat_list()
      |> Enum.map(fn real_sequence ->
        decode(idx_to_label, real_sequence)
      end)

    Enum.zip([given, predicted, rl])
  end)
  |> Enum.map(fn {input, output, r} ->
    true? =
      if output == r do
        "CORRECT"
      else
        "INCORRECT"
      end

    if true? == "CORRECT" do
      Logger.warn(
        "Word was: #{input}, Predicted Category was: #{output} Real Result: #{r}, RETURNED: #{true?}"
      )
    else
      Logger.error(
        "Word was: #{input}, Predicted Category was: #{output} Real Result: #{r}, RETURNED: #{true?}"
      )
    end
  end)
end
```

As you can see above ```predict/6``` requires an input sequence, the ```training paramaters```, the buil't model & the relevant meta data for encoding/decoding (```idx_to```, ```count```, ```real_results```)

<!-- livebook:{"break_markdown":true} -->

To predict the word classification, we require the training paramters from the training execution above & some sample test data from the start. We'll select a random batch from the ```test_train_data``` & the corresponding real results from the ```test_train_labels```

You could use the training dataset for this, however, you don't want to use the sdame dataset the model was trained on as it isn't a realistic or valid test (as it's learnt those sequences already). This tutorial doesn't cover the test:train:validate split, as it's a quite a large area that invlives biases and other important factors.

This is the input sequence the predict function will utilize

```elixir
# Has to be in range
random_int_from_sample = 5
sample_slice = Enum.fetch!(test_train_data, random_int_from_sample)
real_results = Enum.fetch!(test_train_labels, random_int_from_sample)
```

To predict the input_sequence's word classification sequence, we use ```Axon.predict```, this will return a tensor which holds the classified word sequence.

```Nx.argmax``` is used to isolate the tensor with the maximum values, in our case it's the closes classification. We use ```Nx.to_flat_list``` to return the sequence to a flat array so that we can decode the resultant prediction using ```decode/2```

We'll transform  the input sequence so we can decode it, this is becasue we normalized the tensor in the ```transform_words``` function in order to make the model run more efficiently. If we dont do this, the values you'll get back would be a float i.e 0.43, which we then cannot use to decode into the actual word.

We take the ```real_result```, which is the actual labels for that given input sequence, and we decode to compare the prediction

The Result arrays then use ```Enum.zip``` so we can tidy them up neatly to compare our results. I've included a very basic output for the results which just print the word that was given, it's predicted word and the actual word, also displaying if they were a match or not.

## Prediction Execution

We can now execute our predictions.

```elixir
predict(
  sample_slice,
  unpadded_params,
  unpadded_model,
  idx_to_word,
  idx_to_label,
  word_count,
  real_results
)
```
