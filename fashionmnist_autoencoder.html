<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="ExDoc v0.28.6">
    <meta name="project" content="Axon v0.3.0">

    <title>Training an Autoencoder on Fashion MNIST — Axon v0.3.0</title>
    <link rel="stylesheet" href="dist/html-elixir-5CLCQQME.css" />

    <script src="dist/handlebars.runtime-NWIB6V2M.js"></script>
    <script src="dist/handlebars.templates-X7YVL3G2.js"></script>
    <script src="dist/sidebar_items-CEB5417F.js"></script>

      <script src="docs_config.js"></script>

    <script async src="dist/html-PLW5RNNI.js"></script>


  </head>
  <body data-type="extras" class="page-livemd">
    <script>

      try {
        var settings = JSON.parse(localStorage.getItem('ex_doc:settings') || '{}');

        if (settings.theme === 'dark' ||
           ((settings.theme === 'system' || settings.theme == null) &&
             window.matchMedia('(prefers-color-scheme: dark)').matches)
           ) {
          document.body.classList.add('dark')
        }
      } catch (error) { }
    </script>

<div class="main">

<button class="sidebar-button sidebar-toggle" aria-label="toggle sidebar">
  <i class="ri-menu-line ri-lg" title="Collapse/expand sidebar"></i>
</button>

<section class="sidebar">
  <form class="sidebar-search" action="search.html">
    <button type="submit" class="search-button" aria-label="Submit Search">
      <i class="ri-search-2-line" aria-hidden="true" title="Submit search"></i>
    </button>
    <button type="button" tabindex="-1" class="search-close-button" aria-label="Cancel Search">
      <i class="ri-close-line ri-lg" aria-hidden="true" title="Cancel search"></i>
    </button>
    <label class="search-label">
      <p class="sr-only">Search</p>
      <input name="q" type="text" class="search-input" placeholder="Search..." aria-label="Input your search terms" autocomplete="off" />
    </label>
  </form>

  <div class="autocomplete">
    <div class="autocomplete-results">
    </div>
  </div>

  <div class="sidebar-header">

      <a href="Axon.html">
        <img src="assets/logo.png" alt="Axon" class="sidebar-projectImage">
      </a>

    <div class="sidebar-projectDetails">
      <a href="Axon.html" class="sidebar-projectName" translate="no">
Axon
      </a>
      <strong class="sidebar-projectVersion" translate="no">
        v0.3.0
      </strong>
    </div>
    <ul class="sidebar-listNav">
      <li><a id="extras-list-link" href="#full-list">Pages</a></li>

        <li><a id="modules-list-link" href="#full-list">Modules</a></li>


    </ul>
  </div>

  <div class="gradient"></div>
  <ul id="full-list" class="sidebar-fullList"></ul>
</section>

<section class="content">
  <output role="status" id="toast"></output>
  <div class="content-outer">
    <div id="content" class="content-inner">

<h1>
<button class="settings display-settings">
  <i class="ri-settings-3-line"></i>
  <span class="sr-only">Settings</span>
</button>


    <a href="https://github.com/elixir-nx/axon/blob/v0.3.0/notebooks/generative/fashionmnist_autoencoder.livemd#L1" title="View Source" class="view-source" rel="help">
      <i class="ri-code-s-slash-line" aria-hidden="true"></i>
      <span class="sr-only">View Source</span>
    </a>

  <span>Training an Autoencoder on Fashion MNIST</span>
</h1>

  <div class="livebook-badge-container">
    <a href="#" class="livebook-badge">
      <img src="https://livebook.dev/badge/v1/blue.svg" alt="Run in Livebook" width="150" />
    </a>
  </div>

<pre><code class="makeup elixir" translate="no"><span class="nc">Mix</span><span class="o">.</span><span class="n">install</span><span class="p" data-group-id="6905669367-1">(</span><span class="p" data-group-id="6905669367-2">[</span><span class="w">
  </span><span class="p" data-group-id="6905669367-3">{</span><span class="ss">:axon</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;~&gt; 0.3.0&quot;</span><span class="p" data-group-id="6905669367-3">}</span><span class="p">,</span><span class="w">
  </span><span class="p" data-group-id="6905669367-4">{</span><span class="ss">:nx</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;~&gt; 0.4.0&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">override</span><span class="p">:</span><span class="w"> </span><span class="no">true</span><span class="p" data-group-id="6905669367-4">}</span><span class="p">,</span><span class="w">
  </span><span class="p" data-group-id="6905669367-5">{</span><span class="ss">:exla</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;~&gt; 0.4.0&quot;</span><span class="p" data-group-id="6905669367-5">}</span><span class="p">,</span><span class="w">
  </span><span class="p" data-group-id="6905669367-6">{</span><span class="ss">:scidata</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;~&gt; 0.1.9&quot;</span><span class="p" data-group-id="6905669367-6">}</span><span class="w">
</span><span class="p" data-group-id="6905669367-2">]</span><span class="p" data-group-id="6905669367-1">)</span><span class="w">

</span><span class="nc">Nx.Defn</span><span class="o">.</span><span class="n">default_options</span><span class="p" data-group-id="6905669367-7">(</span><span class="ss">compiler</span><span class="p">:</span><span class="w"> </span><span class="nc">EXLA</span><span class="p" data-group-id="6905669367-7">)</span></code></pre><h2 id="introduction" class="section-heading">
  <a href="#introduction" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i>
  <p class="sr-only">introduction</p>
  </a>
  Introduction
</h2>
<p>An autoencoder is a deep learning model which consists of two parts: encoder and decoder. The encoder compresses high dimensional data into a low dimensional representation and feeds it to the decoder. The decoder tries to recreate the original data from the low dimensional representation.
Autoencoders can be used in the following problems:</p><ul><li>Dimensionality reduction</li><li>Noise reduction</li><li>Generative models</li><li>Data augmentation</li></ul><p>Let's walk through a basic autoencoder implementation in Axon to get a better understanding of how they work in practice.</p><h2 id="downloading-the-data" class="section-heading">
  <a href="#downloading-the-data" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i>
  <p class="sr-only">downloading-the-data</p>
  </a>
  Downloading the data
</h2>
<p>To train and test how our model works, we use one of the most popular data sets: <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion MNIST</a>. It consists of small black and white images of clothes. Loading this data set is very simple with the help of <code class="inline">Scidata</code>.</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="9491071650-1">{</span><span class="n">image_data</span><span class="p">,</span><span class="w"> </span><span class="c">_label_data</span><span class="p" data-group-id="9491071650-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Scidata.FashionMNIST</span><span class="o">.</span><span class="n">download</span><span class="p" data-group-id="9491071650-2">(</span><span class="p" data-group-id="9491071650-2">)</span><span class="w">
</span><span class="p" data-group-id="9491071650-3">{</span><span class="n">bin</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="p" data-group-id="9491071650-3">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">image_data</span></code></pre><p>We get the data in a raw format, but this is exactly the information we need to build an Nx tensor.</p><pre><code class="makeup elixir" translate="no"><span class="n">train_images</span><span class="w"> </span><span class="o">=</span><span class="w">
  </span><span class="n">bin</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">from_binary</span><span class="p" data-group-id="0098235361-1">(</span><span class="n">type</span><span class="p" data-group-id="0098235361-1">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">reshape</span><span class="p" data-group-id="0098235361-2">(</span><span class="n">shape</span><span class="p" data-group-id="0098235361-2">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">divide</span><span class="p" data-group-id="0098235361-3">(</span><span class="mf">255.0</span><span class="p" data-group-id="0098235361-3">)</span></code></pre><p>We also normalize pixel values into the range $[0, 1]$.</p><p>We can visualize one of the images by looking at the tensor heatmap:</p><pre><code class="makeup elixir" translate="no"><span class="nc">Nx</span><span class="o">.</span><span class="n">to_heatmap</span><span class="p" data-group-id="7476270261-1">(</span><span class="n">train_images</span><span class="p" data-group-id="7476270261-2">[</span><span class="mi">1</span><span class="p" data-group-id="7476270261-2">]</span><span class="p" data-group-id="7476270261-1">)</span></code></pre><h2 id="encoder-and-decoder" class="section-heading">
  <a href="#encoder-and-decoder" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i>
  <p class="sr-only">encoder-and-decoder</p>
  </a>
  Encoder and decoder
</h2>
<p>First we need to define the encoder and decoder. Both are one-layer neural networks.</p><p>In the encoder, we start by flattening the input using <code class="inline">Axon.flatten</code> because initially, the input shape is {batch_size, 1, 28, 28} and we want to pass the input into a dense layer with <code class="inline">Axon.dense</code>. Our dense layer has only <code class="inline">latent_dim</code> number of neurons. The <code class="inline">latent_dim</code> or latent space is a compressed representation of data. Remember, we want our encoder to compress the input data into a lower-dimensional representation, so we choose a <code class="inline">latent_dim</code> which is less than the dimensionality of the input.</p><p>Next, we pass the output of the encoder to the decoder and try to reconstruct the compressed data into its original form. Since our original input had a dimensionality of 784, we use an <code class="inline">Axon.dense</code> layer with 784 neurons. Because our original data was normalized to have pixel values between 0 and 1, we use a <code class="inline">:sigmoid</code> activation in our dense layer to squeeze output values between 0 and 1. Our original input shape was 28x28, so we use <code class="inline">Axon.reshape</code> to convert the flattened representation of the outputs into an image with correct the width and height.</p><p>If we just bind the encoder and decoder sequentially, we'll get the desired model. This was pretty smooth, wasn't it?</p><pre><code class="makeup elixir" translate="no"><span class="n">encoder</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k" data-group-id="9461707225-1">fn</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">latent_dim</span><span class="w"> </span><span class="o">-&gt;</span><span class="w">
  </span><span class="n">x</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Axon</span><span class="o">.</span><span class="n">flatten</span><span class="p" data-group-id="9461707225-2">(</span><span class="p" data-group-id="9461707225-2">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Axon</span><span class="o">.</span><span class="n">dense</span><span class="p" data-group-id="9461707225-3">(</span><span class="n">latent_dim</span><span class="p">,</span><span class="w"> </span><span class="ss">activation</span><span class="p">:</span><span class="w"> </span><span class="ss">:relu</span><span class="p" data-group-id="9461707225-3">)</span><span class="w">
</span><span class="k" data-group-id="9461707225-1">end</span><span class="w">

</span><span class="n">decoder</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k" data-group-id="9461707225-4">fn</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">-&gt;</span><span class="w">
  </span><span class="n">x</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Axon</span><span class="o">.</span><span class="n">dense</span><span class="p" data-group-id="9461707225-5">(</span><span class="mi">784</span><span class="p">,</span><span class="w"> </span><span class="ss">activation</span><span class="p">:</span><span class="w"> </span><span class="ss">:sigmoid</span><span class="p" data-group-id="9461707225-5">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Axon</span><span class="o">.</span><span class="n">reshape</span><span class="p" data-group-id="9461707225-6">(</span><span class="p" data-group-id="9461707225-7">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">28</span><span class="p">,</span><span class="w"> </span><span class="mi">28</span><span class="p" data-group-id="9461707225-7">}</span><span class="p" data-group-id="9461707225-6">)</span><span class="w">
</span><span class="k" data-group-id="9461707225-4">end</span><span class="w">

</span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w">
  </span><span class="nc">Axon</span><span class="o">.</span><span class="n">input</span><span class="p" data-group-id="9461707225-8">(</span><span class="s">&quot;input&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">shape</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="9461707225-9">{</span><span class="no">nil</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">28</span><span class="p">,</span><span class="w"> </span><span class="mi">28</span><span class="p" data-group-id="9461707225-9">}</span><span class="p" data-group-id="9461707225-8">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">encoder</span><span class="o">.</span><span class="p" data-group-id="9461707225-10">(</span><span class="mi">64</span><span class="p" data-group-id="9461707225-10">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">decoder</span><span class="o">.</span><span class="p" data-group-id="9461707225-11">(</span><span class="p" data-group-id="9461707225-11">)</span></code></pre><p>First we need to define the encoder and decoder. Both are one-layer neural networks.</p><p>In the encoder, we start by flattening the input, so we get from shape <code class="inline">{batch_size, 1, 28, 28}</code> to <code class="inline">{batch_size, 784}</code> and we pass the input into a dense layer. Our dense layer has only <code class="inline">latent_dim</code> number of neurons. The <code class="inline">latent_dim</code> (or the latent space) is a compressed representation of data. Remember, we want our encoder to compress the input data into a lower-dimensional representation, so we choose a <code class="inline">latent_dim</code> which is less than the dimensionality of the input.</p><pre><code class="makeup elixir" translate="no"><span class="n">encoder</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k" data-group-id="6719453806-1">fn</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">latent_dim</span><span class="w"> </span><span class="o">-&gt;</span><span class="w">
  </span><span class="n">x</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Axon</span><span class="o">.</span><span class="n">flatten</span><span class="p" data-group-id="6719453806-2">(</span><span class="p" data-group-id="6719453806-2">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Axon</span><span class="o">.</span><span class="n">dense</span><span class="p" data-group-id="6719453806-3">(</span><span class="n">latent_dim</span><span class="p">,</span><span class="w"> </span><span class="ss">activation</span><span class="p">:</span><span class="w"> </span><span class="ss">:relu</span><span class="p" data-group-id="6719453806-3">)</span><span class="w">
</span><span class="k" data-group-id="6719453806-1">end</span></code></pre><p>Next, we pass the output of the encoder to the decoder and try to reconstruct the compressed data into its original form. Since our original input had a dimensionality of 784, we use a dense layer with 784 neurons. Because our original data was normalized to have pixel values between 0 and 1, we use a <code class="inline">:sigmoid</code> activation in our dense layer to squeeze output values between 0 and 1. Our original input shape was 28x28, so we use <code class="inline">Axon.reshape</code> to convert the flattened representation of the outputs into an image with correct the width and height.</p><pre><code class="makeup elixir" translate="no"><span class="n">decoder</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k" data-group-id="7118828340-1">fn</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">-&gt;</span><span class="w">
  </span><span class="n">x</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Axon</span><span class="o">.</span><span class="n">dense</span><span class="p" data-group-id="7118828340-2">(</span><span class="mi">784</span><span class="p">,</span><span class="w"> </span><span class="ss">activation</span><span class="p">:</span><span class="w"> </span><span class="ss">:sigmoid</span><span class="p" data-group-id="7118828340-2">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Axon</span><span class="o">.</span><span class="n">reshape</span><span class="p" data-group-id="7118828340-3">(</span><span class="p" data-group-id="7118828340-4">{</span><span class="ss">:batch</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">28</span><span class="p">,</span><span class="w"> </span><span class="mi">28</span><span class="p" data-group-id="7118828340-4">}</span><span class="p" data-group-id="7118828340-3">)</span><span class="w">
</span><span class="k" data-group-id="7118828340-1">end</span></code></pre><p>If we just bind the encoder and decoder sequentially, we'll get the desired model. This was pretty smooth, wasn't it?</p><pre><code class="makeup elixir" translate="no"><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w">
  </span><span class="nc">Axon</span><span class="o">.</span><span class="n">input</span><span class="p" data-group-id="6357149604-1">(</span><span class="s">&quot;input&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">shape</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="6357149604-2">{</span><span class="no">nil</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">28</span><span class="p">,</span><span class="w"> </span><span class="mi">28</span><span class="p" data-group-id="6357149604-2">}</span><span class="p" data-group-id="6357149604-1">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">encoder</span><span class="o">.</span><span class="p" data-group-id="6357149604-3">(</span><span class="mi">64</span><span class="p" data-group-id="6357149604-3">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">decoder</span><span class="o">.</span><span class="p" data-group-id="6357149604-4">(</span><span class="p" data-group-id="6357149604-4">)</span></code></pre><h2 id="training-the-model" class="section-heading">
  <a href="#training-the-model" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i>
  <p class="sr-only">training-the-model</p>
  </a>
  Training the model
</h2>
<p>Finally, we can train the model. We'll use the <code class="inline">:adam</code> and <code class="inline">:mean_squared_error</code> loss with <code class="inline">Axon.Loop.trainer</code>. Our loss function will measure the aggregate error between pixels of original images and the model's reconstructed images. We'll also <code class="inline">:mean_absolute_error</code> using <code class="inline">Axon.Loop.metric</code>. <code class="inline">Axon.Loop.run</code> trains the model with the given training data.</p><pre><code class="makeup elixir" translate="no"><span class="n">batch_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">32</span><span class="w">
</span><span class="n">epochs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5</span><span class="w">

</span><span class="n">batched_images</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">to_batched</span><span class="p" data-group-id="8376087205-1">(</span><span class="n">train_images</span><span class="p">,</span><span class="w"> </span><span class="n">batch_size</span><span class="p" data-group-id="8376087205-1">)</span><span class="w">
</span><span class="n">train_batches</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Stream</span><span class="o">.</span><span class="n">zip</span><span class="p" data-group-id="8376087205-2">(</span><span class="n">batched_images</span><span class="p">,</span><span class="w"> </span><span class="n">batched_images</span><span class="p" data-group-id="8376087205-2">)</span><span class="w">

</span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w">
  </span><span class="n">model</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Axon.Loop</span><span class="o">.</span><span class="n">trainer</span><span class="p" data-group-id="8376087205-3">(</span><span class="ss">:mean_squared_error</span><span class="p">,</span><span class="w"> </span><span class="ss">:adam</span><span class="p" data-group-id="8376087205-3">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Axon.Loop</span><span class="o">.</span><span class="n">metric</span><span class="p" data-group-id="8376087205-4">(</span><span class="ss">:mean_absolute_error</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Error&quot;</span><span class="p" data-group-id="8376087205-4">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Axon.Loop</span><span class="o">.</span><span class="n">run</span><span class="p" data-group-id="8376087205-5">(</span><span class="n">train_batches</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="8376087205-6">%{</span><span class="p" data-group-id="8376087205-6">}</span><span class="p">,</span><span class="w"> </span><span class="ss">epochs</span><span class="p">:</span><span class="w"> </span><span class="n">epochs</span><span class="p">,</span><span class="w"> </span><span class="ss">compiler</span><span class="p">:</span><span class="w"> </span><span class="nc">EXLA</span><span class="p" data-group-id="8376087205-5">)</span></code></pre><h2 id="extra-losses" class="section-heading">
  <a href="#extra-losses" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i>
  <p class="sr-only">extra-losses</p>
  </a>
  Extra: losses
</h2>
<p>To better understand what is mean absolute error (MAE) and mean square error (MSE) let's go through an example.</p><pre><code class="makeup elixir" translate="no"><span class="c1"># Error definitions for a single sample</span><span class="w">

</span><span class="n">mean_square_error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k" data-group-id="9731186877-1">fn</span><span class="w"> </span><span class="n">y_pred</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">-&gt;</span><span class="w">
  </span><span class="n">y_pred</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">subtract</span><span class="p" data-group-id="9731186877-2">(</span><span class="n">y</span><span class="p" data-group-id="9731186877-2">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">power</span><span class="p" data-group-id="9731186877-3">(</span><span class="mi">2</span><span class="p" data-group-id="9731186877-3">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">mean</span><span class="p" data-group-id="9731186877-4">(</span><span class="p" data-group-id="9731186877-4">)</span><span class="w">
</span><span class="k" data-group-id="9731186877-1">end</span><span class="w">

</span><span class="n">mean_absolute_erorr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k" data-group-id="9731186877-5">fn</span><span class="w"> </span><span class="n">y_pred</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">-&gt;</span><span class="w">
  </span><span class="n">y_pred</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">subtract</span><span class="p" data-group-id="9731186877-6">(</span><span class="n">y</span><span class="p" data-group-id="9731186877-6">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">abs</span><span class="p" data-group-id="9731186877-7">(</span><span class="p" data-group-id="9731186877-7">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">mean</span><span class="p" data-group-id="9731186877-8">(</span><span class="p" data-group-id="9731186877-8">)</span><span class="w">
</span><span class="k" data-group-id="9731186877-5">end</span></code></pre><p>We will work with a sample image of a shoe, a slightly noised version of that image, and also an entirely different image from the dataset.</p><pre><code class="makeup elixir" translate="no"><span class="n">shoe_image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train_images</span><span class="p" data-group-id="9773098987-1">[</span><span class="mi">0</span><span class="p" data-group-id="9773098987-1">]</span><span class="w">
</span><span class="n">noised_shoe_image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">add</span><span class="p" data-group-id="9773098987-2">(</span><span class="n">shoe_image</span><span class="p">,</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">random_normal</span><span class="p" data-group-id="9773098987-3">(</span><span class="n">shoe_image</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.05</span><span class="p" data-group-id="9773098987-3">)</span><span class="p" data-group-id="9773098987-2">)</span><span class="w">
</span><span class="n">other_image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train_images</span><span class="p" data-group-id="9773098987-4">[</span><span class="mi">1</span><span class="p" data-group-id="9773098987-4">]</span><span class="w">
</span><span class="ss">:ok</span></code></pre><p>For the same image both errors should be 0, because when we have two exact copies, there is no pixel difference.</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="3503575323-1">{</span><span class="w">
  </span><span class="n">mean_square_error</span><span class="o">.</span><span class="p" data-group-id="3503575323-2">(</span><span class="n">shoe_image</span><span class="p">,</span><span class="w"> </span><span class="n">shoe_image</span><span class="p" data-group-id="3503575323-2">)</span><span class="p">,</span><span class="w">
  </span><span class="n">mean_absolute_erorr</span><span class="o">.</span><span class="p" data-group-id="3503575323-3">(</span><span class="n">shoe_image</span><span class="p">,</span><span class="w"> </span><span class="n">shoe_image</span><span class="p" data-group-id="3503575323-3">)</span><span class="w">
</span><span class="p" data-group-id="3503575323-1">}</span></code></pre><p>Now the noised image:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="0415645641-1">{</span><span class="w">
  </span><span class="n">mean_square_error</span><span class="o">.</span><span class="p" data-group-id="0415645641-2">(</span><span class="n">shoe_image</span><span class="p">,</span><span class="w"> </span><span class="n">noised_shoe_image</span><span class="p" data-group-id="0415645641-2">)</span><span class="p">,</span><span class="w">
  </span><span class="n">mean_absolute_erorr</span><span class="o">.</span><span class="p" data-group-id="0415645641-3">(</span><span class="n">shoe_image</span><span class="p">,</span><span class="w"> </span><span class="n">noised_shoe_image</span><span class="p" data-group-id="0415645641-3">)</span><span class="w">
</span><span class="p" data-group-id="0415645641-1">}</span></code></pre><p>And a different image:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="8111377934-1">{</span><span class="w">
  </span><span class="n">mean_square_error</span><span class="o">.</span><span class="p" data-group-id="8111377934-2">(</span><span class="n">shoe_image</span><span class="p">,</span><span class="w"> </span><span class="n">other_image</span><span class="p" data-group-id="8111377934-2">)</span><span class="p">,</span><span class="w">
  </span><span class="n">mean_absolute_erorr</span><span class="o">.</span><span class="p" data-group-id="8111377934-3">(</span><span class="n">shoe_image</span><span class="p">,</span><span class="w"> </span><span class="n">other_image</span><span class="p" data-group-id="8111377934-3">)</span><span class="w">
</span><span class="p" data-group-id="8111377934-1">}</span></code></pre><p>As we can see, the noised image has a non-zero MSE and MAE but is much smaller than the error of two completely different pictures. In other words, both of these error types measure the level of similarity between images. A small error implies decent prediction values. On the other hand, a large error value suggests poor quality of predictions.</p><p>If you look at our implementation of MAE and MSE, you will notice that they are very similar. MAE and MSE can also be called the $L_1$ and $L_2$ loss respectively for the $L_1$ and $L_2$ norm. The $L_2$ loss (MSE) is typically preferred because it's a smoother function whereas $L_1$ is often difficult to optimize with stochastic gradient descent (SGD).</p><h2 id="inference" class="section-heading">
  <a href="#inference" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i>
  <p class="sr-only">inference</p>
  </a>
  Inference
</h2>
<p>Now, let's see how our model is doing! We will compare a sample image before and after compression.</p><pre><code class="makeup elixir" translate="no"><span class="n">sample_image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train_images</span><span class="p" data-group-id="7447577907-1">[</span><span class="mi">0</span><span class="o">..</span><span class="mi">0</span><span class="o">//</span><span class="mi">1</span><span class="p" data-group-id="7447577907-1">]</span><span class="w">
</span><span class="n">compressed_image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Axon</span><span class="o">.</span><span class="n">predict</span><span class="p" data-group-id="7447577907-2">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="n">sample_image</span><span class="p">,</span><span class="w"> </span><span class="ss">compiler</span><span class="p">:</span><span class="w"> </span><span class="nc">EXLA</span><span class="p" data-group-id="7447577907-2">)</span><span class="w">

</span><span class="n">sample_image</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">to_heatmap</span><span class="p" data-group-id="7447577907-3">(</span><span class="p" data-group-id="7447577907-3">)</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">IO</span><span class="o">.</span><span class="n">inspect</span><span class="p" data-group-id="7447577907-4">(</span><span class="ss">label</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Original&quot;</span><span class="p" data-group-id="7447577907-4">)</span><span class="w">

</span><span class="n">compressed_image</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Nx</span><span class="o">.</span><span class="n">to_heatmap</span><span class="p" data-group-id="7447577907-5">(</span><span class="p" data-group-id="7447577907-5">)</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">IO</span><span class="o">.</span><span class="n">inspect</span><span class="p" data-group-id="7447577907-6">(</span><span class="ss">label</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Compressed&quot;</span><span class="p" data-group-id="7447577907-6">)</span><span class="w">

</span><span class="ss">:ok</span></code></pre><p>As we can see, the generated image is similar to the input image. The only difference between them is the absence of a sign in the middle of the second shoe. The model treated the sign as noise and bled this into the plain shoe.</p>
<div class="bottom-actions">
  <div class="bottom-actions-item">

      <a href="mnist_autoencoder_using_kino.html" class="bottom-actions-button" rel="prev">
        <span class="subheader">
          ← Previous Page
        </span>
        <span class="title">
MNIST Denoising Autoencoder using Kino for visualization
        </span>
      </a>

  </div>
  <div class="bottom-actions-item">

      <a href="fashionmnist_vae.html" class="bottom-actions-button" rel="next">
        <span class="subheader">
          Next Page →
        </span>
        <span class="title">
A Variational Autoencoder for MNIST
        </span>
      </a>

  </div>
</div>
      <footer class="footer">

          <p>
            On Hex.pm:

            <span class="line">
              <a href="https://hex.pm/packages/axon/0.3.0" class="line footer-hex-package">Package</a>
              <a href="https://preview.hex.pm/preview/axon/0.3.0" class="line">Preview</a>

                <a href="https://preview.hex.pm/preview/axon/0.3.0/show/notebooks/generative/fashionmnist_autoencoder.livemd">(current file)</a>

            </span>

            <button class="a-main line footer-button display-quick-switch">
              Search
            </button>
          </p>

        <p>
          Built using
          <a href="https://github.com/elixir-lang/ex_doc" title="ExDoc" target="_blank" rel="help noopener" translate="no">ExDoc</a> (v0.28.6) for the

            <a href="https://elixir-lang.org" title="Elixir" target="_blank" translate="no">Elixir programming language</a>

        </p>
      </footer>
    </div>
  </div>
</section>
</div>

<!-- Render math with KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js" integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB+w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
      ]
    });
  });
</script>

<!-- Render diagrams with Mermaid -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@8.13.3/dist/mermaid.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    mermaid.initialize({ startOnLoad: false });
    let id = 0;
    for (const codeEl of document.querySelectorAll("pre code.mermaid")) {
      const preEl = codeEl.parentElement;
      const graphDefinition = codeEl.textContent;
      const graphEl = document.createElement("div");
      const graphId = "mermaid-graph-" + id++;
      mermaid.render(graphId, graphDefinition, function (svgSource, bindListeners) {
        graphEl.innerHTML = svgSource;
        bindListeners && bindListeners(graphEl);
        preEl.insertAdjacentElement("afterend", graphEl);
        preEl.remove();
      });
    }
  });
</script>

  </body>
</html>
