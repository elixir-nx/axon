<!-- livebook:{"persist_outputs":true} -->

# Training and inference mode

```elixir
Mix.install([
  {:axon, github: "elixir-nx/axon"},
  {:nx, "~> 0.3.0", github: "elixir-nx/nx", sparse: "nx", override: true}
])
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Executing models in inference mode

Some layers have different considerations and behavior when running during model training versus model inference. For example *dropout layers* are intended only to be used during training as a form of model regularization. Certain stateful layers like *batch normalization* keep a running-internal state which changes during training mode but remains fixed during inference mode. Axon supports mode-dependent execution behavior via the `:mode` option passed to all building, compilation, and execution methods. By default, all models build in inference mode. You can see this behavior by adding a dropout layer with a dropout rate of 1. In inference mode this layer will have no affect:

```elixir
inputs = Nx.iota({2, 8}, type: :f32)

model =
  Axon.input("data")
  |> Axon.dense(4)
  |> Axon.sigmoid()
  |> Axon.dropout(rate: 0.99)
  |> Axon.dense(1)

{init_fn, predict_fn} = Axon.build(model)
params = init_fn.(inputs, %{})
predict_fn.(params, inputs)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][1]
  [
    [-0.6138466000556946],
    [-0.8409845232963562]
  ]
>
```

You can also explicitly specify the mode:

```elixir
{init_fn, predict_fn} = Axon.build(model, mode: :inference)
params = init_fn.(inputs, %{})
predict_fn.(params, inputs)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][1]
  [
    [0.7551136016845703],
    [0.448221355676651]
  ]
>
```

It's important that you know which mode your model's were compiled for, as running a model built in `:inference` mode will behave drastically different than a model built in `:train` mode.

## Executing models in training mode

By specifying `mode: :train`, you tell your models to execute in training mode. You can see the effects of this behavior here:

```elixir
{init_fn, predict_fn} = Axon.build(model, mode: :train)
params = init_fn.(inputs, %{})
predict_fn.(params, inputs)
```

<!-- livebook:{"output":true} -->

```
%{
  prediction: #Nx.Tensor<
    f32[2][1]
    [
      [0.0],
      [0.0]
    ]
  >,
  state: %{}
}
```

First, notice that your model now returns a map with keys `:prediction` and `:state`. `:prediction` contains the actual model prediction, while `:state` contains the updated state for any stateful layers such as batch norm. When writing custom training loops, you should extract `:state` and use it in conjunction with the updates API to ensure your stateful layers are updated correctly. If your model has stateful layers, `:state` will look similar to your model's parameter map:

```elixir
model =
  Axon.input("data")
  |> Axon.dense(4)
  |> Axon.sigmoid()
  |> Axon.batch_norm()
  |> Axon.dense(1)

{init_fn, predict_fn} = Axon.build(model, mode: :train)
params = init_fn.(inputs, %{})
predict_fn.(params, inputs)
```

<!-- livebook:{"output":true} -->

```
%{
  prediction: #Nx.Tensor<
    f32[2][1]
    [
      [0.03675001487135887],
      [-0.03674999624490738]
    ]
  >,
  state: %{
    "batch_norm_0" => %{
      "mean" => #Nx.Tensor<
        f32[4]
        [0.8784151673316956, 0.7386987209320068, 0.663623571395874, 0.8947045803070068]
      >,
      "var" => #Nx.Tensor<
        f32[4]
        [0.10050597041845322, 0.11294332146644592, 0.16061438620090485, 0.10003116726875305]
      >
    }
  }
}
```
