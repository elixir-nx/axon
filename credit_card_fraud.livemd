# Classifying fraudulent transactions

## General overview

This time we will examine the Credit Card Fraud Data set. Due to confidentiality, the original data were preprocessed by PCA, and then 31 principal components were selected for the final data set. The dataset is highly imbalanced. The positive class (frauds) account for 0.172% of all transactions. Eventually, we will create a classifier which has not only great accuracy but, what is even more important, a high *recall* and *precision*---which are metrics that are much more indicative of performance with imbalanced classification problems

## Imports

Since we'll operate on tabular data, we will use the ```Explorer``` package extensively. We will also use ```Axon``` and ```Nx``` for building a machine learning model and for tensor manipulations.

```elixir
Mix.install([
  {:axon, "~> 0.1.0"},
  {:exla, "~> 0.2.2"},
  {:nx, "~> 0.2.1"},
  {:explorer, "~> 0.2.0"}
])

EXLA.set_as_nx_default([:tpu, :cuda, :rocm, :host])
```

## Data processing

The first step is to prepare the data for training and evaluation. We load the data set into an ```Explorer.DataFrame``` object from CSV. Then we split the data into training and test data (in proportion 80% into a training set and 20% into a test set) using the *split_train_test* function. In the next step, we separate features from target labels using *split_features_targets*. Now we should normalize the data to our neural-net performs better. The normalize_data function divides each value for a particular feature by the maximum absolute value in this feature. In the end, we cast data frames into tensors with *df_to_tensor*.

```elixir
# Download data with a Kaggle account: https://www.kaggle.com/mlg-ulb/creditcardfraud/
@file_name "creditcard.csv"

defp data() do
  IO.puts("Loading #{@file_name}")
  df = Explorer.DataFrame.from_csv!(@file_name, dtypes: [{"Time", :float}])

  {train_df, test_df} = split_train_test(df, 0.8)

  IO.puts("Training Samples: #{inspect(Explorer.DataFrame.n_rows(train_df))}")
  IO.puts("Testing Samples: #{inspect(Explorer.DataFrame.n_rows(test_df))}")
  IO.write("\n\n")

  {train_features, train_targets} = split_features_targets(train_df)
  {test_features, test_targets} = split_features_targets(test_df)

  train_features = normalize_data(train_features)
  test_features = normalize_data(test_features)

  {
    {df_to_tensor(train_features), df_to_tensor(train_targets)},
    {df_to_tensor(test_features), df_to_tensor(test_targets)}
  }
end

defp split_train_test(df, portion) do
  num_examples = Explorer.DataFrame.n_rows(df)
  num_train = ceil(portion * num_examples)
  num_test = num_examples - num_train

  {
    Explorer.DataFrame.slice(df, 0, num_train),
    Explorer.DataFrame.slice(df, num_train, num_test)
  }
end

defp split_features_targets(df) do
  features = Explorer.DataFrame.select(df, &(&1 == "Class"), :drop)
  targets = Explorer.DataFrame.select(df, &(&1 == "Class"), :keep)
  {features, targets}
end

defp normalize(name),
  do: fn df ->
    Explorer.Series.divide(
      df[name],
      Explorer.Series.max(
        Explorer.Series.transform(df[name], fn x ->
          if x >= 0 do
            x
          else
            -x
          end
        end)
      )
    )
  end

defp normalize_data(df) do
  df
  |> Explorer.DataFrame.names()
  |> Map.new(&{&1, normalize(&1)})
  |> then(&Explorer.DataFrame.mutate(df, &1))
end

defp df_to_tensor(df) do
  df
  |> Explorer.DataFrame.names()
  |> Enum.map(&(Explorer.Series.to_tensor(df[&1]) |> Nx.new_axis(-1)))
  |> Nx.concatenate(axis: 1)
end
```

## Building the model

Our model for predicting whether a transaction was fraudulent or not is a dense neural net. It consists of two dense layers with 256 neurons, ReLU activation functions, one dropout layer, and a dense layer with one neuron (since the problem is binary prediction) followed by a sigmoid activation function.

```elixir
defp build_model(num_features) do
  Axon.input({nil, num_features}, "input")
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(1)
  |> Axon.sigmoid()
end
```

## Training, testing & summary

Now since we have both data and model architecture prepared, we design training and test loops. In *train_model*, we pass the loss function and optimizer of our choice and run 30 epochs of the learning process. After the training, there is only one thing left: testing. In *test_model*, we will focus on the number of true positive, true negative, false positive, and false negative values but also on the likelihood of denying legit and fraudulent transactions.

```elixir
defp train_model(model, loss, optimizer, train_data) do
  model
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.run(train_data, %{}, epochs: 30, compiler: EXLA)
end

defp test_model(model, model_state, test_data) do
  model
  |> Axon.Loop.evaluator()
  |> metrics()
  |> Axon.Loop.handle(:epoch_completed, &summarize/1)
  |> Axon.Loop.run(test_data, model_state, compiler: EXLA)
end

defp summarize(%State{metrics: metrics} = state) do
  IO.write("\n\n")

  legit_transactions_declined = Nx.to_number(metrics["fp"])
  legit_transactions_accepted = Nx.to_number(metrics["tn"])
  fraud_transactions_accepted = Nx.to_number(metrics["fn"])
  fraud_transactions_declined = Nx.to_number(metrics["tp"])
  total_fraud = fraud_transactions_declined + fraud_transactions_accepted
  total_legit = legit_transactions_declined + legit_transactions_accepted

  fraud_denial_percent = 100 * (fraud_transactions_declined / total_fraud)
  legit_denial_percent = 100 * (legit_transactions_declined / total_legit)

  IO.puts("Legit Transactions Declined: #{legit_transactions_declined}")
  IO.puts("Fraudulent Transactions Caught: #{fraud_transactions_declined}")
  IO.puts("Fraudulent Transactions Missed: #{fraud_transactions_accepted}")
  IO.puts("Likelihood of catching fraud: #{fraud_denial_percent}%")
  IO.puts("Likelihood of denying legit transaction: #{legit_denial_percent}%")

  {:continue, state}
end

defp metrics(loop) do
  loop
  |> Axon.Loop.metric(:true_positives, "tp", :running_sum)
  |> Axon.Loop.metric(:true_negatives, "tn", :running_sum)
  |> Axon.Loop.metric(:false_positives, "fp", :running_sum)
  |> Axon.Loop.metric(:false_negatives, "fn", :running_sum)
end
```

## Model evaluation

In the end, we combine all things. Load and process data and then chunk them into batches. Next, create the model and set the loss function to binary cross-entropy and optimizer to adam with a learning rate of 0.01. then train and test the model.

```elixir
def run() do
  {train, test} = data()
  {train_inputs, train_targets} = train
  {test_inputs, test_targets} = test

  fraud = Nx.sum(train_targets) |> Nx.to_number()
  legit = Nx.size(train_targets) - fraud

  batched_train_inputs = Nx.to_batched_list(train_inputs, 2048)
  batched_train_targets = Nx.to_batched_list(train_targets, 2048)
  batched_train = Stream.zip(batched_train_inputs, batched_train_targets)

  batched_test_inputs = Nx.to_batched_list(test_inputs, 2048)
  batched_test_targets = Nx.to_batched_list(test_targets, 2048)
  batched_test = Stream.zip(batched_test_inputs, batched_test_targets)

  IO.puts("# of legit transactions (train): #{legit}")
  IO.puts("# of fraudulent transactions (train): #{fraud}")
  IO.puts("% fraudlent transactions (train): #{100 * (fraud / (legit + fraud))}%")
  IO.write("\n\n")

  model = build_model(elem(train_inputs.shape, 1))

  loss =
    &Axon.Losses.binary_cross_entropy(
      &1,
      &2,
      negative_weight: 1 / legit,
      positive_weight: 1 / fraud,
      reduction: :mean
    )

  optimizer = Axon.Optimizers.adam(1.0e-2)

  model
  |> train_model(loss, optimizer, batched_train)
  |> then(&test_model(model, &1, batched_test))
end
```
